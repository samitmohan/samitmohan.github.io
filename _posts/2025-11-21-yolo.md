---
layout: post
title: "Understanding Object Detection"
date: 2025-11-21
---

Say you're on the street, in just milliseconds you spot a car, man, traffic light, dogs etc.. you don't just see a mash of pixels, you see *objects*. You know where they are, what they are and how they're moving. How can you teach a computer the same thing?

For a long time, teaching a computer to do this was a painfully slow process. Early models were like a detective with a magnifying glass, scanning an image patch by patch, asking, "Is this a car? No. Is *this* a car? No." It worked, but it was slow.

In 2016, a paper came out with a name: **You Only Look Once (YOLO)**.

The name says it all. What if, instead of looking at an image thousands of times, a computer could just... look once? This post is the story of that idea. This blog discusses YOLO, figures out *why* it works, and see where it stumbles. This also consists of the history of YOLO models and how we have reached state-of-the-art YOLOv10 model of today.

## Table of Contents

0. [Pipeline](#pipeline)
1. [Before YOLO](#before-yolo)
2. [Why YOLO: The Motivation](#why-yolo-the-motivation)
3. [YOLO High Level Overview](#yolo-high-level-overview)
4. [YOLO Architecture](#yolo-architecture)
5. [Grid Cells & Predictions](#grid-cells-and-predictions)
6. [Bounding Box Format & Encoding](#bounding-box-format-and-encoding)
7. [Prediction Vector Breakdown](#prediction-vector-breakdown)
8. [Training Process](#training-process)
9. [Loss Function Explained](#loss-function-explained)
10. [Inference: From Predictions to Bounding Boxes](#inference-from-predictions-to-bounding-boxes)
11. [Post-Processing: IOU & NMS](#post-processing-iou-and-nms)
12. [Evaluation Metrics: mAP](#evaluation-metrics-map)
13. [Performance & Results](#performance-and-results)
14. [Limitations of YOLOv1](#limitations-of-yolov1)
15. [Evolution: From v1 to v10](#evolution-from-v1-to-v10)
16. [Implementation: PyTorch Code](#implementation-pytorch-code-examples)
17. [Extra](#extra)

---

## Pipeline

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         YOLO OBJECT DETECTION                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Image (448Ã—448)
      â”‚
      â”œâ”€â”€â–º Step 1: Grid Division
      â”‚          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
      â”‚          â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚     7Ã—7 grid
      â”‚          â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     Each cell = 64Ã—64 pixels
      â”‚          â”‚   â”‚ P â”‚   â”‚   â”‚   â”‚   â”‚   â”‚     P = Person center
      â”‚          â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     D = Dog center
      â”‚          â”‚   â”‚   â”‚   â”‚ D â”‚   â”‚   â”‚   â”‚
      â”‚          â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
      â”‚
      â”œâ”€â”€â–º Step 2: CNN Forward Pass (24 conv layers)
      â”‚          Features extracted: 7Ã—7Ã—1024
      â”‚
      â”œâ”€â”€â–º Step 3: Predictions per Cell
      â”‚          Each cell outputs 30 values:
      â”‚          â€¢ 2 boxes: (x,y,w,h,conf) Ã— 2 = 10 values
      â”‚          â€¢ 20 class probabilities = 20 values
      â”‚          Total: 7Ã—7Ã—30 = 1,470 predictions
      â”‚
      â”œâ”€â”€â–º Step 4: Post-Processing
      â”‚          â€¢ Filter by confidence (threshold = 0.2)
      â”‚          â€¢ Apply Non-Maximum Suppression (NMS)
      â”‚          â€¢ Keep top predictions per class
      â”‚
      â””â”€â”€â–º Final Output
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Person: 95.3%   â”‚  [Bounding boxes with labels]
               â”‚  Dog: 87.6%      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Metrics

| Metric | Value | Comparison |
|--------|-------|------------|
| **Speed** | 45 fps | 9Ã— faster than Faster R-CNN |
| **Accuracy** | 63.4% mAP | -7% vs Faster R-CNN |
| **Grid Size** | 7Ã—7 cells | 49 possible detections |
| **Parameters** | ~20M | ResNet34 backbone |
| **Input Size** | 448Ã—448 | Fixed resolution |

---

## Before YOLO

### Traditional Computer Vision

Earlier approaches picked up patterns based on mathematical features from images in matrix form. We figured out where hue, contrast, and pixel density were high and used that to classify images (using models like ResNet).

**The Problem:** This doesn't work well for complex scenarios like CCTV footage with:

- Dozens of objects
- Heavy occlusion
- Complex features
- Real-time requirements (ResNet takes huge amounts of time even during inference)

### Two-Stage Detectors: R-CNN & Faster R-CNN

Then came the two-stage detection pipeline:

```text
Image â†’ Backbone (CNN/ResNet) â†’ Convolutional Feature Map
      â†’ Stage 1: Generate Bounding Box Proposals
      â†’ Stage 2: Classification + Box Refinement (Regression Head)
```

The convolutional feature map contains high-level semantic information about the image. The first stage generates potential object locations (proposals), and the second stage refines these proposals and assigns class labels.

**Why "Two-Stage"?**
The first pass generates a set of proposals (potential object locations), and the second pass refines these proposals to make final predictions.

**Advantages:**

- More accurate than previous methods
- Each component can be optimized separately

**Disadvantages:**

- Multi-stage pipeline is complex
- Each component trained separately
- Not suitable for real-time applications (~5 fps on GPU)
- Computationally expensive

### Comparison: Object Detection Methods (2014-2016)

| Method | Year | Speed (fps) | mAP (%) | Pipeline Type | Proposals | Real-time? |
|--------|------|-------------|---------|---------------|-----------|------------|
| **R-CNN** | 2014 | 0.02 | 66.0 | Two-stage | Selective Search (2000+) | âŒ |
| **Fast R-CNN** | 2015 | 0.5 | 70.0 | Two-stage | Selective Search | âŒ |
| **Faster R-CNN** | 2015 | 7 | 73.2 | Two-stage | RPN (300) | âŒ |
| **DPM** | 2015 | <1 | 30.4 | Sliding window | Dense sampling | âŒ |
| **YOLOv1** | 2016 | **45** | 63.4 | **Single-stage** | **None** | **âœ…** |

---

## Why YOLO: The Motivation

Core Idea: **What if we could do object detection as a single regression problem?**

Instead of:

1. Generate proposals â†’ 2. Classify proposals

Do:
**Single network â†’ Box detection + Category prediction simultaneously**

### Key Advantages of Single-Stage Detection:

- **Speed**: One forward pass through the network
- **Simplicity**: End-to-end training with a unified loss function
- **Global reasoning**: The network sees the entire image, understanding context
- **Real-time performance**: 45+ fps (compared to 5 fps for two-stage detectors)

### The Trade-off:

- Slightly lower accuracy on metrics

YOLO was created by Ultralytics (2016) to solve this problem: a single network that can perform both box detection and category prediction in one pass.

---

## YOLO High Level Overview

YOLO (You Only Look Once) reformulates object detection as a **single regression problem**:

### Concept

1. **Input**: Image of any size (resized to 448Ã—448)
2. **Process**: Divide image into SÃ—S grid cells (S=7 in the paper)
3. **Predict**: For each grid cell, predict B bounding boxes and C class probabilities
4. **Output**: Tensor of shape SÃ—SÃ—(BÃ—5+C)

### Why "You Only Look Once"?

Unlike sliding window approaches (like DPM) that run a classifier at every position, or two-stage detectors that process thousands of region proposals, YOLO looks at the image **once** in a single forward pass.

### Single Pass, End-to-End

- No separate proposal generation
- No batch processing of regions
- One network, one forward pass
- Direct prediction from pixels to bounding boxes and classes

![YOLO High-Level Pipeline](/assets/images/yolo/basic.png)
*Figure: YOLO's single-stage detection pipeline - the entire image is processed once to produce bounding boxes and class predictions*

![YOLO Algorithm Overview](/assets/images/yolo/yoloAlgo.png)
*Figure: The complete YOLO detection algorithm from input image to final predictions*

---

## The "Brain" of YOLO: The Architecture

At its heart, YOLO is a Convolutional Neural Network (CNN). 

CNN Analogy:: Think of it like a company's hierarchy, designed to process information from the ground up.

1. **The Interns (Early Layers):** The first few layers of the network are like interns. They get the raw image and have a very simple job: find basic patterns. One intern looks for vertical edges, another for horizontal edges, another for patches of green, and so on. They're not smart, but they're fast and there are a lot of them.

2. **The Mid-Level Managers (Deeper Layers):** These layers take the simple reports from the interns and combine them into more complex ideas. A manager might see reports for a sharp corner, a curved line, and a circular shape and say, "Hmm, that's starting to look like an eye." Another might combine reports of four straight lines and a rectangle to identify a "window."

3. **The CEO (Final Layers):** The final layers are the executives. They take the high-level reports from the managers ("we've got an eye," "we've got a furry texture," "we've got a wet nose") and make the final call: "That's a dog."

YOLO's architecture is essentially a 24-layer version of this, followed by 2 "fully connected" layers which act like the final board meeting where all the information is synthesized into the final output.

**Backbone (Feature Extraction):**

- **24 convolutional layers** with alternating 1Ã—1 and 3Ã—3 filters
- **4 max-pooling layers** for spatial downsampling
- **Leaky ReLU activation** (for all layers except the last)

**Detection Head:**

- **2 fully connected layers** (4096 neurons â†’ 1470 neurons)
- **Linear activation** on the final layer

![CNN Visualization](/assets/images/yolo/cnn.png)
*Figure: Convolutional Neural Network layers and how they extract features from images*

![CNN Feature Extraction](/assets/images/yolo/cnn2.png)
*Figure: Detailed view of convolutional operations and feature map generation*

![Convolution Operation](/assets/images/yolo/conv.png)
*Figure: How convolution filters slide across the image to detect patterns*

### Input â†’ Output Flow

```text
Input: 448Ã—448Ã—3 (RGB image)
   â†“
[24 Convolutional Layers with MaxPooling]
   â†“
Feature Map: 7Ã—7Ã—1024
   â†“
[Flatten: 7Ã—7Ã—1024 = 50,176 features]
   â†“
[FC Layer 1: 50,176 â†’ 4096]
   â†“
[FC Layer 2: 4096 â†’ 1470]
   â†“
[Reshape: 1470 â†’ 7Ã—7Ã—30]
   â†“
Output: 7Ã—7Ã—30 tensor
```

### Why Stack Convolutional Layers?

**Purpose**: Extract spatial features from the image while preserving structure.

**The Problem**: Simply stacking convolutions doesn't increase non-linearity in the data.

**The Solution**:

- Use **non-linear activation functions** (Leaky ReLU) between layers
- Use **1Ã—1 convolutions** to reduce dimensionality and add non-linearity
- Use **3Ã—3 convolutions** to extract spatial features and downsample

### Why Leaky ReLU?

Standard ReLU: `f(x) = max(0, x)` â†’ can cause "dying neurons" (always output 0)

Leaky ReLU: `f(x) = x if x > 0 else 0.1*x` â†’ allows small negative gradients, preventing dead neurons

### Regularization Techniques:

- **Dropout** (rate=0.5) after the first FC layer 
- **Data augmentation** (random scaling, translation, HSV adjustments)

![YOLO Architecture](/assets/images/yolo/arch.png)
*Figure: Complete YOLOv1 network architecture with 24 convolutional layers and 2 fully connected layers*

![YOLO Architecture Detailed](/assets/images/yolo/arch2.png)
*Figure: Layer-by-layer breakdown showing filter sizes, dimensions, and feature map transformations*

![YOLO Architecture Full](/assets/images/yolo/arch3.png)
*Figure: The full network from input (448Ã—448Ã—3) to output (7Ã—7Ã—30) tensor*

---

## How YOLO Works: A Game of Battleship

So how does it "look once"? The core idea is surprisingly simple and elegant. It's like a high-stakes, super-fast game of Battleship.

Here's the game plan:

1.**Take the image and lay a grid over it.** The original YOLO uses a 7x7 grid. This divides the image into 49 cells.

2.**Each grid cell gets a job.** It's responsible for detecting any object whose *center point* falls within that cell. It's like each square on the Battleship board is responsible for reporting if the center of a ship is in it.

3.**Make a prediction for every cell.** A single, powerful neural network looks at the whole image and, for every single one of the 49 grid cells, it spits out a prediction. This prediction answers a few key questions:
    - "Is an object's center in here? How confident am I?"
    - "If so, where is the bounding box for that object?"
    - "And what class is it (a dog, a person, a car)?"

That's it. One look, one pass through the network, and out comes a flood of predictions from all 49 cells at once. No proposals, no second stage. Just a direct mapping from image pixels to bounding boxes and class probabilities.

### The Grid System

**Step 1**: Divide the 448Ã—448 image into a 7Ã—7 grid (S=7 in the paper)

- Each grid cell: 64Ã—64 pixels (448/7 = 64)

**Step 2**: Assign responsibility (The "Battleship Rule")

- Each cell is responsible for detecting **one object**
- **Which object?** The one whose **center point** falls into that cell
- Think: Each square on the Battleship board reports if a ship's center is in it

![Grid Cells and Center Points](/assets/images/yolo/grid_cell_and_center.png)
*Figure: Image divided into 7Ã—7 grid with object center points marked - each cell is responsible for objects whose centers fall within it*

![Object Centers](/assets/images/yolo/center.png)
*Figure: Identifying which grid cell is responsible for each object based on center point location*

![Center Point Details](/assets/images/yolo/center_2.png)
*Figure: Close-up view of how object center points determine grid cell responsibility*

![Center of Cell Calculation](/assets/images/yolo/center_of_cell.png)
*Figure: Computing the center point and assigning it to the appropriate grid cell*

### What Does "Responsible" Mean?

If an object's center point falls into a grid cell, that cell must:

1. Predict the bounding box coordinates
2. Predict the confidence that an object is present
3. Predict the class probabilities

### Example

- Person's center at (100, 200) â†’ falls into grid cell (1, 3)
- Horse's center at (300, 150) â†’ falls into grid cell (4, 2)
- Grid cell (1, 3) is responsible for detecting "person"
- Grid cell (4, 2) is responsible for detecting "horse"

**But there's a catch... a big one.**

What happens if the center of *two* objects falls into the same grid cell? Imagine a person standing directly in front of a car.

**YOLOv1's limitation: Each grid cell can only detect ONE object.**

This is a fundamental rule of the original YOLO. Even though each cell will propose two bounding boxes (we'll get to that), it can only have one final opinion on the class of the object. It's like a square on the Battleship board that can only shout "Hit!" for one ship, even if two ships happen to cross over it.

This is YOLOv1's biggest weakness. It struggles with crowds or flocks of birds where multiple small objects are clustered together. This is a key problem that later versions of YOLO were desperate to solve. For now, just remember: one cell, one final vote.

**Fixed in YOLOv2+**: Anchor boxes allow multiple detections per cell

**Impact**: Maximum 49 objects per image (7Ã—7 grid). Crowded scenes (flocks of birds, dense crowds) will have missed detections.

---

## Bounding Box Format and Encoding

Ground truth bounding box for person: (100, 200, 130, 202)

- These are large absolute values
- Hard for the network to predict directly
- Solution: **Make predictions relative to the grid cell**

### Target Encoding (Ground Truth â†’ YOLO Format)

For a bounding box with center (x, y), width w, height h:

**Center Coordinates (relative to grid cell):**

```text
x' = (x - x_anchor) / cell_width
y' = (y - y_anchor) / cell_height
```

Where:

- `(x_anchor, y_anchor)` = top-left corner of the grid cell
- `cell_width` = `cell_height` = 64 (for 448/7 grid)

**Intuition**: If I'm standing at the top-left corner of the grid cell, how far do I need to move (as a fraction of the cell size) to reach the object's center?

**Width & Height (relative to entire image):**

```text
w' = w / image_width
h' = h / image_height
```

Where `image_width = image_height = 448`

**Intuition**: Express the box size as a fraction of the total image size.

![Center Calculations](/assets/images/yolo/calculations_on_cneter.png)
*Figure: Computing relative offsets (Î”x, Î”y) from grid cell top-left corner to object center*

![More on Center Encoding](/assets/images/yolo/more_on_center.png)
*Figure: Detailed breakdown of how center coordinates are normalized relative to grid cell dimensions*

![XY Center Coordinates](/assets/images/yolo/x,ycenter.png)
*Figure: Visualizing the x,y center point encoding process with actual coordinate values*

![Normalized Values](/assets/images/yolo/normalised_val_after_calc.png)
*Figure: Final normalized ground truth values (x', y', w', h') ready for training*

### Label Encoding for Training

For each grid cell, we create a target vector:

**If cell contains an object:**

- Bounding box: `(x', y', w', h', confidence=1.0)`
- Class: One-hot encoding `[0, 0, 1, 0, ..., 0]` (20 values for Pascal VOC)

**If cell contains no object:**

- All zeros: `(0, 0, 0, 0, 0, 0, 0, ..., 0)` (30 values total)

![YOLO Target Format](/assets/images/yolo/yolo_targets.png)
*Figure: 30-dimensional vector format with bounding box coordinates, confidence, and class probabilities*

---

## Prediction Vector

### What Does Each Grid Cell Predict?

Each grid cell outputs a **30-dimensional vector**:

**Bounding Boxes (B=2 boxes):**

- Box 1: `(xâ‚', yâ‚', wâ‚', hâ‚', câ‚)` â€” 5 values
- Box 2: `(xâ‚‚', yâ‚‚', wâ‚‚', hâ‚‚', câ‚‚)` â€” 5 values

Where:

- `x', y'` = center offset relative to top-left corner of grid cell (0 to 1)
- `w', h'` = width/height relative to entire image (0 to 1)
- `c` = **objectness confidence** = Pr(Object) Ã— IOU_pred^truth

**Class Probabilities (C=20 for Pascal VOC):**

- `[pâ‚, pâ‚‚, ..., pâ‚‚â‚€]` â€” 20 values
- These are **conditional probabilities**: Pr(Class_i | Object)
- "What is the probability of each class, **given that** an object exists in this cell?"

Total: 5Ã—2 + 20 = 30 values per grid cell

> For the entire image: 7Ã—7Ã—30 = 1,470 values

### Why Predict 2 Boxes per Cell?

**The Problem**: Some objects may have different aspect ratios or sizes.

**The Solution**: Each cell predicts 2 boxes, and during training, we assign each ground truth object to the box with the **highest IOU** with that object.

**At inference**: We keep only the box with the **highest confidence** score for each cell.

### Understanding Confidence Score

The confidence `c` encodes two things:

1. **Pr(Object)**: Probability that the box contains an object
2. **IOU_pred^truth**: How well the predicted box fits the object

```text
c = Pr(Object) Ã— IOU_pred^truth
```

**Three scenarios:**

- **No object in cell**: c = 0
- **Object present, poor fit**: c = low (e.g., 0.3)
- **Object present, good fit**: c = high (e.g., 0.9)

![Class Confidence Scores](/assets/images/yolo/class_conf.png)
*Figure: How class probabilities are combined with objectness confidence to produce final class-specific confidence scores*

---

## Inference: From Predictions to Bounding Boxes

### Step 1: Get Raw Predictions

Network outputs: 7Ã—7Ã—30 tensor

For grid cell (i, j):

- Box 1: `(xâ‚', yâ‚', wâ‚', hâ‚', câ‚)`
- Box 2: `(xâ‚‚', yâ‚‚', wâ‚‚', hâ‚‚', câ‚‚)`
- Classes: `[pâ‚, pâ‚‚, ..., pâ‚‚â‚€]`

### Step 2: Convert to Image Coordinates

**Center coordinates:**

```python
x_anchor, y_anchor = i * 64, j * 64  # top-left of grid cell
xâ‚ = xâ‚' * 64 + x_anchor
yâ‚ = yâ‚' * 64 + y_anchor
```

**Width and height:**

```python
wâ‚ = wâ‚' * 448
hâ‚ = hâ‚' * 448
```

Now we have absolute pixel coordinates for the bounding box.

![Converting Coordinates](/assets/images/yolo/gt_coord.png)
*Figure: Converting relative YOLO format coordinates back to absolute pixel coordinates for visualization*

### Step 3: Compute Class-Specific Confidence

For each box, multiply objectness by class probabilities:

```python
class_confidence = c Ã— max(pâ‚, pâ‚‚, ..., pâ‚‚â‚€)
```

**Example:**

- Box 1 confidence: `câ‚ = 0.85`
- Highest class probability: `pâ‚â‚„ = 0.9` (person class)
- Final confidence: `0.85 Ã— 0.9 = 0.765`

### Step 4: Select Best Box per Cell

Each cell predicted 2 boxes â†’ keep the one with **highest class-specific confidence**.

**Example:**

- Box 1: `câ‚' = 0.765`
- Box 2: `câ‚‚' = 0.621`
- **Keep Box 1, discard Box 2**

![Ground Truth Confidence](/assets/images/yolo/gt_conf.png)
*Figure: Target confidence values and how they're computed from IOU between predicted and ground truth boxes*

### Step 5: Filter by Confidence Threshold

Remove all boxes with confidence below a threshold (e.g., 0.5):

```python
final_boxes = [box for box in boxes if box.confidence > 0.5]
```

### Step 6: Apply NMS

Many grid cells may detect the same object â†’ use Non-Maximum Suppression to remove duplicates.

---

## Post-Processing: IOU and NMS

### Intersection over Union (IOU)

**What is IOU?**
A measure of overlap between two bounding boxes. It tells you how well two boxes align.

**Formula:**

```text
IOU = Area of Intersection / Area of Union
```

**Interpretation:**

- IOU = 0.0 â†’ No overlap
- IOU = 0.5 â†’ Decent overlap
- IOU = 0.8+ â†’ Excellent overlap (typically considered a correct detection)

![IOU Visualization](/assets/images/yolo/iou.png)
*Figure: Intersection over Union (IOU) calculation showing overlap between predicted box (blue) and ground truth box (green), with intersection area highlighted*

### Converting Between Box Formats

**Center format**: `(x_center, y_center, width, height)`
**Corner format**: `(xâ‚, yâ‚, xâ‚‚, yâ‚‚)` where (xâ‚,yâ‚) = top-left, (xâ‚‚,yâ‚‚) = bottom-right

**Conversion:**

```python
xâ‚ = x_center - width/2
yâ‚ = y_center - height/2
xâ‚‚ = x_center + width/2
yâ‚‚ = y_center + height/2
```

### Computing IOU

**Step 1**: Find intersection rectangle coordinates

```python
x_left = max(xâ‚_box1, xâ‚_box2)
y_top = max(yâ‚_box1, yâ‚_box2)
x_right = min(xâ‚‚_box1, xâ‚‚_box2)
y_bottom = min(yâ‚‚_box1, yâ‚‚_box2)
```

**Step 2**: Compute intersection area

```python
if x_right < x_left or y_bottom < y_top:
    intersection = 0  # Boxes don't overlap
else:
    intersection = (x_right - x_left) * (y_bottom - y_top)
```

**Step 3**: Compute union area

```python
area_box1 = (xâ‚‚_box1 - xâ‚_box1) * (yâ‚‚_box1 - yâ‚_box1)
area_box2 = (xâ‚‚_box2 - xâ‚_box2) * (yâ‚‚_box2 - yâ‚_box2)
union = area_box1 + area_box2 - intersection
```

**Step 4**: Compute IOU

```python
iou = intersection / union
```

![Bounding Box Formats](/assets/images/yolo/x1,y1,x2,y2.png)
*Figure: Two common bounding box representations - corner format (xâ‚,yâ‚,xâ‚‚,yâ‚‚) vs center format (x_center, y_center, w, h) and how to convert between them*

### IOU Implementation

```python
def iou(pred, gt):
    """
    Calculate Intersection over Union between two bounding boxes.

    Args:
        pred: [x1, y1, x2, y2] predicted box
        gt: [x1, y1, x2, y2] ground truth box

    Returns:
        iou: float between 0 and 1
    """
    pred_x1, pred_y1, pred_x2, pred_y2 = pred
    gt_x1, gt_y1, gt_x2, gt_y2 = gt

    # Find intersection rectangle
    x_topleft = max(pred_x1, gt_x1)
    y_topleft = max(pred_y1, gt_y1)
    x_bottomright = min(pred_x2, gt_x2)
    y_bottomright = min(pred_y2, gt_y2)

    # Check if boxes overlap at all
    if x_bottomright < x_topleft or y_bottomright < y_topleft:
        return 0.0

    # Calculate areas
    intersection = (x_bottomright - x_topleft) * (y_bottomright - y_topleft)
    pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
    union = pred_area + gt_area - intersection

    iou = intersection / union
    return iou
```

### Non-Maximum Suppression (NMS)

**The Problem**: Multiple grid cells often detect the same object, creating duplicate/redundant boxes.

**The Solution**: Keep only the box with the highest confidence and remove all others that significantly overlap with it.

![NMS Process](/assets/images/yolo/nms.png)
*Figure: Non-Maximum Suppression in action - before (left) showing multiple overlapping boxes per object, and after (right) showing only the highest confidence box kept for each object*

### NMS Algorithm

For each class separately:

1. **Sort** all detections by confidence (descending)
2. **Select** the box with highest confidence
3. **Remove** all boxes with `IOU > threshold` (e.g., 0.5) with the selected box
4. **Repeat** until no boxes remain

**Why per-class?** We want to suppress duplicate detections of the same object, but not suppress a person detection because it overlaps with a car detection.

> **Picking NMS Threshold**
>
> Choosing the wrong IOU threshold for NMS drastically affects performance.
>
> **Threshold too low** (e.g., 0.3):

> - Suppresses too many boxes
> - **Result**: Misses objects that are close together
> - **Example**: Two people standing next to each other â†’ only one detected
>
> **Threshold too high** (e.g., 0.7):

> - Suppresses too few boxes
> - **Result**: Multiple boxes per object (duplicates)
> - **Example**: Single person gets 3 bounding boxes
>
> **Paper uses 0.5**:

> - Balances duplicate suppression and multi-object detection
> - Works well for most scenarios
>
> **NMS Intuition**: Imagine a talent show where multiple judges rate the same performance. NMS is like keeping only the judge with the highest score and ignoring others who rated the same performance. This prevents counting the same act multiple times.

### NMS Implementation

```python
def nms(detections, nms_threshold=0.5):
    """
    Apply Non-Maximum Suppression to remove duplicate detections.

    Args:
        detections: list of [x1, y1, x2, y2, score]
        nms_threshold: IOU threshold for suppression

    Returns:
        keep_detections: filtered list of detections
    """
    # Sort detections by score (descending)
    sorted_det = sorted(detections, key=lambda k: -k[-1])
    keep_detections = []

    while len(sorted_det) > 0:
        # Keep the highest confidence box
        best_box = sorted_det[0]
        keep_detections.append(best_box)

        # Remove this box and all boxes with high overlap
        sorted_det = [
            box for box in sorted_det[1:]
            if iou(best_box[:-1], box[:-1]) < nms_threshold
        ]

    return keep_detections
```

---

## Training Process

### Dataset: Pascal VOC

**Pascal VOC 2007 + 2012**:

- 20 object classes
- Contains images with bounding box annotations
- Standard benchmark for object detection

### Two-Stage Training Strategy

**Why two stages?** To help the network learn useful features first, then fine-tune for detection.

#### Stage 1: Pretraining (Classification)

- **Task**: Image classification on ImageNet
- **Input size**: 224Ã—224
- **Network**: First 20 convolutional layers + 1 FC layer
- **Goal**: Learn general visual features
- **Why?** Reduces training time and improves convergence

#### Stage 2: Detection Training

- **Task**: Object detection on Pascal VOC
- **Input size**: 448Ã—448 (doubled for finer localization)
- **Network**: Full 24 conv layers + 2 FC layers
- **Why higher resolution?** Detection requires fine-grained spatial information

### Training Hyperparameters

**Epochs**: ~135 epochs

**Batch size**: 64

**Optimizer**: SGD with momentum

- Momentum: 0.9
- Weight decay: 0.0005

**Learning rate schedule**:

- Epochs 1-5: Warm-up from 10â»Â³ to 10â»Â² (prevents divergence from unstable gradients)
- Epochs 6-75: 10â»Â² (main training)
- Epochs 76-105: 10â»Â³ (fine-tuning)
- Epochs 106-135: 10â»â´ (final refinement)

**Why warm-up?** Starting with high learning rate causes unstable gradients and divergence. Gradual warm-up stabilizes training.

### Data Augmentation

To prevent overfitting and improve generalization:

1. **Random scaling and translation**: up to 20% of original image size
2. **Random HSV adjustment**:
   - Exposure and saturation: up to 1.5Ã— factor
   - Helps with lighting variations

3. **Dropout**: Rate = 0.5 after first FC layer

### Annotation Format Conversion

**Original annotation**: `[x_min, y_min, x_max, y_max, class_id]`

**YOLO format conversion**:

```python
# Calculate center and size
center_x = (x_min + x_max) / 2
center_y = (y_min + y_max) / 2
width = x_max - x_min
height = y_max - y_min

# Normalize to 0-1 range
center_x_norm = center_x / image_width
center_y_norm = center_y / image_height
width_norm = width / image_width
height_norm = height / image_height

# Find which grid cell this object belongs to
grid_x = int(center_x_norm * 7)
grid_y = int(center_y_norm * 7)

# Calculate offsets relative to grid cell
x_offset = (center_x_norm * 7) - grid_x  # 0 to 1
y_offset = (center_y_norm * 7) - grid_y  # 0 to 1

# Create target: [x_offset, y_offset, width_norm, height_norm, 1.0, class_one_hot]
```

---

## Going back to Loss Function

### Why Regression Loss?

Object detection in YOLO is formulated as a **regression problem**: we're predicting continuous values (box coordinates, confidence scores, class probabilities).

The loss function uses **Mean Squared Error (MSE)** for all components, but with careful weighting to handle class imbalance and scale differences.

### Three Components of Loss

1. **Localization Loss**: Ensures predicted box coordinates match ground truth
2. **Confidence Loss**: Trains confidence scores to reflect object presence and fit quality
3. **Classification Loss**: Ensures correct class probabilities for cells with objects

**Total Loss** = Î»_coord Ã— L_box + L_conf + L_class

### The Class Imbalance Problem

**Problem**: In most images, most grid cells don't contain objects.

- ~2-5 cells with objects
- ~44-47 cells without objects

**Impact**:

- Gradients from "no object" cells dominate training
- Overwhelms gradients from "object" cells
- Network learns to predict "no object" everywhere

**Solution**: Weight the losses differently

- **Î»_coord = 5**: Increase weight of box coordinate loss
- **Î»_noobj = 0.5**: Decrease weight of "no object" confidence loss

> **Intuition: Why Î»_coord = 5 and Î»_noobj = 0.5?**
>
> **The Numbers Game**:

> - Typical image: 2-3 cells with objects, 46-47 cells without objects
> - Ratio: ~20:1 (no-object : object)
>
> **Without weighting** (all Î» = 1):

> ```text
> Total confidence loss = 2 Ã— (object conf loss) + 47 Ã— (no-object conf loss)
>                      â‰ˆ 2 Ã— 0.5 + 47 Ã— 0.1 = 5.7
>                      â‰ˆ 82% from no-object cells!
> ```

> The "no object" gradient overwhelms "object" gradient â†’ network learns to predict "no object" everywhere.
>
> **With weighting** (Î»_coord=5, Î»_noobj=0.5):

> ```text
> Box coord loss:     5 Ã— (2 cells Ã— coord error) = 5 Ã— 1.0 = 5.0
> Object conf loss:   1 Ã— (2 cells Ã— conf error) = 1 Ã— 0.5 = 0.5
> No-obj conf loss:   0.5 Ã— (47 cells Ã— conf error) = 0.5 Ã— 4.7 = 2.35
> Total â‰ˆ 7.85, where localization now matters most.
> ```
>
> **Why these specific values?**

> - **Î»_coord = 5**: Emphasizes getting boxes right (5Ã— more important than classification)
> - **Î»_noobj = 0.5**: De-emphasizes empty cells (2Ã— less important than object cells)
> - Balance found empirically through ablation studies in the paper
>
> **Intuition**: Like a teacher grading homework â€” give more points for correct answers (Î»_coord=5), fewer points for saying "I don't know" (Î»_noobj=0.5).

> **Wrong Lambda Values**
>
> **Mistake #1**: Using Î»_coord = 1 (same as others)

> - **Result**: Network struggles with localization. Boxes are detected but poorly positioned.
> - **Symptom**: Low IOU scores even when objects are detected
>
> **Mistake #2**: Using Î»_noobj = 1 (same as object cells)

> - **Result**: No-object gradient dominates, network predicts low confidence for everything
> - **Symptom**: Very few detections, even on objects that are clearly visible
>
> **Mistake #3**: Using Î»_coord = 10 (too high)

> - **Result**: Network focuses only on boxes, ignores classification
> - **Symptom**: Good IOU but wrong class labels
>
> **How to tune**: Start with paper values (5, 0.5). Only adjust if you have domain-specific reasons (e.g., small dataset â†’ increase Î»_coord to 7).

![Loss Penalty Weighting](/assets/images/yolo/loss_penalty.png)
*Figure: Visualization of class imbalance problem - most grid cells (gray) contain no objects while only a few (colored) contain objects, showing why we need Î» weighting factors*

### 1. Localization Loss (Box Regression)

**Goal**: Make predicted box coordinates close to ground truth.

**Formula**:

```text
L_box = Î»_coord Ã— Î£ Î£ ğŸ™áµ¢â±¼áµ’áµ‡Ê² [(xáµ¢ - xÌ‚áµ¢)Â² + (yáµ¢ - Å·áµ¢)Â²]
        + Î»_coord Ã— Î£ Î£ ğŸ™áµ¢â±¼áµ’áµ‡Ê² [(âˆšwáµ¢ - âˆšÅµáµ¢)Â² + (âˆšháµ¢ - âˆšÄ¥áµ¢)Â²]
```

Where:

- `ğŸ™áµ¢â±¼áµ’áµ‡Ê²` = 1 if cell i contains an object and box j is responsible for it
- `(x, y, w, h)` = ground truth box coordinates (YOLO format)
- `(xÌ‚, Å·, Åµ, Ä¥)` = predicted box coordinates
- **Responsible box** = the box with highest IOU with ground truth

**Why square root of w and h?**

**Problem**: MSE treats errors equally regardless of box size.

- 10-pixel error in 100Ã—100 box = small mistake
- 10-pixel error in 20Ã—20 box = huge mistake

**Solution**: Predict âˆšw and âˆšh instead of w and h directly.

- Small boxes: âˆšw changes more for same absolute change
- Large boxes: âˆšw changes less for same absolute change
- This makes errors more balanced across different box sizes

**Example**:

- Box 1: w=100, âˆšw=10, error of 10 â†’ âˆš110 - 10 = 0.49
- Box 2: w=25, âˆšw=5, error of 10 â†’ âˆš35 - 5 = 0.92 (penalized more)

> **Intuition: Why âˆšw and âˆšh?**
>
> **The Problem**: Absolute errors matter more for small objects.
>
> Imagine two prediction errors:

> - **Large box** (200Ã—200 pixels): Predicted width = 210, GT = 200 â†’ error = 10 pixels
> - **Small box** (20Ã—20 pixels): Predicted width = 30, GT = 20 â†’ error = 10 pixels
>
> Both have the same **absolute error** (10 pixels), but the small box error is **much worse** (50% error vs 5% error)!
>
> **Without âˆš**: MSE treats both equally â†’ Loss = 10Â² = 100 for both.
>
> **With âˆš**: Square root scaling makes errors proportional:

> ```text
> Large box: (âˆš210 - âˆš200)Â² = (14.49 - 14.14)Â² = 0.12
> Small box: (âˆš30 - âˆš20)Â² = (5.48 - 4.47)Â² = 1.02
> ```
> Now the small box error is ~8Ã— larger in the loss â†’ Network learns precision for small objects!
>
> **Geometric intuition**: âˆš operation compresses large values more than small values, creating a more balanced loss landscape across different object sizes.

> **Forgetting the Square Root**
>
> **Mistake**: Predicting w and h directly instead of âˆšw and âˆšh
>
> **Consequence**: Network struggles to learn small objects accurately. Large objects dominate the loss, making the model biased toward bigger objects.
>
> **How to spot**: If your model detects large objects well but completely misses small ones, check if you're applying sqrt() to the width and height predictions!
>
> **In code**: Always use `torch.sqrt(w)` in your target encoding and `torch.square(w_pred)` when converting back.

![All Loss Components](/assets/images/yolo/all_losses.png)
*Figure: The three components of YOLO loss function - localization (box coordinates), confidence (objectness), and classification (class probabilities)*

### 2. Confidence Loss

**Goal**: Train confidence scores to reflect:

1. Whether an object is present
2. How well the predicted box fits the object

**Formula**:

```text
L_conf = Î£ Î£ ğŸ™áµ¢â±¼áµ’áµ‡Ê² (Cáµ¢ - Äˆáµ¢)Â²
         + Î»_noobj Ã— Î£ Î£ ğŸ™áµ¢â±¼â¿áµ’áµ’áµ‡Ê² (Cáµ¢ - Äˆáµ¢)Â²
```

Where:

- `ğŸ™áµ¢â±¼áµ’áµ‡Ê²` = 1 if object exists and box j is responsible
- `ğŸ™áµ¢â±¼â¿áµ’áµ’áµ‡Ê²` = 1 if no object exists in cell i
- `C` = target confidence (1 if object, 0 if no object)
- `Äˆ` = predicted confidence
- `Î»_noobj = 0.5` = weight for "no object" cells

**Two parts**:

1. **Object cells**: Push confidence toward 1.0
2. **No-object cells**: Push confidence toward 0.0 (but weighted less)

**Target confidence for object cells**:

```text
C = Pr(Object) Ã— IOU_pred^truth = 1.0 Ã— IOU
```

So if predicted box has IOU=0.9 with ground truth, target confidence = 0.9.

![Loss Again](/assets/images/yolo/loss_again.png)
*Figure: Detailed breakdown of how each loss component is computed for a single grid cell prediction*

### 3. Classification Loss

**Goal**: Predict correct class probabilities for cells containing objects.

**Formula**:

```text
L_class = Î£ ğŸ™áµ¢áµ’áµ‡Ê² Î£ (páµ¢(c) - pÌ‚áµ¢(c))Â²
```

Where:
- `ğŸ™áµ¢áµ’áµ‡Ê²` = 1 if cell i contains an object
- `páµ¢(c)` = ground truth probability for class c (one-hot: 1 for correct class, 0 otherwise)
- `pÌ‚áµ¢(c)` = predicted probability for class c
- Sum over all C classes (20 for Pascal VOC)

**Important**: This loss is **only computed for cells that contain objects**.

**Example**:

- Cell contains a person (class 14)
- Target: `[0, 0, ..., 1, ..., 0]` (1 at position 14)
- Predicted: `[0.1, 0.05, ..., 0.8, ..., 0.02]`
- Loss: `Î£(target - pred)Â² = (0-0.1)Â² + ... + (1-0.8)Â² + ... = 0.05`

### Complete Loss Function

![Classification Loss](/assets/images/yolo/loss_class.png)
*Figure: Classification loss computation showing how class probabilities are compared with one-hot encoded ground truth labels*

![Complete Loss Function](/assets/images/yolo/entire_loss.png)
*Figure: The complete YOLO loss function combining all three components with their respective weighting factors*

![Loss Equation Final](/assets/images/yolo/loss_eqn_final.png)
*Figure: Mathematical formulation of the complete loss function with indicator functions and summations*

![All Losses Equation](/assets/images/yolo/all_losses_eqn.png)
*Figure: All three loss components shown together with their mathematical expressions and weighting parameters*

```text
L = Î»_coord Ã— L_box + L_conf + L_class

Where:
- Î»_coord = 5.0
- Î»_noobj = 0.5
```

![Recap of Loss](/assets/images/yolo/loss_final.png)

### Training Process with Loss

**Forward pass**:

1. Image â†’ Network â†’ Predictions (7Ã—7Ã—30)
2. For each grid cell, compare predictions to targets
3. Compute three loss components
4. Sum weighted losses

**Backward pass**:
5. Compute gradients via backpropagation
6. Update weights using SGD with momentum

**Key insight**: The indicator functions `ğŸ™áµ¢â±¼áµ’áµ‡Ê²` ensure we only compute loss for relevant predictions:

- Box coordinates: only for responsible boxes
- Confidence: for all boxes (but weighted differently)
- Classification: only for cells with objects

---

## The Final Exam: How Do We Grade an Object Detector? (mAP)

So, our model is trained and it's making predictions. How do we know if it's any good? We need a final exam. In object detection, the standard "exam" is called **mAP (mean Average Precision)**.

To understand mAP, we first need to understand two simpler concepts: Precision and Recall.

Imagine our model is taking a test. The test image has 10 dogs in it. The model detects 8 boxes that it claims are dogs.

- Of those 8 boxes, 6 are actually correct (they overlap a real dog with an IOU > 0.5). These are **True Positives(TP)**  
- The other 2 boxes were mistakes (e.g., it drew a box around a bush). These are **False Positives (FP)**
- The model completely missed 4 of the real dogs. These are **False Negatives (FN)**.

### Understanding Detection Metrics

**Three types of predictions:**

1. **True Positive (TP)**: Predicted box matches ground truth (IOU â‰¥ threshold)
2. **False Positive (FP)**: Predicted box doesn't match any ground truth
3. **False Negative (FN)**: Ground truth object that wasn't detected

### Precision and Recall

With these numbers, we can ask two questions:

1.**Precision: "Of the answers you gave, how many were right?"**
    - `Precision = TP / (TP + FP) = 6 / (6 + 2) = 75%`
    - This measures how trustworthy the model's predictions are. A high precision model doesn't make many silly mistakes.

2.**Recall: "Of all the things you *should* have found, how many did you find?"**
    - `Recall = TP / (TP + FN) = 6 / (6 + 4) = 60%`
    - This measures how comprehensive the model is. A high recall model doesn't miss much.

> The Inevitable Trade-off

There's a constant tug-of-war between precision and recall.

- If you're very timid and only make a prediction when you're 100% sure, you'll have high precision but low recall (you'll miss a lot).
- If you're very bold and make tons of guesses, you'll have high recall but low precision (you'll have a lot of junk predictions).

### So, What is mAP?

A good model needs to be both precise and comprehensive. To measure this, we can't just look at one point. We need to see how precision and recall change as we adjust our confidence threshold.

**Average Precision (AP)** is a single number that summarizes this trade-off for a *single class* (like "dog"). It's the area under the precision-recall curve. A high AP means the model maintains high precision even as it tries to find more objects (i.e., as recall increases).

**Precision-Recall Curve**: Plot precision vs recall at different confidence thresholds.

**Steps to calculate AP**:

1. Sort all predictions by confidence
2. For each confidence threshold, compute precision and recall
3. Plot the curve
4. Compute area under curve

**Mean Average Precision (mAP)** is simply the average of the AP scores across all classes. If you have 20 classes, you calculate the AP for dogs, the AP for cats, the AP for cars, and so on, and then average them all together.

```text
mAP = (AP_class1 + AP_class2 + ... + AP_class20) / 20
```

This gives us a single, comprehensive number to judge the overall performance of our model. It's the final GPA for our object detector.

**mAP@0.5**: Use IOU threshold of 0.5 to determine TP
**mAP@0.75**: Use IOU threshold of 0.75 (stricter)
**mAP@[0.5:0.95]**: Average mAP across IOU thresholds from 0.5 to 0.95 in steps of 0.05

### mAP Implementation

```python
def compute_map(pred_boxes, gt_boxes, iou_threshold=0.5):
    """
    Calculate mean Average Precision for object detection.

    Args:
        pred_boxes: List of predictions per image
                   [{class: [[x1,y1,x2,y2,score], ...], ...}, ...]
        gt_boxes: List of ground truths per image
                 [{class: [[x1,y1,x2,y2], ...], ...}, ...]
        iou_threshold: IOU threshold to consider a detection as TP

    Returns:
        mean_ap: Mean average precision across all classes
        all_aps: Dictionary of AP for each class
    """
    # Get all class labels from ground truth
    gt_labels = set()
    for im_gt in gt_boxes:
        for cls_key in im_gt.keys():
            gt_labels.add(cls_key)
    gt_labels = sorted(list(gt_labels))

    all_aps = {}
    aps = []

    # Compute AP for each class
    for label in gt_labels:
        # Collect all predictions for this class across all images
        cls_preds = []
        for im_idx, im_pred in enumerate(pred_boxes):
            if label in im_pred:
                for box in im_pred[label]:
                    cls_preds.append((im_idx, box))

        # Sort predictions by confidence (descending)
        cls_preds = sorted(cls_preds, key=lambda k: -k[1][-1])

        # Track which GT boxes have been matched
        gt_matched = [[False for _ in im_gts.get(label, [])]
                      for im_gts in gt_boxes]

        # Count total GT boxes for this class (for recall)
        num_gts = sum([len(im_gts.get(label, [])) for im_gts in gt_boxes])

        # Track TP and FP for each prediction
        tp = [0] * len(cls_preds)
        fp = [0] * len(cls_preds)

        # For each prediction, determine if TP or FP
        for pred_idx, (im_idx, pred_box) in enumerate(cls_preds):
            # Get GT boxes for this image and class
            im_gts = gt_boxes[im_idx].get(label, [])

            # Find best matching GT box
            max_iou_found = -1
            max_iou_gt_idx = -1
            for gt_box_idx, gt_box in enumerate(im_gts):
                gt_box_iou = iou(pred_box[:-1], gt_box)
                if gt_box_iou > max_iou_found:
                    max_iou_found = gt_box_iou
                    max_iou_gt_idx = gt_box_idx

            # TP only if IOU >= threshold AND GT box hasn't been matched yet
            if max_iou_found < iou_threshold or \
               (max_iou_gt_idx >= 0 and gt_matched[im_idx][max_iou_gt_idx]):
                fp[pred_idx] = 1
            else:
                tp[pred_idx] = 1
                gt_matched[im_idx][max_iou_gt_idx] = True

        # Compute cumulative TP and FP
        tp = np.cumsum(tp)
        fp = np.cumsum(fp)

        # Compute precision and recall at each threshold
        recalls = tp / num_gts
        precisions = tp / (tp + fp)

        # Smooth precision curve (ensures precision is monotonic)
        # Add boundary values
        recalls = np.concatenate(([0.0], recalls, [1.0]))
        precisions = np.concatenate(([0.0], precisions, [0.0]))

        # Make precision monotonically decreasing
        for i in range(precisions.size - 1, 0, -1):
            precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])

        # Compute AP as area under curve
        # Get points where recall changes
        i = np.where(recalls[1:] != recalls[:-1])[0]
        ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])

        if num_gts > 0:
            aps.append(ap)
            all_aps[label] = ap
        else:
            all_aps[label] = np.nan

    mean_ap = sum(aps) / len(aps) if len(aps) > 0 else 0.0
    return mean_ap, all_aps
```

---

## Performance and Results

### Speed: Real-Time Detection

**YOLOv1 Performance**:

- **45 fps** on Nvidia Titan X GPU
- **155 fps** for Fast-YOLO (9 conv layers instead of 24)
- This is 9Ã— faster than previous state-of-the-art (Faster R-CNN at ~5 fps)

**Why so fast?**

1. **Single forward pass**: No region proposals, no batch processing
2. **Unified architecture**: One network handles everything
3. **Efficient design**: Optimized from the ground up for speed

### Accuracy Comparison

**YOLOv1 on Pascal VOC 2007**:

- **mAP**: 63.4%
- **Fast-YOLO**: 52.7% mAP

**Comparison with other methods**:

- **Faster R-CNN**: ~70% mAP, but only 7 fps
- **DPM** (Deformable Part Model): ~30% mAP, slower than YOLO
- **R-CNN**: Higher accuracy, but 47 seconds per image

**The trade-off**:

- YOLO sacrifices ~7% mAP for 9Ã— speed improvement
- Enables entirely new applications (real-time video, embedded systems)

### Key Advantages Over Two-Stage Detectors

1. **Global reasoning**: YOLO sees the entire image during prediction
   - Understands context (less likely to classify background as object)
   - Fewer background false positives than Fast R-CNN

2. **Generalization**: Better performance on new domains
   - Learns generalizable features
   - Better transfer to artwork, sketches, etc.

3. **Simplicity**: Single network, end-to-end training
   - No separate proposal generation
   - Unified loss function
   - Easier to optimize

4. **Real-time performance**: 45+ fps enables:
   - Live video analysis
   - Robotics and autonomous vehicles
   - Interactive applications

![YOLOv1 Results](/assets/images/yolo/results.png)
*Figure: Performance comparison of YOLOv1 with other detection methods on Pascal VOC 2007 dataset*

![YOLOv1 Detailed Results](/assets/images/yolo/results2.png)
*Figure: Detailed accuracy and speed metrics showing YOLO's superiority in real-time performance while maintaining competitive accuracy*

---

## Limitations of YOLOv1

### 1. Limited Objects per Grid Cell

**Constraint**: Each grid cell can only predict **one object** (despite predicting 2 boxes).

- Maximum detections: 7Ã—7 = **49 objects** per image

**Problem**: Fails in crowded scenarios

- Flocks of birds
- Dense crowds
- Multiple small objects in one cell

**Why?** Each cell predicts only **one set of class probabilities**, shared by both boxes.

**Example failure case**:

- Grid cell contains centers of both a dog and bicycle
- Cell can only classify as one class
- One object will be missed

### 2. Struggles with Small Objects

**Problem**: Small objects (especially in groups) are hard to detect.

**Why?**

- 7Ã—7 grid is coarse (each cell = 64Ã—64 pixels)
- Small objects may not strongly activate any single cell
- Multiple small objects may share a grid cell

**Example**: A flock of small birds in the sky.

### 3. Unusual Aspect Ratios

**Problem**: Objects with uncommon shapes or aspect ratios are hard to detect.

**Why?**

- Network learns aspect ratio priors from training data
- If training data contains mostly cars that are 2:1 (width:height)
- Long, thin cars or unusually shaped objects will be missed

**Example**: Very long limousine, tall narrow doorway

### 4. Localization Errors

**Problem**: Box coordinates are less accurate than two-stage detectors.

**Why?**

- Coarse feature map (7Ã—7) for final prediction
- No iterative refinement (unlike Faster R-CNN)
- Especially affects small objects

**Impact**: Lower IOU values, affects mAP at higher IOU thresholds

### 5. Box Size Sensitivity

**Problem**: Despite using âˆšw and âˆšh, small vs large box errors still not perfectly balanced.

**Why?** MSE fundamentally treats all errors equally in loss calculation.

### 6. Class Imbalance Despite Weighting

**Problem**: Even with Î»_noobj = 0.5, "no object" cells still dominate.

**Why?**

- Typically 47 cells without objects vs 2-3 with objects
- Still creates ~20Ã— more "no object" loss terms

**Impact**:

- Confidence scores may be systematically lower
- May miss objects in difficult scenarios

## Failure Cases: When YOLO Struggles

### Failure Case #1: Small Grouped Objects

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image: Flock of Birds in Sky       â”‚
â”‚                                      â”‚
â”‚    â€¢ â€¢ â€¢  â€¢ â€¢  â€¢  â€¢  â€¢ â€¢ â€¢          â”‚  â† 15 birds
â”‚   â€¢  â€¢  â€¢  â€¢ â€¢  â€¢  â€¢  â€¢             â”‚
â”‚  â€¢  â€¢  â€¢  â€¢ â€¢   â€¢  â€¢  â€¢   â€¢         â”‚
â”‚                                      â”‚
â”‚  YOLOv1 Detection:                   â”‚
â”‚  âœ“ 3 birds detected                  â”‚
â”‚  âœ— 12 birds missed!                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why it fails:**

- 7Ã—7 grid = only 49 possible detections maximum
- Small birds (5Ã—5 pixels each) â†’ multiple birds share grid cells
- Each cell can only detect ONE object (one set of class probs)
- Small objects have weak features at 7Ã—7 resolution

**YOLOv3 Fix:** Multi-scale detection (13Ã—13, 26Ã—26, 52Ã—52 grids) â†’ 8,000+ possible detections 

**YOLOv10 Fix:** NMS-free detection + multi-scale â†’ perfect for dense small objects 

### Failure Case #2: Unusual Aspect Ratios

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image: Stretch Limousine              â”‚
â”‚  â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚            â”‚  â† Very long car (10:1 ratio)
â”‚  â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€            â”‚
â”‚                                        â”‚
â”‚  YOLOv1 Prediction:                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”                â”‚  â† Two separate "car" boxes
â”‚  â”‚ Car â”‚ ...gap...â”‚ Car â”‚              â”‚     (misses it's one object)
â”‚  â””â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  âœ— Poor fit, low confidence            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why it fails:**

- Training data: mostly ~2:1 aspect ratio cars
- Limousine: 10:1 aspect ratio â†’ **out of distribution**
- Network hasn't learned to predict such extreme shapes
- Two grid cells detect parts separately

**YOLOv2 Fix:** Anchor boxes with multiple aspect ratios (1:1, 1:2, 2:1, 1:3, 3:1) 

**YOLOv10 Fix:** Anchor-free design adapts to any shape automatically 

### Failure Case #3: Crowded Scenes

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image: Dense Crowd (Concert)           â”‚
â”‚  ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤  â† 100+ people            â”‚
â”‚  ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤                           â”‚
â”‚  ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤                           â”‚
â”‚                                         â”‚
â”‚  YOLOv1 Detection:                      â”‚
â”‚  âœ“ 35 people detected                   â”‚
â”‚  âœ— 65 people missed!                    â”‚
â”‚  (7Ã—7 = 49 max, but overlap reduces)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why it fails:**

- Hard limit: 7Ã—7 = 49 grid cells
- Each cell: only 1 object
- Crowded scene: many people share cells
- Result: **Systematic undercounting**

**YOLOv3 Fix:** Finer grids (52Ã—52 = 2,704 cells) + 3 predictions per cell 

**YOLOv10 Fix:** No cell limit + NMS-free â†’ handles 100s of objects 

### Failure Case #4: Heavy Occlusion

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image: Overlapping Objects      â”‚
â”‚                                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”                     â”‚
â”‚      â”‚  A  â”‚ â† Person A          â”‚
â”‚    â”Œâ”€â”¼â”€â”€â”€â”€â”€â”¤                     â”‚
â”‚    â”‚Bâ”‚  A  â”‚ â† Person B (behind) â”‚
â”‚    â””â”€â”´â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚      Center of B is inside A     â”‚
â”‚                                  â”‚
â”‚  YOLOv1 Detection:               â”‚
â”‚  âœ“ Person A detected             â”‚
â”‚  âœ— Person B missed!              â”‚
â”‚  (Both centers in same cell)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why it fails:**

- Both object centers fall in same grid cell
- YOLO chooses higher confidence box
- Occluded object has lower features â†’ lower confidence â†’ ignored

**YOLOv2 Fix:** Multiple anchors per cell â†’ can detect both

**YOLOv10 Fix:** Dual heads (o2m + o2o) â†’ better occlusion handling

## Failure Rate Summary

| Scenario | YOLOv1 Performance | YOLOv10 Performance |
|----------|-------------------|---------------------|
| **Small grouped objects** (birds) | 20% recall | 90% recall |
| **Unusual aspect ratios** (limousine) | 40% IOU | 85% IOU |
| **Crowded scenes** (>50 objects) | 35% recall | 95% recall |
| **Heavy occlusion** (overlapping) | 50% recall | 85% recall |

> ğŸ’¡ **Key Takeaway**: YOLOv1 excels at **sparse, medium-sized objects with typical aspect ratios**. For edge cases, use YOLOv3+ or YOLOv10.

---

## Evolution: From v1 to v10

### The YOLO Family Tree

YOLOv1 (2016) established the foundation, and subsequent versions addressed its limitations while maintaining the real-time performance philosophy.

### Key Improvements Across Versions

**YOLOv2 (YOLO9000) - 2017**:

- **Anchor boxes**: Instead of predicting boxes directly, predict offsets from predefined anchors
- **Batch normalization**: Added to all conv layers, improved convergence
- **High-resolution classifier**: Pretrain at 448Ã—448 (not 224Ã—224)
- **Multi-scale training**: Train on different input sizes
- **Better backbone**: Darknet-19 (19 conv layers + 5 maxpool)
- **Performance**: 67% mAP at 40 fps, 78.6% mAP at slower speeds

**YOLOv3 - 2018**:

- **Multi-scale predictions**: Detect at 3 different scales (better for small objects)
- **Better backbone**: Darknet-53 (53 conv layers, residual connections)
- **Binary classification**: Use logistic regression instead of softmax (allows multi-label)
- **Performance**: 57.9% mAP@0.5, comparable to RetinaNet but 3-4Ã— faster

**YOLOv4 - 2020**:

- **Bag of freebies**: Techniques that improve accuracy without increasing inference cost
  - Data augmentation (Mosaic, MixUp)
  - Label smoothing
  - DropBlock regularization
- **Bag of specials**: Techniques with small inference cost increase
  - Mish activation
  - CSPNet backbone
  - SPP (Spatial Pyramid Pooling)
  - PANet neck
- **Performance**: 43.5% mAP@0.5:0.95, state-of-the-art at the time

**YOLOv5 - 2020** (Ultralytics):

- **PyTorch implementation**: Easier to use and customize
- **Auto-learning anchors**: Automatically cluster anchors from training data
- **Model family**: Nano, Small, Medium, Large, XLarge variants
- **Better augmentations**: Albumentations integration
- **Export options**: ONNX, TensorRT, CoreML, etc.

**YOLOv6, v7, v8 - 2022-2023**:
- Various architectural improvements
- Better training strategies
- Improved backbone designs
- Enhanced data augmentation

**YOLOv10 - 2024**:

- **NMS-free detection**: Eliminates need for NMS post-processing
  - Consistent matching strategy during training
  - Dual label assignment
  - Significantly faster inference
- **Efficiency optimizations**:
  - Compact inverted block design
  - Partial self-attention
  - Spatial-channel decoupled downsampling
- **Model variants**: N/S/M/B/L/X for different speed/accuracy trade-offs
- **Performance**:
  - YOLOv10-X: ~54% mAP@0.5:0.95 on COCO
  - YOLOv10-N: Real-time on edge devices

### Major Paradigm Shifts

1. **v1â†’v2**: Direct prediction â†’ Anchor-based
2. **v2â†’v3**: Single-scale â†’ Multi-scale detection
3. **v3â†’v4**: Manual design â†’ Automated architecture search and bag-of-tricks
4. **v4â†’v5**: Darknet (C) â†’ PyTorch (Python), better tooling
5. **v8â†’v10**: Anchor-based â†’ Anchor-free, NMS â†’ NMS-free

### What Remained Constant

Despite 10 versions, the core YOLO philosophy persists:

- **Single-stage detection**: One network, one forward pass
- **Real-time performance**: Speed is a primary goal
- **End-to-end training**: Unified loss function
- **Practical focus**: Easy to deploy and use

### YOLOv10: Deep Dive into the Latest Version

YOLOv10 introduces two major innovations: **NMS-free detection** and **holistic efficiency-accuracy driven design**.

#### 1. NMS-Free Training with Dual Heads

**The Problem with NMS:**

- Traditional YOLO models generate multiple overlapping boxes per object
- Non-Maximum Suppression (NMS) is required as post-processing
- NMS adds latency and isn't differentiable (can't be optimized end-to-end)

**YOLOv10's Solution: Consistent Dual Label Assignment**

YOLOv10 uses **two detection heads** during training:

1. **One-to-Many (o2m) Head**:
   - Matches each ground truth object with **multiple predictions**
   - Provides rich supervision signal (explores diverse object locations)
   - Similar to traditional YOLO training
   - Analogy: *"I think the cat is here, here, and maybe here"*

2. **One-to-One (o2o) Head**:
   - Matches each ground truth object with **exactly one prediction**
   - Learns to give precise, single predictions
   - Used during inference (no NMS needed!)
   - Analogy: *"The cat is exactly here"*

**Consistent Matching Metric:**

Both heads use the same scoring formula to match predictions with ground truth:

```text
m(Î±, Î²) = spatial_prior Ã— classification_score^Î± Ã— IOU(bbox_pred, bbox_GT)^Î²
```

Where:

- `spatial_prior` (s): Whether the prediction point lies in/near the GT box (geometric constraint)
- `classification_score` (p): Predicted class confidence score
- `IOU`: Overlap between predicted and ground truth boxes (localization quality)
- `Î±, Î²`: Hyperparameters that balance the impact of classification score vs IOU

**Intuition**: The metric `m` scores "how good is this guess" â€” rewarding predictions that are both the right class (high p) and have good box overlap (high IOU). This ensures both classification accuracy and precise localization matter for matching.

**Why Consistent Matching?**

Using the **same metric** on both branches harmonizes the supervision of both heads. If we can align the supervision of the o2o head with that of the o2m head, we can optimize the o2o head toward the direction of o2m head's optimization. As a result, the o2o head provides improved quality samples during inference, leading to better performance.

**Understanding the Supervision Gap:**

Both heads produce classification targets, but in different forms:

- **o2m** â†’ distributes probability mass across **multiple predictions** (explores diverse locations)
- **o2o** â†’ puts all probability mass on a **single prediction** (precise, sharp target)

Naturally, these targets don't look the same â€” this mismatch is called the **supervision gap**. To measure how "far apart" the two supervision signals are, YOLOv10 uses the **1-Wasserstein distance**:

```text
Distance = "how much sand you need to move" to align distributions
```

**The Formula**: For denoting an instance's largest IoU as u*, and largest matching scores as m*(o2m) and m*(o2o):

```text
Wasserstein Distance = Î£ |target_o2m(i) - target_o2o(i)|
```

Where:

- **First term**: Difference at the shared best prediction index
- **Second term**: All leftover o2m mass on other predictions

**Key insight**: By using consistent matching, the best positive sample for the o2m head is also the best for the o2o head. Both heads optimize consistently and harmoniously â€” the metric ensures you don't need to move much "sand."

By minimizing this gap during training, the o2o head learns to provide **clean, single-box predictions** without needing NMS at inference.

**Training Optimization:**

During training, both heads are jointly optimized:

1. **o2m and o2o metrics** influence label assignments and supervision info for both heads
2. Both use the **consistent matching metric** to find the best matches
3. For o2m: picks multiple answers (predictions p1, p2, p3 boxes)
4. For o2o: picks single best answer (e.g., only p1)
5. Calculate **Wasserstein distance** between top o2m picks and top o2o pick
6. **Minimize loss function** that includes both heads' losses + the supervision gap

**Result**: The o2o head benefits from the rich supervision of o2m (exploring multiple locations) while learning to produce clean, NMS-free single predictions.

**At Inference:**

- Discard the o2m head completely (no extra cost!)
- Use only the o2o head â†’ **NMS-free predictions**
- Simply pick top-K class scores from [B, N, 4+nc] tensor â†’ done!
- For o2o branch, top-1 selection by the same metric (Hungarian matching not needed)

**Dry Run Example**: Detecting a Dog

1. **Training**: Model learns via o2m/o2o supervision how to score/classify/draw boxes
   - o2m equation spreads label mass across several positives (exploration)
   - o2o equation gives sharp target: single o2o positive gets full target, others get zero
   - Wasserstein distance measures gap between these two distributions

2. **Inference**: Model outputs raw tensor [B, N, 4+nc]
   - Preprocessing: Letterbox resize, normalize to [0,1], convert to CHW
   - Model forward pass: YOLOv10 produces predictions

3. **Postprocessing** (v10 NMS-free pipeline):
   - Convert xywh â†’ xyxy bbox format
   - Scale boxes back to original image (undo letterbox padding)
   - Pick top-K class scores from [N, 4+nc] predictions
   - No NMS needed! Just filter by confidence threshold

4. **Final Result**: Clean detection with one strong prediction per object
   - Dog box + label, no duplicate messy boxes
   - 40% faster than YOLOv8 (no NMS overhead)

#### 2. Architectural Innovations

**A. Lightweight Classification Head**

**Observation**: In YOLOv8:

- Classification head: 5.95G FLOPs, 1.51M params
- Regression head: 2.34G FLOPs, 0.64M params
- Classification is **2.5Ã— heavier** despite regression being more important for accuracy!

**Solution**: Redesign classification head using:

- Two **depthwise separable 3Ã—3 convolutions** (cheap spatial learning)
- One **1Ã—1 convolution** (channel mixing)
- Result: Significantly lower computational cost while maintaining performance

**B. Spatial-Channel Decoupled Downsampling**

**Traditional Approach**:
Single 3Ã—3 conv with stride=2 does both:

- Spatial reduction: HÃ—W â†’ H/2Ã—W/2
- Channel increase: C â†’ 2C
- Cost: **O(9/2 Ã— HÃ—WÃ—CÂ²)** â€” expensive!

**YOLOv10 Approach**: Decouple the operations:

1. **Pointwise conv (1Ã—1)**: Only changes channels (C â†’ 2C)
2. **Depthwise conv (3Ã—3, stride=2)**: Only reduces spatial dimensions

- New cost: **O(2Ã—HÃ—WÃ—CÂ² + 9/2Ã—HÃ—WÃ—C)** â€” much cheaper!

**C. Rank-Guided Block Design**

**The Problem**: Not all network stages are equally important â€” some are redundant.

**The Solution**:

1. Compute **intrinsic rank** of each stage's last convolution (using SVD)
2. Sort stages from lowest rank (most redundant) to highest
3. Replace redundant stages with **Compact Inverted Blocks (CIB)**:
   - Depthwise convs for efficient spatial mixing
   - Pointwise convs for cheap channel mixing
   - Much lighter than standard bottleneck blocks
4. Keep high-rank (important) stages strong

**For important (high-rank) stages**:

- Use **7Ã—7 large kernel convolutions** in deeper layers (bigger receptive field)
- Apply **Partial Self-Attention (PSA)** to half the channels (global context at low cost)

#### 3. Enhanced Backbone: CSPNet

**CSP (Cross-Stage Partial) Network** builds on DenseNet:

**DenseNet**: Each layer concatenates all previous layers:

```text
xâ‚ = wâ‚ Ã— xâ‚€
xâ‚‚ = wâ‚‚ Ã— [xâ‚€, xâ‚]
...
xâ‚– = wâ‚– Ã— [xâ‚€, xâ‚, ..., xâ‚–â‚‹â‚]
```

**CSPNet**: Separates feature map into two parts:

- Part 1: Goes through dense block + transition layer
- Part 2: Bypassed and combined later
- Reduces computation while maintaining rich feature representation

#### 4. Performance Results on big dataset

**Iteration 1**: 20k images (14k train, 3k val, 3k test)

- YOLOv10m: **mAP50-95 = 0.73**
- YOLOv8m (baseline): mAP50-95 = 0.69
- Classes: 8
- Batch size: 4

**Iteration 3**: 32k images (25k train, 3.5k val, 3.5k test)

- **YOLOv10m Final Results:**
  - **mAP50 = 0.906** (90.6% precision at IOU threshold 0.5)
  - **mAP50-95 = 0.76** (76% precision across IOU 0.5 to 0.95)
  - **Speed: 40% faster** than YOLOv8m
  - Majority classes achieve 0.9+ mAP (excellent performance)

**Key Improvements over YOLOv8:**

- **+4% mAP50-95** improvement (0.73 vs 0.69 baseline)
- **40% faster inference** (NMS-free architecture)
- Better performance on rare classes with more training data
- Cleaner predictions (no duplicate boxes to filter)

**Why the Improvement?**

1. **NMS-free**: Eliminates post-processing latency
2. **Efficient architecture**: Lighter heads, decoupled downsampling
3. **Better supervision**: Dual-head training with consistent matching
4. **Optimized blocks**: CIB for redundant stages, large kernels + PSA for important stages

#### 5. Deployment

**Model Export:**

- Best model: `best.pt` (PyTorch weights)
- Converted to ONNX for production deployment
- Custom preprocessing: Ultralytics letterbox (aspect ratio preserved, padded to stride=32)
- Custom postprocessing: `postprocess_nms_free` (top-K filtering, box scaling)

**Inference Pipeline:**

1. **Input**: RGB image from RTSP stream (any size)

2. **Preprocessing** (Ultralytics letterbox):
   - Keep aspect ratio, pad to multiple of stride=32
   - Convert to RGB, scale to [0,1]
   - Convert to CHW format (channels first)
   - Returns image tensor + scale factor + padding values (for later box mapping)

3. **Model Forward Pass**:
   - YOLOv10m processes image
   - Output: tensor of shape **[B, N, 4+nc]** or **[B, 4+nc, N]**
   - Where: B=batch, N=number of predictions, nc=number of classes
   - Boxes in xywh format (center x, center y, width, height - all scaled)
   - Scores are per-class probabilities

4. **Postprocessing** (`postprocess_nms_free` function):
   - **Accept both layouts**: [B, N, 4+nc] or [B, 4+nc, N] (works for NumPy arrays)
   - **xywh â†’ xyxy conversion**: Convert center format to corner format
   - **scale_boxes**: Undo letterbox transform (subtract pad, divide by ratio, clip to bounds)
   - **Top-K filtering**: Select predictions with highest class scores
   - **No NMS needed** â€” one prediction per object by design!
   - Returns: List of dicts per image `{boxes: [M,4], scores: [M], labels: [M]}`

5. **Output**: Clean detections ready for visualization/downstream use

#### Comparison: YOLOv1 vs YOLOv10

| Metric | YOLOv1 (2016) | YOLOv10 (2024) |
|--------|---------------|----------------|
| **mAP50** | 63.4% (VOC) | 90.6% (ODN dataset) |
| **mAP50-95** | Not reported | 76% |
| **Speed (fps)** | 45 fps | ~63 fps (40% faster than v8) |
| **Grid Size** | 7Ã—7 (fixed) | Multi-scale, adaptive |
| **Boxes per Cell** | 2 | Dynamic (o2o/o2m) |
| **Max Objects** | 49 (7Ã—7 grid limit) | Unlimited |
| **Post-processing** | NMS required | **NMS-free** |
| **Small Objects** | Struggles | Excellent (multi-scale detection) |
| **Architecture** | 24 conv + 2 FC | Efficient CSPNet + lightweight heads |
| **Training** | Single-head | **Dual-head** (o2m + o2o) |

**Key Takeaways:**

- YOLOv1 pioneered single-stage detection and real-time performance
- YOLOv10 perfects the vision: **truly end-to-end**, no post-processing, even faster
- Both maintain the core philosophy: simple, fast, effective

---

## Implementation: PyTorch Code Examples

This section provides a complete PyTorch implementation of YOLOv1, demonstrating how the concepts we've discussed translate into working code.

### 1. Model Architecture

The YOLO network uses a ResNet34 backbone (pretrained on ImageNet) followed by detection layers:

```python
import torch
import torch.nn as nn
import torchvision

class YOLOV1(nn.Module):
    """
    YOLOv1 Implementation using ResNet34 backbone

    Args:
        img_size: Input image size (448x448)
        num_classes: Number of classes (20 for Pascal VOC)
        model_config: Configuration dict with S, B, and architectural params

    Output:
        Tensor of shape (batch_size, S, S, 5*B + C)
    """
    def __init__(self, img_size, num_classes, model_config):
        super(YOLOV1, self).__init__()
        self.img_size = img_size
        self.S = model_config['S']  # Grid size (7x7)
        self.B = model_config['B']  # Boxes per cell (2)
        self.C = num_classes  # Number of classes (20)

        # Load pretrained ResNet34 backbone (trained on ImageNet 224x224)
        backbone = torchvision.models.resnet34(
            weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1
        )

        # Feature extraction layers (before FC layers)
        self.features = nn.Sequential(
            backbone.conv1,    # 7x7 conv, stride 2
            backbone.bn1,
            backbone.relu,
            backbone.maxpool,
            backbone.layer1,   # ResNet blocks
            backbone.layer2,
            backbone.layer3,
            backbone.layer4,   # Output: 512 channels
        )

        # Detection head: 3 conv layers for feature refinement
        yolo_conv_channels = model_config['yolo_conv_channels']  # 1024
        leaky_relu_slope = model_config['leaky_relu_slope']  # 0.1

        self.conv_layers = nn.Sequential(
            nn.Conv2d(512, yolo_conv_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(yolo_conv_channels),
            nn.LeakyReLU(leaky_relu_slope),

            nn.Conv2d(yolo_conv_channels, yolo_conv_channels, 3,
                     stride=2, padding=1, bias=False),
            nn.BatchNorm2d(yolo_conv_channels),
            nn.LeakyReLU(leaky_relu_slope),

            nn.Conv2d(yolo_conv_channels, yolo_conv_channels, 3,
                     padding=1, bias=False),
            nn.BatchNorm2d(yolo_conv_channels),
            nn.LeakyReLU(leaky_relu_slope)
        )

        # Final 1x1 conv to get S*S*(5B+C) output
        self.final_conv = nn.Conv2d(yolo_conv_channels, 5 * self.B + self.C, 1)

    def forward(self, x):
        # x: (batch, 3, 448, 448)
        out = self.features(x)      # (batch, 512, 14, 14)
        out = self.conv_layers(out)  # (batch, 1024, 7, 7)
        out = self.final_conv(out)   # (batch, 30, 7, 7)

        # Permute to (batch, S, S, 5B+C)
        out = out.permute(0, 2, 3, 1)  # (batch, 7, 7, 30)
        return out
```

### 2. Loss Function

The complete YOLOv1 loss with all three components:

```python
import torch
import torch.nn as nn

def iou(box1, box2):
    """
    Calculate Intersection over Union between two sets of boxes.

    Args:
        box1, box2: Tensors of shape (..., 4) in format (x1, y1, x2, y2)

    Returns:
        iou: Tensor of shape (...) with IOU values
    """
    # Calculate areas
    area1 = (box1[..., 2] - box1[..., 0]) * (box1[..., 3] - box1[..., 1])
    area2 = (box2[..., 2] - box2[..., 0]) * (box2[..., 3] - box2[..., 1])

    # Find intersection rectangle
    x_topleft = torch.max(box1[..., 0], box2[..., 0])
    y_topleft = torch.max(box1[..., 1], box2[..., 1])
    x_bottomright = torch.min(box1[..., 2], box2[..., 2])
    y_bottomright = torch.min(box1[..., 3], box2[..., 3])

    # Calculate intersection area (clamp to handle non-overlapping boxes)
    intersection = (x_bottomright - x_topleft).clamp(min=0) * \
                   (y_bottomright - y_topleft).clamp(min=0)

    # Calculate union and IOU
    union = area1.clamp(min=0) + area2.clamp(min=0) - intersection
    iou = intersection / (union + 1e-6)  # Add epsilon to avoid division by zero
    return iou

class YOLOLoss(nn.Module):
    """
    YOLOv1 Loss Function: Localization + Confidence + Classification

    Loss = Î»_coord Ã— L_box + L_conf + L_class
    """
    def __init__(self, S=7, B=2, C=20):
        super(YOLOLoss, self).__init__()
        self.S = S
        self.B = B
        self.C = C
        self.lambda_coord = 5.0    # Increase weight for box coordinates
        self.lambda_noobj = 0.5    # Decrease weight for no-object cells

    def forward(self, preds, targets):
        """
        Args:
            preds: (batch, S, S, 5*B + C) - model predictions
            targets: (batch, S, S, 5*B + C) - ground truth targets

        Returns:
            loss: Scalar tensor
        """
        batch_size = preds.size(0)

        # Create coordinate shift grids for converting relative â†’ absolute coords
        xshift = torch.arange(0, self.S, device=preds.device) / float(self.S)
        yshift = torch.arange(0, self.S, device=preds.device) / float(self.S)
        yshift, xshift = torch.meshgrid(yshift, xshift, indexing='ij')
        xshift = xshift.reshape((1, self.S, self.S, 1)).repeat(1, 1, 1, self.B)
        yshift = yshift.reshape((1, self.S, self.S, 1)).repeat(1, 1, 1, self.B)

        # Reshape predictions and targets: (batch, S, S, B, 5)
        pred_boxes = preds[..., :5*self.B].reshape(batch_size, self.S, self.S, self.B, 5)
        target_boxes = targets[..., :5*self.B].reshape(batch_size, self.S, self.S, self.B, 5)

        # Convert from (x_offset, y_offset, âˆšw, âˆšh) to (x1, y1, x2, y2) format
        def boxes_to_x1y1x2y2(boxes, xshift, yshift):
            x_center = boxes[..., 0] / self.S + xshift
            y_center = boxes[..., 1] / self.S + yshift
            width = torch.square(boxes[..., 2])   # w = (âˆšw)Â²
            height = torch.square(boxes[..., 3])  # h = (âˆšh)Â²

            x1 = (x_center - 0.5 * width).unsqueeze(-1)
            y1 = (y_center - 0.5 * height).unsqueeze(-1)
            x2 = (x_center + 0.5 * width).unsqueeze(-1)
            y2 = (y_center + 0.5 * height).unsqueeze(-1)
            return torch.cat([x1, y1, x2, y2], dim=-1)

        pred_boxes_xyxy = boxes_to_x1y1x2y2(pred_boxes, xshift, yshift)
        target_boxes_xyxy = boxes_to_x1y1x2y2(target_boxes, xshift, yshift)

        # Calculate IOU between predicted and target boxes
        iou_pred_target = iou(pred_boxes_xyxy, target_boxes_xyxy)

        # Find responsible box (highest IOU with ground truth)
        max_iou, max_iou_idx = iou_pred_target.max(dim=-1, keepdim=True)
        max_iou_idx = max_iou_idx.repeat(1, 1, 1, self.B)

        # Create mask for responsible boxes
        box_indices = torch.arange(self.B, device=preds.device).reshape(1, 1, 1, self.B)
        box_indices = box_indices.expand_as(max_iou_idx)
        is_responsible_box = (max_iou_idx == box_indices).long()

        # Object indicator: 1 if cell contains object, 0 otherwise
        obj_indicator = targets[..., 4:5]  # Shape: (batch, S, S, 1)

        # Indicator for responsible boxes in cells with objects
        responsible_obj_indicator = is_responsible_box * obj_indicator

        # --- 1. LOCALIZATION LOSS (only for responsible boxes) ---
        x_loss = (pred_boxes[..., 0] - target_boxes[..., 0]) ** 2
        y_loss = (pred_boxes[..., 1] - target_boxes[..., 1]) ** 2
        w_loss = (pred_boxes[..., 2] - target_boxes[..., 2]) ** 2  # âˆšw loss
        h_loss = (pred_boxes[..., 3] - target_boxes[..., 3]) ** 2  # âˆšh loss

        localization_loss = self.lambda_coord * (
            (responsible_obj_indicator * x_loss).sum() +
            (responsible_obj_indicator * y_loss).sum() +
            (responsible_obj_indicator * w_loss).sum() +
            (responsible_obj_indicator * h_loss).sum()
        )

        # --- 2. CONFIDENCE LOSS (for object cells) ---
        # Target confidence = IOU for responsible boxes
        obj_conf_loss = ((pred_boxes[..., 4] - max_iou) ** 2 *
                        responsible_obj_indicator).sum()

        # --- 3. CONFIDENCE LOSS (for no-object cells) ---
        no_obj_indicator = 1 - responsible_obj_indicator
        noobj_conf_loss = self.lambda_noobj * (
            (pred_boxes[..., 4] ** 2 * no_obj_indicator).sum()
        )

        # --- 4. CLASSIFICATION LOSS (only for cells with objects) ---
        class_preds = preds[..., 5*self.B:]
        class_targets = targets[..., 5*self.B:]
        class_loss = ((class_preds - class_targets) ** 2 * obj_indicator).sum()

        # Total loss
        total_loss = (localization_loss + obj_conf_loss +
                     noobj_conf_loss + class_loss) / batch_size

        return total_loss
```

### 3. Dataset & Target Encoding

Converting Pascal VOC annotations to YOLO format:

```python
import torch
import albumentations as alb
import cv2
from torch.utils.data import Dataset

class VOCDataset(Dataset):
    """Pascal VOC Dataset with YOLO target encoding"""

    def __init__(self, split='train', img_size=448, S=7, B=2, C=20):
        self.split = split
        self.img_size = img_size
        self.S = S  # Grid size
        self.B = B  # Boxes per cell
        self.C = C  # Number of classes

        # Data augmentation for training
        self.transforms = {
            'train': alb.Compose([
                alb.HorizontalFlip(p=0.5),
                alb.Affine(scale=(0.8, 1.2),
                          translate_percent=(-0.2, 0.2)),
                alb.ColorJitter(brightness=(0.8, 1.2),
                               saturation=(0.8, 1.2)),
                alb.Resize(self.img_size, self.img_size)
            ], bbox_params=alb.BboxParams(format='pascal_voc',
                                          label_fields=['labels'])),
            'test': alb.Compose([
                alb.Resize(self.img_size, self.img_size)
            ], bbox_params=alb.BboxParams(format='pascal_voc',
                                          label_fields=['labels']))
        }

        # Load Pascal VOC annotations...
        # (XML parsing code omitted for brevity)

    def __getitem__(self, index):
        # Load image and annotations
        img_info = self.images_info[index]
        img = cv2.imread(img_info['filename'])
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        bboxes = [det['bbox'] for det in img_info['detections']]  # (x1,y1,x2,y2)
        labels = [det['label'] for det in img_info['detections']]

        # Apply augmentations
        transformed = self.transforms[self.split](
            image=img, bboxes=bboxes, labels=labels
        )
        img = transformed['image']
        bboxes = torch.tensor(transformed['bboxes'])
        labels = torch.tensor(transformed['labels'])

        # Normalize image to [0, 1] and apply ImageNet normalization
        img_tensor = torch.from_numpy(img / 255.0).permute(2, 0, 1).float()

        # --- Create YOLO target tensor ---
        target_dim = 5 * self.B + self.C
        yolo_target = torch.zeros(self.S, self.S, target_dim)

        h, w = img.shape[:2]
        cell_size = h // self.S  # Pixels per grid cell

        if len(bboxes) > 0:
            # Convert (x1, y1, x2, y2) â†’ (x_center, y_center, width, height)
            box_width = bboxes[:, 2] - bboxes[:, 0]
            box_height = bboxes[:, 3] - bboxes[:, 1]
            box_center_x = bboxes[:, 0] + 0.5 * box_width
            box_center_y = bboxes[:, 1] + 0.5 * box_height

            # Determine which grid cell each object belongs to
            grid_i = torch.floor(box_center_x / cell_size).long()
            grid_j = torch.floor(box_center_y / cell_size).long()

            # Compute relative coordinates within grid cell (0 to 1)
            box_x_offset = (box_center_x - grid_i * cell_size) / cell_size
            box_y_offset = (box_center_y - grid_j * cell_size) / cell_size

            # Normalize width and height to image size
            box_w_norm = box_width / w
            box_h_norm = box_height / h

            # Fill YOLO target tensor
            for idx in range(len(bboxes)):
                # Assign same target to all B boxes (model picks responsible one)
                for b in range(self.B):
                    s = 5 * b
                    yolo_target[grid_j[idx], grid_i[idx], s] = box_x_offset[idx]
                    yolo_target[grid_j[idx], grid_i[idx], s+1] = box_y_offset[idx]
                    yolo_target[grid_j[idx], grid_i[idx], s+2] = box_w_norm[idx].sqrt()
                    yolo_target[grid_j[idx], grid_i[idx], s+3] = box_h_norm[idx].sqrt()
                    yolo_target[grid_j[idx], grid_i[idx], s+4] = 1.0  # Confidence

                # One-hot encode class
                label = int(labels[idx])
                yolo_target[grid_j[idx], grid_i[idx], 5*self.B + label] = 1.0

        return img_tensor, yolo_target
```

### 4. Training Loop

Complete training code with proper hyperparameters:

```python
import torch
from torch.optim import SGD
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader

# Initialize model, loss, and dataset
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = YOLOV1(img_size=448, num_classes=20, model_config={
    'S': 7, 'B': 2, 'yolo_conv_channels': 1024, 'leaky_relu_slope': 0.1
}).to(device)

criterion = YOLOLoss(S=7, B=2, C=20)

train_dataset = VOCDataset(split='train', img_size=448)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Optimizer: SGD with momentum (as per paper)
optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)

# Learning rate schedule: reduce at epochs [75, 105]
# Paper uses warm-up from 1e-3 to 1e-2 for first epochs, then steps down
scheduler = MultiStepLR(optimizer, milestones=[75, 105], gamma=0.1)

# Training loop
num_epochs = 135  # As per paper
model.train()

for epoch in range(num_epochs):
    epoch_loss = 0.0

    for images, targets in train_loader:
        images = images.to(device)
        targets = targets.to(device)

        # Forward pass
        predictions = model(images)
        loss = criterion(predictions, targets)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    scheduler.step()
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')
```

### 5. Inference with NMS

Post-processing predictions to get final detections:

```python
def convert_predictions_to_boxes(predictions, S=7, B=2, C=20,
                                conf_threshold=0.2, nms_threshold=0.5):
    """
    Convert YOLO predictions to bounding boxes with NMS.

    Args:
        predictions: (S, S, 5*B + C) tensor
        conf_threshold: Minimum confidence to keep box
        nms_threshold: IOU threshold for NMS

    Returns:
        boxes: (N, 4) tensor in (x1, y1, x2, y2) format
        scores: (N,) confidence scores
        labels: (N,) class labels
    """
    predictions = predictions.reshape(S, S, 5*B + C)

    # Get class predictions (same for all boxes in a cell)
    class_probs, class_labels = predictions[..., 5*B:].max(dim=-1)

    # Create coordinate shift grid
    shifts_x = torch.arange(S, device=predictions.device) / float(S)
    shifts_y = torch.arange(S, device=predictions.device) / float(S)
    shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing='ij')

    all_boxes = []
    all_scores = []
    all_labels = []

    # Process each of B boxes per cell
    for b in range(B):
        # Extract box parameters
        x_offset = predictions[..., b*5 + 0]
        y_offset = predictions[..., b*5 + 1]
        w = predictions[..., b*5 + 2]
        h = predictions[..., b*5 + 3]
        conf = predictions[..., b*5 + 4]

        # Convert to absolute coordinates
        x_center = (x_offset / S + shifts_x)
        y_center = (y_offset / S + shifts_y)
        width = torch.square(w)   # w = âˆšw_predÂ²
        height = torch.square(h)

        # Convert to (x1, y1, x2, y2) format
        x1 = (x_center - 0.5 * width).reshape(-1, 1)
        y1 = (y_center - 0.5 * height).reshape(-1, 1)
        x2 = (x_center + 0.5 * width).reshape(-1, 1)
        y2 = (y_center + 0.5 * height).reshape(-1, 1)
        boxes = torch.cat([x1, y1, x2, y2], dim=-1)

        # Compute class-specific confidence scores
        scores = conf.reshape(-1) * class_probs.reshape(-1)
        labels = class_labels.reshape(-1)

        all_boxes.append(boxes)
        all_scores.append(scores)
        all_labels.append(labels)

    # Concatenate all boxes
    boxes = torch.cat(all_boxes, dim=0)
    scores = torch.cat(all_scores, dim=0)
    labels = torch.cat(all_labels, dim=0)

    # Confidence thresholding
    keep = scores > conf_threshold
    boxes = boxes[keep]
    scores = scores[keep]
    labels = labels[keep]

    # Apply NMS per class
    keep_mask = torch.zeros_like(scores, dtype=torch.bool)
    for class_id in torch.unique(labels):
        class_indices = labels == class_id
        class_boxes = boxes[class_indices]
        class_scores = scores[class_indices]

        # NMS (using torchvision)
        keep_indices = torch.ops.torchvision.nms(
            class_boxes, class_scores, nms_threshold
        )

        # Mark these boxes as kept
        class_keep_indices = torch.where(class_indices)[0][keep_indices]
        keep_mask[class_keep_indices] = True

    final_boxes = boxes[keep_mask]
    final_scores = scores[keep_mask]
    final_labels = labels[keep_mask]

    return final_boxes, final_scores, final_labels

# Example usage
model.eval()
with torch.no_grad():
    img_tensor = ... # Load and preprocess image
    predictions = model(img_tensor.unsqueeze(0))[0]  # Remove batch dim

    boxes, scores, labels = convert_predictions_to_boxes(
        predictions, conf_threshold=0.2, nms_threshold=0.5
    )

    # boxes: (N, 4) in normalized 0-1 coordinates
    # Multiply by image dimensions to get pixel coordinates
```

### Key Implementation Details

1. **âˆšw and âˆšh**: The model predicts square root of width/height to balance loss across different box sizes
2. **Relative Coordinates**: x,y offsets are relative to grid cell top-left corner
3. **Responsible Box Selection**: During training, only the box with highest IOU with GT is penalized for coordinates
4. **Class Probabilities**: Shared across all B boxes in a cell (limitation of v1)
5. **Lambda Weighting**: Î»_coord=5 to emphasize localization, Î»_noobj=0.5 to de-emphasize empty cells

This implementation closely follows the original paper and can achieve ~63% mAP on Pascal VOC 2007 after 135 epochs of training.

---

### Optimization Techniques

#### Precision Options

| Precision | Speed | Accuracy | Memory | Best For |
|-----------|-------|----------|--------|----------|
| **FP32** (default) | 1Ã— | 100% (baseline) | 4 bytes/param | Training, research |
| **FP16** (half precision) | **2-3Ã—** | 99.5% | 2 bytes/param | Production (GPU) |
| **INT8** (quantization) | **4Ã—** | 97-99% | 1 byte/param | Edge devices |

**INT8 Quantization** (PyTorch):

```python
import torch.quantization

model = YOLOV1(...)
model.eval()

# Quantization-aware training (best accuracy)
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
model_prepared = torch.quantization.prepare_qat(model)

# Train for a few epochs...
model_quantized = torch.quantization.convert(model_prepared)

# Result: 4Ã— smaller, 2-4Ã— faster on CPU
```

#### Layer Fusion

TensorRT automatically fuses operations:

```text
Conv â†’ BatchNorm â†’ ReLU  â†’  Single fused kernel
                            (3Ã— fewer memory transfers)
```

#### Dynamic Tensor Memory

- Reuses memory buffers across layers
- Reduces GPU memory usage by 30-50%

### 3. Hardware-Specific Deployment

NVIDIA GPU (RTX 3060/4090)
**Recommendation**: TensorRT with FP16

```bash
# Export PyTorch â†’ ONNX â†’ TensorRT
python export.py --weights best.pt --format onnx
trtexec --onnx=yolov1.onnx --saveEngine=yolov1_fp16.trt --fp16
```

**Expected Performance**:

- RTX 4090: 180 fps (FP16), 90 fps (FP32)
- RTX 3060: 120 fps (FP16), 60 fps (FP32)
- Edge Devices (Jetson, Raspberry Pi)
**Recommendation**: TensorRT INT8 or TFLite

**NVIDIA Jetson Orin**:

```bash
# INT8 quantization for Jetson
trtexec --onnx=yolov1.onnx \
        --int8 \
        --workspace=2048 \
        --saveEngine=yolov1_int8.trt
```

Performance: 120 fps (INT8), 45 fps (FP16)

---

## Conclusion: The Legacy of "You Only Look Once"

YOLOv1 was more than just a model; it was a philosophical shift. It taught the world that object detection could be framed as a single, elegant regression problem. It prioritized speed and simplicity, opening the door to applications that were previously unimaginable.

From the clunky but brilliant v1 to the sleek, end-to-end v10, the journey of YOLO is a perfect story of scientific progress: a great idea, honestly acknowledged limitations, and a decade of relentless, community-driven effort to build something better. The next time you see a self-driving car navigate a busy street, you'll know it stands on the shoulders of a giantâ€”an idea that dared to just look once.

**What YOLOv1 proved:**

1. **Single-stage detection works**: No need for complex multi-stage pipelines
2. **Real-time is possible**: 45 fps without sacrificing too much accuracy
3. **Simplicity is powerful**: Unified architecture and loss function
4. **Context matters**: Seeing the whole image reduces false positives

While YOLOv1 had limitations (small objects, crowded scenes, unusual aspect ratios), it established the foundation for a family of detectors that continues to push the boundaries of speed and accuracy.

The evolution from v1 to v10 shows continuous improvement while maintaining the core philosophy: **You Only Look Once** â€” simple, fast, and effective object detection.

---

## References

- Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. CVPR 2016.
- Pascal VOC Dataset: http://host.robots.ox.ac.uk/pascal/VOC/
- ImageNet: http://www.image-net.org/
- [PyTorch Implementation](https://github.com/samitmohan/YOLOv1/tree/master/implementation)

---

## Extra

Training a phone detection model. What is happening at each epoch?

### 1. Training

For each batch (32 images):

**a. Data Loading**

- Load 32 images from train set
- Apply augmentations (copy_paste, HSV, rotation, etc.)
- Resize to 640x640
- Normalize pixel values

**b. Forward Pass**

- Pass batch through YOLOv11m network
- Get predictions: bboxes (x,y,w,h), class probabilities, objectness scores
- Network has ~20M parameters (backbone + neck + head)

**c. Loss Calculation**

- Box loss (7.5x weight): IoU/GIoU loss for bbox coordinates
- Class loss (1.5x weight): Binary cross-entropy for classification
- DFL loss (1.5x weight): Distribution focal loss for bbox refinement
- Total loss = weighted sum of above

> **What do these mean?**

> - **Box loss**: How well the model draws a box around the phone
> - **Class loss**: How sure the model is that the thing is a phone
> - **DFL loss**: How precisely the model guesses the edges of the box

**d. Backward Pass**

- Calculate gradients using total loss
- Backpropagation and updating of gradients using Optimiser (I used AdamW with momentum)

**e. Optimizer Step (AdamW)**

- Update weights using gradients
- Apply learning rate (starts at 0.0005, decays to 0.01 Ã— lr0)
- Apply weight decay (0.0005) for regularization
- Apply momentum (0.9)

### 2. Validation (every epoch)

After all training batches:

**a. Switch to eval mode**

- Disable dropout (full model instead of skipping some layers)
- Use batch normalization in eval mode (already learned features instead of learning new ones)
- No augmentations (no flips)

**b. For each validation image**

- Forward pass (no gradient calculation)
- Get predictions with conf threshold (default 0.001 for val)
- Apply NMS (non-maximum suppression: if the model draws many boxes on the same phone, keep only the best one and throw away the rest) with IoU threshold 0.45

**c. Metric Calculation**

- Match predictions to ground truth (IoU >= 0.5)
- Calculate per-class metrics:
  - Precision = TP / (TP + FP)
  - Recall = TP / (TP + FN)
  - mAP@50 = mean AP at IoU=0.5
  - mAP@50-95 = mean AP averaged over IoU 0.5 to 0.95
- Calculate overall metrics (averaged across all classes)

---

[You can see how CNN works here](https://cs231n.stanford.edu/)

[PyTorch implementation of YOLOV1 paper](https://github.com/samitmohan/YOLOv1)

[Intro to YOLO](https://www.youtube.com/watch?v=lcArnTfpPBM)
