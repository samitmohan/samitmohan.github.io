---
layout: post
title:  "building RAG for my website"
date:   2026-01-27 14:06:04 +0530
categories: tech
tokens: "~2.5k"
---

People have told me my blogs are too long, I like long blogs, they don't.. Too bad. I need to solve this problem, sure I can write shorter blogs, but there is no fun in that.

I wanted the ability to search on my blog, search content and put my own local LLM on this blog so I and other people can just hit samitmohan.github.io/ask and ask any blog related question.

> Basically build a RAG.

The only problem? I have never built a RAG. So I decided to watch some stanford videos, read some docs and start using chatGPT to plan my project.

So I decided to build a RAG system over my own blog — essentially a smarter, explain-as-you-go `Ctrl+F` for everything I’ve written.

What even is a RAG? Why can't I just use chatGPT on my blog? Because chatGPT doesn't know about samitmohan.github.io; it is a generalised model which can't answer questions related to my blog. For that I need to fine-tune chatGPT so it answers only questions relevant to my blog; in context to my blog.

![finetune](/assets/images/rag/fine-tune.png)

This post documents how I built it, what worked, what didn’t, and why certain design choices matter.

---

## What This Project Does

The goal was to build a retrieval-augmented question-answering system that:

- **Indexes** all my blog posts.
- **Retrieves** relevant text (when I ask for explanation/why) or code chunks (when I ask for implementation) based on semantic similarity + boost/penalise content not relevant
- **Generates** grounded answers with citations using a local Large Language Model (LLM).

## Architecture

```text
GitHub Pages UI (/ask)
        ↓
FastAPI backend
        ↓
FAISS vector store
        ↓
Intent-aware retrieval + reranking
        ↓
Local LLM (Ollama)
        ↓
Answer + sources
```

![arch](/assets/images/rag/arch.png)

This is what I thought of, and it works pretty well. I know FAISS isn't the best option but I think for my use case it is pretty cool.

### Repo Structure

The project is organized to separate concerns between data ingestion, retrieval logic, and the API surface.

```text
blog-rag/
├── README.md
├── requirements.txt
├── .env.example
├── .gitignore
├── data/ (this is generated by local_ingest.py which just fetches all posts from website. parses, creates chunks, embeds and stores in database)
│   ├── raw/                 # Raw blog markdown 
│   └── embeddings/          # Persisted FAISS index + metadata
├── ingestion/
│   ├── load_github.py       # Fetch all blog posts from this website, ~15 lines, easy
│   ├── parse_markdown.py    # Understands and parses markdown from my raw blogs so they are interpretable, this was little tricky, had to use GPT.
│   ├── clean_text.py        # normalise and clean text, also easy
│   └── chunker.py           # creates chunks of data -> so all paragraph level text -> chunks (easy, had to seperate code and text chunks chunk size)
├── embeddings/
│   ├── embed_chunks.py      # SentenceTransformer embeddings (which was quite slow) -> switched to fastembed (bge-small model; 384 output dimension)
│   └── vector_store.py      # FAISS + metadata store (takes my query embedding, searches in stored embeddings via cosine similarity and save data)
├── retrieval/
│   └── retrieve.py          # embed query text (types=code, text), retrieve top-k results from FAISS, re-rank by intent (boost / penalise)
├── generation/
│   ├── prompt.py            # Prompt construction (Just add system prompt and user prompt (fetched by our retrieval) to send to our local LLM)
│   └── answer.py            # LLM call + citations (generate answer from OLLAMA qwen2.5:7b and retrieve citations)
├── backend/
│   └── app.py            # FastAPI app, uvicorn, get, ingest (all blogs), generate end point
└── ask/
    └── index.html           # GitHub Pages frontend
```

Design should be simple, UI should mimic chatGPT; load text directly instead of one at a time

> This is how the UI looks like

![ragop](/assets/images/rag/ragop.png)

**Let's start in depth on how I made this;**

---

## Phase 1: Ingestion & Corpus Creation

**Goal:** Turn blog content into clean, inspectable retrieval units.

![phase1](/assets/images/rag/rag1.png)
![phase1step](/assets/images/rag/search_index.png)

### 1. Loading Blog Data

All my blog posts already exist as Markdown files in my GitHub repository. All I had to do was write a simple `load_all_blogs.py` script that fetches them from the website.

### 2. Parsing Markdown 

Markdown can be messy. `parse_markdown.py` extracts structured data:

- **Frontmatter:** Title, date, categories.
- **Structure:** Section headings, paragraphs, code blocks, and links.

Avoided using a heavy-weight Markdown AST parser and instead relied on careful regex and structure matching. This gave me more control but was trickier to get right than I initially expected, had to use GPT.

### 3. Cleaning Without Summarizing

The script `clean_text.py` handles normalization:

- Normalizes whitespace.
- Removes non-content "junk".
- Does *not* rewrite or summarize.

![parsing](/assets/images/rag/doc_parsing.png)

This is important because **RAG quality starts with data integrity**. If the source data is lost or mutated, the retrieval will be flawed. This part was pretty easy as well.

### 4. Chunking

A **chunk** is the smallest piece of text that should be retrieved on its own to answer a question.

**My Chunking Rules:**

- One paragraph = one chunk (I know this isn't the most optimised; but it was the easiest to implement and works fine)
- Multiple sentences are fine.
- Multiple distinct ideas are *not* fine in one chunk.
- **Code blocks** are treated as separate chunks.
- **Links** are embedded in text, not standalone.
- Chunks must be **self-contained**.
- Max length: ~200–300 words.

**Example Output:**

```json
{
  "id": "600LC_intro_0",
  "text": "nothing. spend your time doing something more meaningful...",
  "metadata": {
    "post_title": "600+ leetcode questions: lessons",
    "section": "intro",
    "date": "2025-10-25",
    "url": "/tech/2025/10/25/600LC.html",
    "type": "text"
  }
}
```

![chunk](/assets/images/rag/doc_chunk.png)

Chunking has some hyper-parameters

- **chunk size** = how many words in a paragraph is 1 chunk
- **chunk overlap** = is there any overlap between chunks; if yes how many words?
- **seperator** -> usually a ' ' and
- **strip_whitespace** to normalise

For my project; I manually split and made chunks because it was simple paragraph level chunks; there are tons of models which do this for you and have a lot of customisastions (document level chunk, sentence level etc..)

This is how one model looks like to use;

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

chunks = []
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 30)
chunks = text_splitter.split_documents(raw_docs)
print(f"{len(chunks)} chunks ready for embedding")
```

![chunk2](/assets/images/rag/chunk2.png)

---

## Phase 2: Embeddings & Vector Store

- I need to store these chunks into a searchable index; from where I can search these texts/chunks. This should be fairly easy..
- We need to pick the right database, for RAGs we use vector database since chunks / texts are stored in forms of vectors and semantically which means related words are stored together (this is how vector embeddings works; the vector for king and vector for queen will be stored nearby since they are related)
- We use cosine similarity to measure the angle and distance between 2 vectors (or query and our stored chunks in this case) and assign `score` to them.

### Embeddings

I started with the **Sentence Transformers** library for generating embeddings:

```python
SentenceTransformerEmbeddings("all-MiniLM-L6-v2")
```

- **384-dimensional vectors** which means 1 chunk has dimension of 384 in space (more dimensions = more spacial awareness in vector space = better semantic search)
- Fast inference
- Runs locally
- Excellent performance for semantic search

Both text and code are embedded using the same model, but they are flagged via metadata to distinguish between them during retrieval.

Update; this didn't work out in the long run; I wanted others to use my /ask features and storing creating query embeddings is fast in SentenceTransformerEmbedding but takes too much memory (my back-end for this was deployed on Render and it crashes);

So I switched to another embedding model; fastembed `TextEmbedding(model_name="BAAI/bge-small-en-v1.5")`

It's fast, lightweight and onnx weights are only ~200mb; so doesn't consume a lot of cores (deployable on render)

But both/all embedding model work the same; they take your chunks that you just created -> convert into vector embeddings -> which are later stored in a database so that you can search using cosine similarity;


![Embedding](/assets/images/rag/embed.png)


### Vector Store (FAISS)

![search](/assets/images/rag/index.png)

The next step as I said; is to store these vector embeddings in a database.

I use **FAISS** (Facebook AI Similarity Search) to store the vectors efficiently. Since FAISS only stores the vectors themselves, I maintain a parallel metadata store:

```text
vector → embedding
payload → { id, text, metadata }
```

**Query Example:**

- **Query:** “Why do we use softmax in MNIST?”
- **Returns:**
  - **Distance:** 0.29 (lower the score; more the similarity between our chunks / stored_embeddings and query)
  - **Chunk:** "Softmax is used in MNIST classification..."

![search2](/assets/images/rag/search.png)
![search3](/assets/images/rag/rag2.png)

Til now we have taken all our blog documents -> cleaned them -> converted them to chunks -> converted these chunks into embeddings -> stored them in a database which is searchable.

And we have our query/question which is also converted into embedding.

These two are searched using cosine similarity; lesser score means closer -> but it's not necessary that the lowest score only will be the correct answer.

We need more scores; this is where the `retrieval` in RAG comes from. How do we retrieve data?

---

## Phase 3: Retrieval Control & Reranking

![retrieval](/assets/images/rag/retrieval.png)

Raw semantic search isn’t always enough. User intent varies.

- If I ask **“show me code”** → I want code blocks.
- If I ask **“why does this work”** → I want conceptual explanations.

To handle this, `retrieve.py` implements an intent-aware pipeline:

```text
query
  ↓
intent classification
  ↓
embedding
  ↓
FAISS search
  ↓
intent-aware reranking
  ↓
top-k chunks
```

This step alone improved answer quality dramatically by filtering out irrelevant chunk types (e.g., ignoring text chunks when the user explicitly wants code).

![retrieval2](/assets/images/rag/rag3.png)

---

## Phase 4: Prompting & Generation

Now that we have our query and these searchable chunk index database that we have created.

Simple RAG:

- Provide these chunks to LLMs and let them handle it (Not optimal)

![simpleLLM](/assets/images/rag/simple_rag.png)

A better way would be to also add a prompt (usually called the system prompt) which gives the context of the chunks and instructions on how to answer questions

### Prompt Construction

Context is king. I explicitly tell the LLM what each retrieved chunk represents to avoid confusion:

```text
[TEXT]
explanation...

[CODE]
code snippet...
```

Then I append the user's question. This simple formatting prevents the model from "guessing" the context type.

![promptLLM](/assets/images/rag/prompt+llm.png)

### Generation (Local LLM)

Our final step is to pick LLM where we can pass our prompt (system) + prompt (user (chunks retrieved)) to... 

I don't have openAI key because I am not rich (yet) & I don't trust Sam Altman; **Ollama** is great for local inference and has multiple models (I am using qwen2.5 which has 7 billion parameters)

```bash
ollama serve
ollama pull qwen2.5:7b
```

- **No cloud APIs**
- **No token limits**
- **No costs**

### Citations & Grounding

The system is designed to never hallucinate sources. Instead, it:

- Tracks exactly which chunks were used to generate the answer.
- Returns specific citations alongside the answer.

**Example Response:**
> **Answer:** Softmax converts logits into probabilities...
>
> **Sources:**
> `600+ leetcode questions: lessons_intro_0` (text, section=intro)

This was fairly easy since I was storing meta-data along with the vectors; so whatever answer is returned; just return the source along with it.

---

## Persistence & Deployment

### Persistence

Since I have to deploy this on my website; everytime user goes to samitmohan.github.io/ask it shouldn't reindex / embed all my blogs; I need to do it once and store it in /data folder.

The FAISS index and metadata are persisted to disk. On restart:

- No re-embedding required.
- No re-indexing required.
- **Instant retrieval.**

### Frontend & Deployment

- **UI:** Lives at [samitmohan.github.io/ask](/ask) which is a simple index.html script made in 15 mins
- **Backend:** FastAPI: I like fastAPI; easy to write backend and works well for a mini project like mine.

This is how the /query api looks like:

```python
@app.post("/query")
def query_endpoint(req: RetrieveRequest):
    """
    Legacy/All-in-one endpoint.
    """
    # 1. Retrieve
    retrieval_resp = retrieve_endpoint(req)
    chunks = retrieval_resp["chunks"]
    
    if not chunks:
        return {"answer": "dawg i don't have that info on me.", "citations": []}

    # 2. Generate
    gen_req = GenerateRequest(query=req.query, chunks=chunks)
    return generate_endpoint(gen_req)

@app.get("/status")
def status():
    return {"ready": vector_store is not None}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))
```

- **Deployment/Connectivity:** Since I don't want my laptop to be running constantly for you to use this; I had to come up with a better solution; also I have 0 money.

My first try was Cloudflare Tunnel (to expose the local server securely) but it doesn't work when my laptop is closed.
Also the LLM doesn't work since that isn't hosted anywhere. 

So my app doesn't work for anyone... yet.

I thought of a Dual-Mode Architecture:

- Public Mode (Search): Runs purely on retrieval (CPU-only). Fast, free, and deployable anywhere. Returns relevant excerpts.
- Local Mode (AI Chat): When running locally, connects to a local LLM (Qwen 2.5:7b via Ollama) to synthesize answers from the retrieved chunks.

When you want to use the AI chat; just run Ollama serve on your computer; and it'll detect if your localhost is using Ollama and run the AI.

Otherwise the backend works all the time.. I exposed my localhost to [Render](https://dashboard.render.com/) which is free of cost and runs my backend continously; a problem with this was my LLM (Ollama) doesn't work with this routing; so I had to use ngrok to expose my localhost LLM to my render dashboard. It took me a little time and chatgippitying to figure this out; but it works!!

![forwarding](/assets/images/rag/forwarding.png)

> All I had to do was map my Ollama LLM to my render URL.

![env_var](/assets/images/rag/env_var.png)


Deployed it using Render (extremely slow) but now it works when I am connected to Ollama using local and even if I am not connected; the retrieval search works (without the LLM part)

![render](/assets/images/rag/rag-render.png)

In addition to vector semantic search, I also added score boost; (multiplier) to chunks that contain the exact keywords from query.
This ensures that if you ask for "Softmax", chunks actually containing the word "Softmax" float to the top, making the "offline" results much sharper

**Final Pipeline:**

![flow](/assets/images/rag/flow.png)

- **GitHub Pages UI** captures input.
- **FastAPI backend** processes the request.
- **FAISS** performs retrieval.
- **Reranker** optimizes results based on intent.
- **Local LLM** generates the response.
- **UI** renders the grounded answer + sources.

---

## Final Thoughts

Lessons learnt about building RAG:

- **RAG is mostly data engineering**, not just model selection.
- **Chunking strategy** matters more than the specific embedding model.
- **Intent control** beats simply using larger embeddings.
- **Local LLMs** are surprisingly capable for many real-world use cases.
- **Deployment** is hard... and I am not rich enough.

You can test it by going to [samitmohan.github.io/ask](https://www.samitmohan.github.io/ask/)

To test the LLM; follow this [github](https://github.com/samitmohan/rag-for-blogs) (all you have to do is run your local LLM + tunnel it via ngrok and connect)

## Demo

<video width="100%" height="auto" controls>
  <source src="/assets/videos/rag.mov" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Optional

How do we measure accuracy of RAG?

![accuracy](/assets/images/rag/contextrelevance.png)

- Another project (Customer Support Bot) which is much simpler and I built to learn RAG basics (LangChain for everything) can be found [here](https://github.com/samitmohan/bbg_course/blob/master/project2/)