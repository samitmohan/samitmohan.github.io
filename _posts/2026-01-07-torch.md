---
layout: post
title:  "NumPy & PyTorch for Dummies"
date:   2026-01-07 14:06:04 +0530
categories: tech
---

## NumPy Primer

**NumPy is the foundational library for numerical computing in Python.** It provides a high-performance, multidimensional array object (`ndarray`) and a vast collection of tools to work with these arrays.

### "But I Already Have Python Lists..." - The Need for Speed

Imagine you need to compute the dot product of two very large vectors. In pure Python, you might write a simple loop:

```python
import time

def python_dot(a, b):
    """Computes dot product of two lists."""
    result = 0
    for i in range(len(a)):
        result += a[i] * b[i]
    return result

# Let's try it with million-element lists
size = 1_000_000
list_a = list(range(size))
list_b = list(range(size))

start_time = time.time()
py_result = python_dot(list_a, list_b)
end_time = time.time()

print(f"Python dot product result: {py_result}")
print(f"Time taken with Python lists: {(end_time - start_time) * 1000:.2f} ms")
```

Now, let's do the exact same thing using NumPy:

```python
import numpy as np

# Let's try it with million-element NumPy arrays
array_a = np.arange(size)
array_b = np.arange(size)

start_time = time.time()
np_result = np.dot(array_a, array_b)
end_time = time.time()

print(f"NumPy dot product result: {np_result}")
print(f"Time taken with NumPy arrays: {(end_time - start_time) * 1000:.2f} ms")
```

On a typical machine, you'll see that the NumPy version is **100x to 1000x faster**. Why?

1. **Less Overhead:** NumPy arrays are dense, fixed-type arrays in memory. Python lists are arrays of pointers to objects, adding a layer of indirection and overhead.
2. **Vectorization & C-Magic:** NumPy operations are implemented in C and Fortran. Instead of iterating in slow Python, NumPy performs the entire operation in a single, optimized C loop. This is called **vectorization**.

For a programmer, this is the killer feature: write concise Python code that runs at near-C speeds.

## The Heart of NumPy: The `ndarray`

The core data structure is the `ndarray` (n-dimensional array). Let's create one and see what it's made of.

```python
# Create a 2D array (a "matrix") from a Python list of lists
matrix = np.array([[1, 2, 3], 
                   [4, 5, 6]])

print(matrix)
```

Every array has a few vital attributes:

- `.ndim`: The number of dimensions (or "axes"). Our `matrix` has 2.
- `.shape`: A tuple indicating the size of the array in each dimension. Our `matrix` has a shape of `(2, 3)` (2 rows, 3 columns).
- `.size`: The total number of elements in the array. Our `matrix` has a size of 6.
- `.dtype`: The data type of the elements. NumPy arrays are **homogeneous**—all elements must be the same type. This is key to its performance!

```python
print(f"Dimensions: {matrix.ndim}")  # Prints: 2
print(f"Shape: {matrix.shape}")      # Prints: (2, 3)
print(f"Size: {matrix.size}")        # Prints: 6
print(f"Data type: {matrix.dtype}")  # Prints: int64 (or similar, depending on your system)
```

You can explicitly set the data type on creation or change it later with `.astype()`, which creates a *new* array.

```python
float_matrix = matrix.astype(np.float32)
print(f"New data type: {float_matrix.dtype}") # Prints: float32
```

## Creating Arrays: Your Building Blocks

You'll rarely create large arrays from lists. Instead, NumPy provides a suite of functions to create arrays from scratch:

```python
# Create an array of all zeros
zeros_array = np.zeros((2, 3)) 
print(f"Zeros:\n{zeros_array}\n")

# Create an array of all ones
ones_array = np.ones((3, 2), dtype=np.int16)
print(f"Ones:\n{ones_array}\n")

# Create a 2x2 identity matrix (1s on the diagonal)
identity_matrix = np.eye(2)
print(f"Identity:\n{identity_matrix}\n")

# Create an array with a range of elements (like Python's range)
ranged_array = np.arange(0, 10, 2) # Start, stop (exclusive), step
print(f"Ranged array: {ranged_array}\n")

# Create an array with evenly spaced numbers over an interval
linspace_array = np.linspace(0, 1, 5) # Start, stop (inclusive), number of points
print(f"Linspace array: {linspace_array}\n")
```

## Universal Functions (Ufuncs): Element-wise Magic

Remember vectorization? Ufuncs are the workhorses that make it happen. They perform element-wise operations on arrays.

Instead of writing this:

```python
matrix = np.array([[1, 2, 3], [4, 5, 6]])
new_matrix = np.zeros(matrix.shape)

for i in range(matrix.shape[0]):
    for j in range(matrix.shape[1]):
        new_matrix[i, j] = matrix[i, j] + 10
```

You just write this:

```python
matrix = np.array([[1, 2, 3], [4, 5, 6]])
new_matrix = matrix + 10 # So clean!
print(f"Matrix + 10:\n{new_matrix}")
```


This works for all basic arithmetic (`+`, `-`, `*`, `/`, `**`). You can also perform these operations between two arrays of the same shape.

Need to sum up all elements? Or just along a row or column? Use an aggregate function with the `axis` parameter.

- `axis=0` collapses along the rows (computes a result for each column).
- `axis=1` collapses along the columns (computes a result for each row).

```python
x = np.array([[1, 2], [3, 4]])

print(f"Sum of all elements: {np.sum(x)}")         # Prints: 10
print(f"Sum of each column: {np.sum(x, axis=0)}")  # Prints: [4 6]
print(f"Sum of each row: {np.sum(x, axis=1)}")      # Prints: [3 7]
```

## Indexing and Slicing: Getting What You Need

This is where NumPy's power truly shines. It's like Python list slicing on steroids.

### Basic Slicing

You specify a slice for each dimension, separated by commas.

```python
a = np.array([[1, 2, 3, 4], 
              [5, 6, 7, 8], 
              [9, 10, 11, 12]])

# Get the first two rows and columns 1 and 2
# (row 0 and 1, column 1 and 2)
b = a[:2, 1:3] 
print(f"Sliced array b:\n{b}") # Prints [[2 3], [6 7]]
```

### Advanced Indexing: Integer and Boolean

This is where it gets really cool.

**Integer Array Indexing (or "Fancy Indexing"):** Use lists or arrays of integers to pick out exactly the elements you want.

```python
# Create an array
a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
print(f"Original array a:\n{a}\n")

# Create an array of indices to select one element from each row
b_indices = np.array([0, 2, 0, 1])

# Select one element from each row of a using the indices in b
# This selects a[0,0], a[1,2], a[2,0], a[3,1]
selected = a[np.arange(4), b_indices]
print(f"Selected elements: {selected}")  # Prints: [ 1  6  7 11]

# We can also use this to modify elements
a[np.arange(4), b_indices] += 100
print(f"\nModified array a:\n{a}")
```

**Boolean Array Indexing:** This is a game-changer. Select elements based on a condition.

```python
bool_idx = (a > 10) # This creates a boolean array!
print(f"\nBoolean mask (a > 10):\n{bool_idx}\n")

# Use the boolean mask to select only the elements that are > 10
print(f"Elements > 10: {a[bool_idx]}") # or just a[a > 10]
```

## A Critical Gotcha: Views vs. Copies

This is one of the most important concepts for new NumPy users. **Basic slicing creates a *view* of the original array, not a copy.**

A view is just a different way of looking at the *same underlying data*. Modifying a view **will modify the original array**.

the .view() method is incredibly powerful—it reshapes a tensor without copying the data. It's a "free" operation that just creates a new view of the same underlying data.

The -1 trick is particularly useful: it tells PyTorch to automatically infer that dimension based on the total number of elements. For example, if you have a tensor with 12 elements and call .view(3, -1), PyTorch will automatically make it (3, 4)

```python
# Create a tensor with initial shape
original = torch.arange(12)
print("Original shape:", original.shape)
print("Original tensor:", original)

# Reshape using view with -1 trick
reshaped = original.view(2, -1)
print("\nReshaped to (2, -1):\n", reshaped)
print("New shape:", reshaped.shape)
```

```python
ary = np.array([[1, 2, 3], [4, 5, 6]])
print(f"Original array:\n{ary}\n")

# Create a view of the first row
first_row_view = ary[0]
first_row_view += 100 # Modify the view

print(f"Modified first row: {first_row_view}")
print(f"\nOriginal array is CHANGED:\n{ary}")
```

This is done for performance, to avoid copying large amounts of data. If you *want* a copy, you must explicitly ask for one using the `.copy()` method.

```python
copied_first_row = ary[0].copy()
copied_first_row -= 50 # Modify the copy

print(f"\nModified copy: {copied_first_row}")
print(f"Original array is UNCHANGED:\n{ary}")
```

**Rule of thumb:**

- Basic slicing creates views.
- Advanced indexing (integer or boolean) creates copies.

## Squeeze, Unsqueeze and View

- Adding and Removing Dimensions

unsqueeze() and squeeze() are essential for managing tensor dimensions. The most common practical use case: You will use unsqueeze(0) constantly to add a 'batch' dimension to a single data point before feeding it to a model.

unsqueeze(dim): Adds a dimension of size 1 at the specified position
squeeze(dim): Removes a dimension of size 1 at the specified position (or all size-1 dimensions if no dim specified)

```python
# Create a tensor
tensor = torch.tensor([1, 2, 3, 4, 5])
print("Original shape:", tensor.shape)
print("Original tensor:", tensor)

# Add a dimension at position 0 (adds batch dimension)
with_batch = tensor.unsqueeze(0)
print("\nAfter unsqueeze(0):")
print("Shape:", with_batch.shape)
print("Tensor:\n", with_batch)

# Remove the dimension we just added
back_to_original = with_batch.squeeze(0)
print("\nAfter squeeze(0):")
print("Shape:", back_to_original.shape)
print("Tensor:", back_to_original)

tensor = torch.tensor(2)
print(tensor.shape) # Output: torch.Size([])
batch = tensor.unsqueeze(0) # Add batch dimension
print(batch.shape) # Output: torch.Size([1])

# This is especially useful when we need to match tensors shape, and when there's a shape mismatch error between the model and the input data (Main problem while working with single data points instead of batches).

# Squeeze removes dimensions of size 1 from the tensor.
squeezed = batch.squeeze(0) # Remove batch dimension
print(squeezed.shape) # Output: torch.Size([])

# view is another function that serves as a reshaping operation. It allows you to specify the desired shape of the tensor.
tensor = torch.tensor([0, 1, 2, 3, 4]) # tensor([0, 1, 2, 3, 4])
tensor.shape                           # torch.Size([5])

# The first parameter of the view function is the row and the second is the column.

reshaped = tensor.view(5, 1) # Reshape to have a batch dimension
print(reshaped.shape) # Output: torch.Size([5, 1])
# we went from a tensor with 1 dimension, a list of 5 elements, to a new 2-dimensional tensor with 5 rows and 1 column.
# The size went from [5] to [5, 1].

# If we have a tensor with a dynamic size, we can use the -1 placeholder to let PyTorch automatically infer the size of that dimension. For example, if we want to reshape a tensor to have 1 column but don't know the number of rows in advance, we can do the following:   
dynamic_reshaped = tensor.view(-1, 1) # Automatically infer the number of rows
print(dynamic_reshaped.shape) # Output: torch.Size([5, 1])
```

## Slicing + math

```python
tensor = torch.tensor([
    [1, 2, 3, 4],
    [5, 6, 7, 8],
    [9, 10, 11, 12]
])

second_row_third_column = tensor[1, 2] # second_row_third_column = tensor[1][2] same thing
last_row = tensor[-1]
second_row_third_column # tensor(7)
last_row                # tensor([9, 10, 11, 12])


# Math operations
data = torch.tensor([10.0, 20.0, 30.0, 40.0, 50.0])
data.mean() # tensor(30.)
data.std()  # tensor(15.8114)

# Dot Product
t1 = torch.tensor([1, 2, 3])
t2 = torch.tensor([4, 5, 6])

torch.dot(t1, t2) # tensor(32)
# 1 * 4 + 2 * 5 + 3 * 6
# 4 + 10 + 18
# 32

# Derivatives

x = torch.tensor(2.0, requires_grad=True) # tensor(2., requires_grad=True)
y = x ** 2                                # tensor(4., grad_fn=<PowBackward0>)
y.backward() # x^2 derivative = 2x (2*2) = 4
x.grad                                    # tensor(4.)
```

- For a tensor of value 2, we use it to build a linear relationship for y. Calling backward on y, it computes the gradients of y with respect to all tensors that y depends on, in this case, x.

- The gradient is stored in the grad attribute of x in the form of 2 * x (which is the derivative of x ** 2). When calling x.grad, it passes x = 2.0 and returns 2 * 2.0 = 4.0 in the form of a tensor tensor(4.).

## Reshaping and Concatenating

Often your data isn't in the right shape. NumPy makes it trivial to rearrange it.

```python
a = np.arange(12) # A 1D array of 0-11
print(f"Original: {a}\n")

# Reshape it into a 3x4 matrix
b = a.reshape(3, 4)
print(f"Reshaped to 3x4:\n{b}\n")

# Flatten it back to 1D
c = b.ravel() # ravel() creates a view if possible
d = b.flatten() # flatten() always creates a copy
print(f"Raveled: {c}\n")

# Transpose it (swap rows and columns)
print(f"Transposed:\n{b.T}\n")

# Join arrays together
ary1 = np.array([[1,2], [3,4]])
ary2 = np.array([[5,6]])
# Join along rows (axis=0)
print(f"Concatenated (axis 0):\n{np.concatenate([ary1, ary2], axis=0)}\n")
# Join along columns (axis=1) - shapes must match!
print(f"Concatenated (axis 1):\n{np.concatenate([ary1, ary1], axis=1)}\n")
```

## Broadcasting: The Silent Hero

Broadcasting is a powerful mechanism that allows NumPy to perform operations on arrays of different shapes. It can feel like magic, but it follows strict rules.

The simplest example is one we've already seen: `array + 5`. We are broadcasting the scalar `5` to every element in the array.

A more complex example is adding a vector to each row of a matrix.

```python
# A 4x3 matrix
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
# A 1x3 vector
v = np.array([1, 0, 1])

# Add v to each row of x
y = x + v 

print(f"Original matrix x:\n{x}\n")
print(f"Vector v: {v}\n")
print(f"Result of broadcasting y = x + v:\n{y}")
```

![Broadcasting](/assets/images/pytorch/np_broadcasting.png)

How does this work? NumPy compares the shapes of the two arrays dimension-by-dimension, starting from the right. Two dimensions are compatible if:
1. They are equal, or
2. One of them is 1.

NumPy then "stretches" the dimension with size 1 to match the other.

## A Quick Dip into Linear Algebra

NumPy is the language of linear algebra in Python.

```python
m = np.array([[1,2,3], [4,5,6]]) # (2,3)
v = np.array([1,0,1]) # (3,)

# Matrix-vector multiplication
# The @ symbol is the modern, preferred way
result = m @ v 
print(f"m @ v = {result}") # (2,3) @ (3,) -> (2,)

# For explicit matrix-matrix multiplication, shapes must be compatible
# (m, n) @ (n, p) -> (m, p)
m1 = np.arange(6).reshape(2,3)
m2 = np.arange(6).reshape(3,2)

print(f"\nMatrix multiplication:\n{m1 @ m2}")
# You can also use np.matmul(m1, m2)
```

[More here](https://www.labri.fr/perso/nrougier/from-python-to-numpy/)

---

## PyTorch Primer

Python based deep learning library which makes training neural networks way easier instead of doing math by hand (it's more than just math, loading you data, cleaning your data, training data, calculating loss, updating gradients, finding best model at a specific iteration/epoch)

### Internals

![PyTorchOverview](/assets/images/pytorch/pytorchoverview.png)

- **Tensor library** is very similar to numpy with support for GPU acceleration.
- **Autograd library** is where the heart of PyTorch is, no need to compute backprop by hand anymore!! PyTorch allows you to save gradients and creates a graph of all gradients of weights and biases so it can update the weights easily.
- **DL library** has pretty cool things too, prebuilt loss functions, models, datasets, optimizers which are all used in modern time.

### Installation

```bash
pip install torch
```

```python
import torch
print(torch.__version__)
# 2.0.0

print(torch.cuda.is_available())
# False # this would print True if you are using Google Collab GPU or personal GPU. Not on apple silicon chips / CPU 
# (for M-series chips you have print(torch.backends.mps.is_available()) which makes inference really faster)
```

### What is a Tensor?

Fancy word for a data container / array.
- **Scalar** is a tensor of rank 0
- **Vector** is tensor of rank 1
- **Matrix** is tensor of rank 2

![tensor](/assets/images/pytorch/tensor.png)

How are they stored?
A tensor is a mathematical concept. But to represent it on our computers, we have to define some sort of physical representation for them. The most common representation is to lay out each element of the tensor **contiguously in memory**, writing out each row to memory.

![CT](/assets/images/pytorch/contigous_tensor.png)

[matrix]: 2d tensor

- Whenever a function is called for an operation (like torch.dot(x, y)) -> PyTorch internally does two things:
  - calls the dynamic dispath which checks for dot product implementation based on kernel (CPU implementation, CUDA implementation) 
  - same for dtype (float, double, int) (This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.)
    - it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on float is different from the code for int

![tw](/assets/images/pytorch/tensor_wrapper.png)

- The device, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla). The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.
- The layout, which describes how we logically interpret this physical memory. The most common layout is a strided tensor (which maps the mathematical tensor -> physical location in memory like we talked about), but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.
- The dtype, which describes what it is that is actually stored in each element of the tensor. This could be floats or integers, or it could be, for example, quantized integers.

## When you call a function like torch.add, what actually happens? If you remember the discussion we had about dispatching, you already have the basic picture in your head:

- We have to translate from Python realm to the C++ realm (Python argument parsing)
- We handle variable dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)
- We handle device type / layout dispatch (Type)
- We have the actual kernel, which is either a modern native function, or a legacy TH function.
- They hold multi-dimensional data where each dimension represents a feature.



Taking a cricketer -> Virat Kohli
Virat Kohli Tensor would have multiple dimensions (Batting, Bowling, Fielding, Coaching) basically becoming a big matrix with these features and values for them.

```python
# How to create tensors and play with them
tensor0d = torch.tensor(1)
tensor1d = torch.tensor([1,2,3])
tensor2d = torch.tensor([[1, 2], [3, 4]])
tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

print(tensor3d.dtype) # torch.int64 (PyTorch adopts the default 64-bit integer data type from Python)
print(tensor2d.shape) # torch.Size([2, 2]) matrix of 2x2
# You can reshape arrays
tensor2d.reshape(1,2)
print(tensor2d)

matrix1 = torch.tensor([
                            [1, 2, 3],
                            [4, 5, 6]
                       ]) # shape = 2 * 3

print(matrix1.shape)
print(matrix1.reshape(3,2)) # opposite (we do this a lot while multiplying matrices since we need (p,q) * (q, m) so q = q for them to be multiplied (here p,q,q,m are rows and columns of two matrices))

'''
This prints matrix of 3 * 2 while preserving all elements and now we can use this to multiply to a matrix of (2 x whatever)
tensor([[1, 2],
        [3, 4],
        [5, 6]])
'''
```

- Instead of reshape we just use view, which is same as .eye in numpy

```python
print(matrix1.view(3,2))
```

- For transposing matrices we can use .T

```python
tensor = torch.tensor([1, 2, 3, 4, 5])
# Add a dimension at position 0 (adds batch dimension)
with_batch = tensor.unsqueeze(0)
print(with_batch.shape) # torch.Size([1, 5])

# Remove the dimension we just added
back_to_original = with_batch.squeeze(0)
```

The `.view()` method reshapes a tensor without copying the data. It's a "free" operation.

```python
multiply = matrix1 @ matrix2  # (2 * 3) * (3 * 2) = final matrix (2 * 2)
print(f"After multiplication: {multiply} of shape {multiply.shape}")
```

```text
After multiplication: tensor(
        [[14, 32],
        [32, 77]]) of shape torch.Size([2, 2])

Which makes sense because... um do you really need a tutorial on how matrix multiplication works? ok,, going back to 10th grade I guess.


Matrix1 = [
            [1,2,3]
            [4,5,6]
            ]

Matrix2 = [
        [1, 4],
        [2, 5],
        [3, 6]
        ]

Matrix1 * Matrix2 = every elem of row of m1 gets multiplied with every elem of col of m2 and added
{1*1 + 2*2 + 3*3} = {14} which is the first number
Similarily
{1 * 4 + 2 * 5 + 3 * 6} = {32} which is the second number

and so on...
```

## PyTorch Dimensions

```python
tensor = torch.tensor([1,2,3,4,5]) 
print(tensor.shape) [] # torch.Size([5])
s = torch.sum(tensor,dim=0) 
print(s, s.shape) # scalar has no shape
# tensor(15) torch.Size([])
```

> More operations

```python
tensor = torch.tensor([
        [1,2,3], [4,5,6]
]) # 2x3

print("Dimension 0")
dimo0 = torch.sum(tensor, dim=0) # adds both lists (1 + 4, 2 + 5, 3 + 6)
print(dimo0)
print()
print("Dimension 1")
dimo1 = torch.sum(tensor, dim=1) # adds inner list (1+2+3, 4+5+6)
print(dimo1)

print("Inner most (just like last access in list)")
dims = torch.sum(tensor, dim=-1)
print(dims)
'''
torch.Size([2, 3])
Dimension 0
tensor([5, 7, 9])

Dimension 1
tensor([ 6, 15])

Inner most (just like last access in list)
tensor([ 6, 15])
'''
# 3d!!
tensor = torch.tensor([
    [ [1,2,3], [4,5,6] ],
    [ [7,8,9], [10,11,12] ]
])
print(tensor.shape) # 2 outer dimensions, each has 2 elements (2,2,3)
```

## Coming to the big guns -> autograd

- PyTorchs autograd system provides functions to compute gradients in dynamic computational graphs (directed graph which we can use to visualise math expressions) automatically.
- Computation graph lays out the sequence of calculations needed to compute the output of a neural network 

> Let's write a simple linear regression problem and calculate loss (we have already done this by hand, let's see how pytorch handles this)

## How autograd works (pictures)

![ag1](/assets/images/pytorch/autogradmeta.png)
![ag2](/assets/images/pytorch/autogradops.png)
![ag1](/assets/images/pytorch/autograd_graph.png)
![ag1](/assets/images/pytorch/autogradengine.png)

- By default, PyTorch destroys the computation graph after calculating the gradients to free memory. However, since we are going to reuse this computation graph shortly, we set retain_graph=True so that it stays in memory.

- For instance, we can call .backward on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph, which will be stored via the tensors’ .grad attributes: instead of calling for looping grad and setting retain_graph=True for all tensors

```python

# Create neural net.
- When implementing a neural network in PyTorch, we typically subclass the torch.nn.Module class to define our own custom network architecture. This Module base class provides a lot of functionality, making it easier to build and train models.
- We define the network layers in the __init__ constructor and specify how they interact in the forward method. The forward method describes how the input data passes through the network and comes together as a computation graph.

### Model

```text
NeuralNetwork(
  (layers): Sequential(
    (0): Linear(in_features=50, out_features=30, bias=True)
    (1): ReLU()
    (2): Linear(in_features=30, out_features=20, bias=True)
    (3): ReLU()
    (4): Linear(in_features=20, out_features=3, bias=True)
  )
)
```

- A linear layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes also referred to as a feedforward or fully connected layer.

- We want to keep using small random numbers as initial values for our layer weights, we can make the random number initialization reproducible by seeding PyTorch’s random number generator via manual_seed:
- grad_fn=<AddmmBackward0> means that the tensor we are inspecting was created via a matrix multiplication and addition operation
- Addmm stands for matrix multiplication (mm) followed by an addition (Add).
- If we just want to use a network without training or backpropagation, for example, if we use it for prediction after training, constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. So, when we use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the torch.no_grad()

- In PyTorch, it’s common practice to code models such that they return the outputs of the last layer (logits) without passing them to a nonlinear activation function. (So for inference/computation class membership: use softmax fn)

# Setting up efficient data loaders

- What we iterate over while training a the model.

Custom dataset class: defines how individual data records are loaded. -> instantiate -> training dataset -> DataLoader class -> instantiate -> training dataloader & test dataset -> test dataloader
Each dataset object is fed to a data loader -> each dataloader object handles dataset shuffling, assembling data records into batches etc...

> num_workers

- This parameter in PyTorch’s DataLoader function is crucial for parallelizing data loading and preprocessing. When num_workers is set to 0, the data loading will be done in the main process and not in separate worker processes. This might seem unproblematic, but it can lead to significant slowdowns during model training when we train larger networks on a GPU.
- In contrast, when num_workers is set to a number greater than zero, multiple worker processes are launched to load data in parallel, freeing the main process to focus on training your model and better utilizing your system’s resources

- Loading data without multiple workers (setting `num_workers=0`) will create a data loading bottleneck where the model sits idle until the next batch is loaded as illustrated in the left subpanel. If multiple workers are enabled, the data loader can already queue up the next batch in the background as shown in the right subpanel.

- However, if we are working with very small datasets, setting num_workers to 1 or larger may not be necessary since the total training time takes only fractions of a second anyway.

> validation set

- we often use a third dataset, a so-called validation dataset, to find the optimal hyperparameter settings. 
- A validation dataset is similar to a test set. However, while we only want to use a test set precisely once to avoid biasing the evaluation, we usually use the validation set multiple times to tweak the model settings.
- We also introduced new settings called model.train() and model.eval(). As these names imply, these settings are used to put the model into a training and an evaluation mode. This is necessary for components that behave differently during training and inference, such as dropout or batch normalization layers.
- It is important to include an optimizer.zero_grad() call in each update round to reset the gradients to zero. Otherwise, the gradients will accumulate, which may be undesired.

```python
# -- autograd --

y = torch.tensor([1.0]) # true label
x1 = torch.tensor([1.1]) # inp
w1 = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)
z = x1 * w1 + b
a = torch.sigmoid(z)
loss = F.binary_cross_entropy(a, y)
print(loss)
grad_L_w1 = grad(loss, w1, retain_graph=True)
print(f"Gradient of Loss wrt Weight: {grad_L_w1}")
grad_L_b = grad(loss, b, retain_graph=True)
print(f"Gradient of Loss wrt Bias: {grad_L_b}")

loss.backward() # instead of doing all that jazz, just call this
print(w1.grad) # same results
print(b.grad)
```

---

```python
import torch
import torch.nn.functional as F
target = torch.tensor([1.0]) 
input = torch.tensor([1.1])
weight = torch.tensor([1.2])
bias = torch.tensor([0.0]) 


# y = mx + c
forward_pass = input * weight + bias

# applying activation function to this
prediction = torch.sigmoid(forward_pass)

loss = F.binary_cross_entropy(prediction, target)
print(loss) # tensor(0.2368) So we have a loss of 0.23 (our goal is to minimise this but that is for later)
```

This is how the graph for this looks like:
![graph](/assets/images/pytorch/autograd_graph.png)

---

- **If we carry out computations in PyTorch, it will build such a graph internally by default if one of its terminal nodes has the requires_grad attribute set to True. This is useful if we want to compute gradients. Gradients are required when training neural networks via the popular backpropagation algorithm (chain rule)**

![ag](/assets/images/pytorch/autograd_for_nn.png)

> we start from the output layer (or the loss itself) and work backward through the network to the input layer. This is done to compute the gradient of the loss with respect to each parameter (weights and biases) in the network, which informs how we update these parameters during training.

- A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input
- chain rule is a way to compute gradients of a loss function with respect to the model’s parameters in a computation graph. This provides the information needed to update each parameter in a way that minimizes the loss function, which serves as a proxy for measuring the model’s performance, using a method such as gradient descent

So how do we do the same thing in PyTorch which we did by hand in numpy?

```python
import torch
import torch.nn.functional as F
from torch.autograd import grad
target = torch.tensor([1.0]) 
input = torch.tensor([1.1])
weight = torch.tensor([1.2], requires_grad=True) # we need to save this and bias for it to compute partial derivative
bias = torch.tensor([0.0], requires_grad=True) 


# y = mx + c
forward_pass = input * weight + bias

# applying activation function to this
prediction = torch.sigmoid(forward_pass)

loss = F.binary_cross_entropy(prediction, target)
grad_loss_weight = grad(loss, weight, retain_graph=True)
grad_loss_bias = grad(loss, bias, retain_graph=True)
print(grad_loss_weight, grad_loss_bias) # (tensor([-0.2319]),) (tensor([-0.2108]),) [We need to update the weights and bias in the opposite direction of this (shift this) so that loss becomes minimum (same thing we were doing in numpy)]

```

> In the case of our neural network model with the two hidden layers above, these trainable parameters are contained in the torch.nn.Linear layers. A linear layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes also referred to as a feedforward or fully connected layer.

- The numbers in the weight matrix will likely differ. This is because the model weights are initialized with small random numbers, which are different each time we instantiate the network

- To make sure the random numbers are reproducible and stay same throughout our training we use manual_seed(with a random number) so it shuffles the random matrix same.

```python
def note_about_randomness():
    Randomness shows up in many places: parameter initialization, dropout, data ordering, etc.
    For reproducibility, we recommend you always pass in a different random seed for each use of randomness.
    Determinism is particularly useful when debugging, so you can hunt down the bug.
    There are three places to set the random seed which you should do all at once just to be safe.
    # Torch
    seed = 0
    torch.manual_seed(seed)
    # NumPy
    import numpy as np
    np.random.seed(seed)
    # Python
    import random
    random.seed(seed)
```

```python

torch.manual_seed(42)

model = NN(50, 3)
print(model.layers[0].weight) # weights of first linear layer (weights and biases)
'''

Parameter containing:
tensor([[ 0.1081,  0.1174, -0.0331,  ...,  0.0253,  0.0718, -0.0862],
        [-0.1400, -0.0546, -0.1085,  ..., -0.0477, -0.0501, -0.1368],
        [-0.0810,  0.0353, -0.0187,  ...,  0.1142,  0.1288, -0.1121],
        ...,
        [-0.0031, -0.0573,  0.0515,  ...,  0.0271, -0.0928, -0.1175],
        [-0.0444, -0.1318, -0.0660,  ...,  0.0647, -0.1230, -0.0531],
        [ 0.0023, -0.1223,  0.0797,  ...,  0.0369,  0.0862,  0.1328]],
       requires_grad=True)
'''

X = torch.rand((1, 50)) # random input tensor
y = model(X) # forward pass
print(y) # 3 outputs

# tensor([[ 0.1685, -0.1599,  0.2402]], grad_fn=<AddmmBackward0>)

# Addmm stands for matrix multiplication (mm) followed by an addition (Add) Which is the same as y = mx + c (forward pass)
```

## What is the forward pass?

- The forward pass refers to calculating output tensors from input tensors. This involves passing the input data through all the neural network layers, starting from the input layer, through hidden layers, and finally to the output layer.


## What do we do at inference time?

- We don't want to train or backpropagate, just use it for prediction post training there is no need to compute the graph (consumes unncessary memory when we don't need it)
- So during inference time we use torch.no_grad()


```python
model = NN(50, 3) # 50 inp, 3 out
X = torch.rand((1, 50)) # random input tensor
with torch.no_grad():
    output = model(X)
print(output)

# tensor([[ 0.1073, -0.0125, -0.0168]]) (there is no grad function here we aren't saving it, we are just computing and printing output of our network which is what we need at inference time)


```

- Often we don't need these random numbers, we want them to be normalised in the range of 0-1 (i.e: Apply an activation function along the first dimension (the numbers))

```python
model = NN(50, 3) # 50 inp, 3 out
X = torch.rand((1, 50)) # random input tensor
with torch.no_grad():
    output = torch.softmax(model(X), dim=1) # softmax to convert to probabilities
print(output)
# tensor([[0.3101, 0.3544, 0.3355]]) (And now we take the highest (second number here and that is our prediction/answer)) {All these class prob sum upto 1, you can read more about softmax function its a fun trick in math which basically maps all numbers b/w 0-1 and they sum upto 1}

```

> How softmax formula looks like
$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

### MLP

torchvision.datasets

- PyTorch's torchvision.datasets module provides easy access to many popular datasets, including MNIST. It handles downloading, extracting, and organizing the data for you.

DataLoader

- The DataLoader is a crucial component that groups your data into batches, shuffles it (for training), and makes it easy to loop over. Without it, you'd have to manually manage batches, shuffling, and iteration—tasks that DataLoader handles efficiently.

transforms.ToTensor()

- The transforms.ToTensor() step converts PIL images or NumPy arrays into PyTorch tensors. It also automatically scales pixel values from [0, 255] to [0.0, 1.0], which is the standard format neural networks expect. 

# For any model you need three things

- Loading dataset
- Initialising model setup
- Actual training and inference

## Loading dataset

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define transforms: Convert to tensor and Normalize
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,)) # MNIST mean and std
])

# Download datasets
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Create DataLoaders
batch_size = 64
train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_dataloader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False
)

print(f"Training samples: {len(train_dataset)}")
print(f"Test samples: {len(test_dataset)}")
print(f"Batch size: {batch_size}")

```

Let's think about what we expect:

Batch size: We set batch_size=64, so each batch contains 64 images.
Image dimensions: MNIST images are grayscale (1 channel) and are 28x28 pixels.
Shape convention: PyTorch uses the format (batch_size, channels, height, width).
Therefore, the shape of a single batch should be (64, 1, 28, 28).

The labels will be a 1D tensor of shape (64,) containing the digit labels (0-9).kj:w

nn.Module - The Base Class
All PyTorch models inherit from nn.Module, the base class for all neural network modules. When you create a model, you must define two essential methods:

- __init__(self): Where you define the layers of your network. This is where you instantiate all the layers (Linear, ReLU, etc.) that will be used in the forward pass.

- forward(self, x): Where you define how data flows through those layers. This method is called automatically when you do model(X) or model.forward(X).

The Layers We'll Use

- nn.Flatten(): Converts the 2D image tensor (1, 28, 28) into a 1D vector (784). This is necessary because fully-connected layers expect 1D input.

- nn.Linear(in_features, out_features): A standard fully-connected (dense) layer. It performs the operation y = xW^T + b, where W is a weight matrix and b is a bias vector.

- nn.ReLU(): The Rectified Linear Unit activation function. It applies max(0, x) element-wise, introducing non-linearity into the network. Non-linearity is essential for neural networks to learn complex patterns.

# Initialising the model
```python
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        # flatten
        self.flatten = nn.Flatten()
        # first linear layer transformation (first y=mx+c) : 28*28(784) -> 128
        self.linear1 = nn.Linear(784, 128)
        self.activation = nn.ReLU()
        self.linear2 = nn.Linear(128, 64) # second layer (MNIST has 2 hidden layers we are using)
        self.activation = nn.ReLU()
        self.linear3 = nn.Linear(64, 10) # take 64 inputs from hidden layer 2 and output the final 10 class labels

    def forward(self, x):
        x = self.flatten(x)
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.linear3(x)
        return x # raw logits (no softmax here, CrossEntropyLoss handles it)
    
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
model = MLP().to(device)
print(model)

'''
MLP(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear1): Linear(in_features=784, out_features=128, bias=True)
  (activation): ReLU()
  (linear2): Linear(in_features=128, out_features=64, bias=True)
  (linear3): Linear(in_features=64, out_features=10, bias=True)
)
'''
```

The Training Essentials
To train a neural network, you need three components:

- Loss Function: Measures how wrong the model's predictions are. For classification tasks with multiple classes, we use nn.CrossEntropyLoss. It combines a softmax activation and negative log-likelihood loss in one efficient operation.

- Optimizer: Adjusts the model's weights to reduce the loss. torch.optim.Adam is a popular choice—it's an adaptive learning rate algorithm that works well for most problems. The optimizer needs access to the model's parameters (model.parameters()) so it knows which weights to update.

The Training Loop: The iterative process of:

- Feeding data to the model
- Computing the loss
- Updating the weights
- Repeating until the model learns

# Training model

Each training iteration consists of five critical steps:

- Forward Pass: Pass the batch of images through the model to get predictions. The model outputs raw scores (logits) for each of the 10 digit classes.

- Calculate Loss: Compare the model's predictions to the true labels using the loss function. This gives us a single number representing how wrong the model is.

- Backpropagation: Calculate the gradients (the direction and magnitude of error). This is what loss.backward() does—it computes gradients for all parameters in the model.

- Update Weights: The optimizer takes a "step" in the right direction to reduce the loss. This is optimizer.step()—it updates all the model's parameters using the computed gradients.

- Zero Gradients: Reset the gradients to zero before the next batch. This is optimizer.zero_grad()—critical because PyTorch accumulates gradients by default, and we want fresh gradients for each batch.


Code for training + evaluation (on test) + inference (on specific image)

```python
print("Evaluation")

def evaluate(dataloader, model):
    model.eval() # set to evaluation mode 
    correct, total = 0, 0

    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            preds = logits.argmax(dim=1)
            correct += (preds == y).sum().item()
            total += y.size(0)

    print(f"Test accuracy: {correct / total:.4f}")

evaluate(test_dataloader, model)

# --------
print("Starting inference on a sample image from test dataset")
def inference():
    # sample input (digit 8) and predict its digit 8 and also show to verify
    # set to inference/eval mode
    model.eval()
    with torch.no_grad():
        sample_idx = 0
        sample_img, sample_label = test_dataset[sample_idx]
        sample_img = sample_img.to(device)
        print(f"Actual label: {sample_label}")
        sample_img_reshaped = sample_img.unsqueeze(0) # add batch dimension
        pred_logits = model(sample_img_reshaped)
        predicted_label = pred_logits.argmax(dim=1)
        print(f"Predicted label: {predicted_label}")
```

## Looking at the entire code with output

```python

import torch
from torch import nn
import torch.nn.functional as F
import tqdm
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# step1: training and testing dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,)) # mnist mean and std (why normalise? to speed up convergence during training and avoid getting stuck in local minima)
])
train_dataset = datasets.MNIST(root='./data/', train=True, download=False, transform=transform)
test_dataset = datasets.MNIST(root='./data/', train=False, download=False, transform=transform)
batch_size = 64
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
torch.manual_seed(42) # for reproducibility

print(f"Training samples: {len(train_dataset)}")
print(f"Test samples: {len(test_dataset)}")
print(f"Batch size: {batch_size}")

# step2: setting up the model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        # self.net = nn.Sequential(
        #     nn.Flatten(),
        #     nn.Linear(784, 128),
        #     nn.ReLU(),
        #     nn.Linear(128, 64),
        #     nn.ReLU(),
        #     nn.Linear(64, 10)
        # )

        
        # flatten
        self.flatten = nn.Flatten()
        # first linear layer transformation (first y=mx+c) : 28*28(784) -> 128
        self.linear1 = nn.Linear(784, 128)
        self.activation = nn.ReLU()
        self.linear2 = nn.Linear(128, 64) # second layer (MNIST has 2 hidden layers we are using)
        self.linear3 = nn.Linear(64, 10) # take 64 inputs from hidden layer 2 and output the final 10 class labels

    def forward(self, x):
        # return self.net(x)

        x = self.flatten(x)
        x = self.linear1(x)
        x = self.activation(x)
        x = self.linear2(x)
        x = self.activation(x)
        x = self.linear3(x)
        return x # raw logits

# step3: training and inference

device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
model = MLP()
model.to(device)
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)
print(model)
print("Loss function:", loss_fn)
print("Optimiser:", optimiser)

def train(dataloader, optimizer, loss_fn, epochs=50, model=model):
    model.train() # necessary to put the model in training mode
    for epoch in range(epochs):
        total_loss, batch = 0, 0
        for batch_no, (inp, target) in enumerate(tqdm.tqdm(dataloader) ):

            inp = inp.to(device)
            target = target.to(device)
            # zero gradient for next iteration so weights don't accumulate (Gradients accumulate by default in PyTorch)
            optimiser.zero_grad()

            # 2. Forward pass
            preds = model(inputs)
            
            # 3. Calculate loss
            loss = loss_fn(preds, targets)
            
            # 4. Backward pass
            loss.backward()
            
            # 5. Optimize
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({"loss": loss.item()})
            
        print(f"Epoch {epoch+1} Avg Loss: {total_loss/len(dataloader):.4f}")

print("Starting training...")
train(train_loader, model, loss_fn, optimizer)
```

### 4. Evaluation & Inference

During inference, we don't need gradients. Use `torch.no_grad()` to save memory and computation.

```python
def evaluate(dataloader, model):
    model.eval() # Set to evaluation mode
    correct = 0
    total = 0

    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            preds = logits.argmax(dim=1)
            correct += (preds == y).sum().item()
            total += y.size(0)

    print(f"Test accuracy: {correct / total:.4f}")

evaluate(test_dataloader, model)

# --------
print("Starting inference on a sample image from test dataset")
def inference():
    # sample input (digit 8) and predict its digit 8 and also show to verify
    # set to inference/eval mode
    model.eval()
    with torch.no_grad():
        sample_idx = 0
        sample_img, sample_label = test_dataset[sample_idx]
        sample_img = sample_img.to(device)
        print(f"Actual label: {sample_label}")
        sample_img_reshaped = sample_img.unsqueeze(0) # add batch dimension
        pred_logits = model(sample_img_reshaped)
        predicted_label = pred_logits.argmax(dim=1)
        print(f"Predicted label: {predicted_label}")


inference()
print("Inference completed")


'''
  warnings.warn(
Training samples: 60000
Test samples: 10000
Batch size: 64
MLP(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear1): Linear(in_features=784, out_features=128, bias=True)
  (activation): ReLU()
  (linear2): Linear(in_features=128, out_features=64, bias=True)
  (linear3): Linear(in_features=64, out_features=10, bias=True)
)
Loss function: CrossEntropyLoss()
Optimiser: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

Training completed
Evaluation
Test accuracy: 0.9797
Starting inference on a sample image from test dataset
Actual label: 7
Predicted label: tensor([7], device='mps:0')
'''
```

## Enough with the basics, how do I write the same MNIST code with 2 hidden layers with pytorch?

- Input: 28×28 → 784
- Hidden layer 1: 784 → H1
- Hidden layer 2: H1 → H2
- Output: H2 → 10 (10 numbers)
- ReLU activations (if negative -> make it 0 else let it be)
- Cross-entropy loss (logits, no softmax in forward, raw loss)
        - CrossEntropyLoss = LogSoftmax + NLLLoss internally.

```python
#!/usr/bin/env python3

import torch
import torch.nn.functional as F
from torch.autograd import grad
from torchvision import datasets, transforms # for datasets and transformations
from torch.utils.data import DataLoader # to load data


'''
How a training loop looks like:
- any neural network / deep learning class has torch.nn.Module as a base class
- all neural networks are initialised in the init method
- there is a forward pass which is here y = mx + c (the magic) happens
- there is a loss calculation phase, and .backward() which is for backprop
- also updation of weights 
'''
class MNIST(torch.nn.Module):
    '''
    h1 = ReLU(XW1 + b1)
    h2 = ReLU(h1W2 + b2)
    logits = h2W3 + b3
    '''
    def __init__(self, h1=256, h2=128):
        super().__init__()
        self.fc1 = torch.nn.Linear(28*28, h1)
        self.fc2 = torch.nn.Linear(h1, h2)
        self.fc3 = torch.nn.Linear(h2, 10)

    def forward(self, x):
        # x: [B, 1, 28, 28] (batch size, channels, height, width)

        x = x.view(-1, 28*28) # # flatten → [B, 784]
        x = F.relu(self.fc1(x)) # hidden layer 1 with ReLU activation
        x = F.relu(self.fc2(x)) # hiddfen layer 2 with ReLU activation
        x = self.fc3(x)
        return x


transforms = transforms.ToTensor() # transform to convert images to tensors

# make the training and testing set
train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms, download=True) 
test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms)  

# load training and testing set via DataLoader 
# parameters, batch_size = how many images does the model see at once
train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)  

model = MNIST()
# GPU selection
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)


# learning rate gradient descent (how to take steps towards achieving the loss function minima)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
num_epochs = 5 # iterations

for epoch in range(num_epochs):
    model.train() # set the model to training mode (important)
    for batch_idx, (data, target) in enumerate(train_loader): # data : to be predicted input, target: what we want
        optimizer.zero_grad() # zero the gradients (necessary cleanup for good training)
        output = model(data) # forward pass
        loss = F.cross_entropy(output, target) # compute the loss
        loss.backward() # backward pass
        optimizer.step() # update the weights

        if batch_idx % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')

    model.eval() # set the model to evaluation mode 
    correct = 0
    total = 0
    with torch.no_grad():
        # test again test dataset
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')

# Save the model checkpoint
torch.save(model.state_dict(), 'mnist_model.pth')
```

- Output

```text
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 3303368.96it/s]
Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw

Epoch [1/5], Step [1/938], Loss: 2.3024
Epoch [1/5], Step [101/938], Loss: 0.9854
Epoch [1/5], Step [201/938], Loss: 0.5473
Epoch [1/5], Step [301/938], Loss: 0.2938
Epoch [1/5], Step [401/938], Loss: 0.4356
Epoch [1/5], Step [501/938], Loss: 0.2321
Epoch [1/5], Step [601/938], Loss: 0.1121
Epoch [1/5], Step [701/938], Loss: 0.2327
Epoch [1/5], Step [801/938], Loss: 0.1148
Epoch [1/5], Step [901/938], Loss: 0.3475
Accuracy of the model on the test images: 92.91%
Epoch [2/5], Step [1/938], Loss: 0.1527
Epoch [2/5], Step [101/938], Loss: 0.1537
Epoch [2/5], Step [201/938], Loss: 0.1619
Epoch [2/5], Step [301/938], Loss: 0.1462
Epoch [2/5], Step [401/938], Loss: 0.1451
Epoch [2/5], Step [501/938], Loss: 0.2443
Epoch [2/5], Step [601/938], Loss: 0.0834
Epoch [2/5], Step [701/938], Loss: 0.1496
Epoch [2/5], Step [801/938], Loss: 0.2065
Epoch [2/5], Step [901/938], Loss: 0.2134
Accuracy of the model on the test images: 95.22%
Epoch [3/5], Step [1/938], Loss: 0.2393
Epoch [3/5], Step [101/938], Loss: 0.0949
Epoch [3/5], Step [201/938], Loss: 0.1192
Epoch [3/5], Step [301/938], Loss: 0.0756
Epoch [3/5], Step [401/938], Loss: 0.1127
Epoch [3/5], Step [501/938], Loss: 0.1054
Epoch [3/5], Step [601/938], Loss: 0.1361
Epoch [3/5], Step [701/938], Loss: 0.0541
Epoch [3/5], Step [801/938], Loss: 0.1342
Epoch [3/5], Step [901/938], Loss: 0.1384
Accuracy of the model on the test images: 96.63%
Epoch [4/5], Step [1/938], Loss: 0.1616
Epoch [4/5], Step [101/938], Loss: 0.0430
Epoch [4/5], Step [201/938], Loss: 0.0624
Epoch [4/5], Step [301/938], Loss: 0.1149
Epoch [4/5], Step [401/938], Loss: 0.0344
Epoch [4/5], Step [501/938], Loss: 0.1722
Epoch [4/5], Step [601/938], Loss: 0.0945
Epoch [4/5], Step [701/938], Loss: 0.0853
Epoch [4/5], Step [801/938], Loss: 0.1009
Epoch [4/5], Step [901/938], Loss: 0.1571
Accuracy of the model on the test images: 97.06%
Epoch [5/5], Step [1/938], Loss: 0.0931
Epoch [5/5], Step [101/938], Loss: 0.0426
Epoch [5/5], Step [201/938], Loss: 0.0144
Epoch [5/5], Step [301/938], Loss: 0.2501
Epoch [5/5], Step [401/938], Loss: 0.0178
Epoch [5/5], Step [501/938], Loss: 0.0938
Epoch [5/5], Step [601/938], Loss: 0.0999
Epoch [5/5], Step [701/938], Loss: 0.0356
Epoch [5/5], Step [801/938], Loss: 0.1522
Epoch [5/5], Step [901/938], Loss: 0.0206
Accuracy of the model on the test images: 97.17%
```

- Now using this to evaluate on a specific number
![sample](/assets/images/pytorch/load_sample_input.png)

```python
import matplotlib.pyplot as plt
import numpy as np
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# Load the model for inference
model = MNIST()
model.load_state_dict(torch.load('mnist_model.pth')) # load the pth model you saved

model.eval()
# Get a sample from the test dataset
dataiter = iter(test_loader)
images, labels = next(dataiter)

# Show the image
imshow(torchvision.utils.make_grid(images[:1]))

# Predict the label
output = model(images[:1]) # slicing (from 0->1 (one image) : prediction on single image)
_, predicted = torch.max(output, 1)
print(f'Predicted Label: {predicted.item()}')
```
- Output

```text
> python3 ./main.py
Predicted Label: 7
```

> So our model works just like it did in numpy, but now its way easier to do this in little lines

## Additional
- how to do this in pure matrix math
        - Without nn.Linear layers for forward pass, let's write it on our own in pytorch

## Training without nn.Linear, using nn.Parameter manually

```python
#!/usr/bin/env python3
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class MLP2Hidden(torch.nn.Module):
    def __init__(self, h1=256, h2=128):

        super().__init__()
        # Manually define weights and biases
        self.W1 = nn.Parameter(torch.randn(784, 128) * 0.01)
        self.b1 = nn.Parameter(torch.zeros(128))
        self.W2 = nn.Parameter(torch.randn(128, 10) * 0.01)
        self.b2 = nn.Parameter(torch.zeros(10))

    def forward(self, x):
        x = x.view(-1, 784) # Flatten
        
        # Layer 1: X @ W + b
        z1 = x @ self.W1 + self.b1
        h1 = F.relu(z1)
        
        # Layer 2
        logits = h1 @ self.W2 + self.b2
        return logits

transform = transforms.ToTensor()
train_ds = datasets.MNIST(root="data", train=True, download=True, transform=transform)
test_ds = datasets.MNIST(root="data", train=False, download=True, transform=transform)

train_loader= DataLoader(train_ds, batch_size=128, shuffle=True)
test_loader= DataLoader(test_ds, batch_size=128)


model = MLP2Hidden()
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for x, y in train_loader:
        optimizer.zero_grad()
        prediction = model(x)
        loss = loss_fn(prediction, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")


# now prediction

def test():
    model.eval()
    with torch.no_grad():
        x, y = next(iter(test_loader))
        prediction = model(x)
        print("Shape: ", prediction.shape)  # [B, 10]
        loss = loss_fn(prediction, y)
        print("Loss: ", loss.item())
        predicted_classes = torch.argmax(prediction, dim=1)
        print("True labels:", y[:10].tolist())
        print("Pred labels:", predicted_classes[:10].tolist())
        accuracy = (predicted_classes == y).float().mean()
        print("Accuracy: ", accuracy.item())

def main():
    test()

main()
```

- As we train for longer, our loss function converges and we get more accurate results:

```text
Using device:  mps
Epoch 1, Loss: 2.3016
Epoch 2, Loss: 2.2995
Epoch 3, Loss: 2.2955
Epoch 4, Loss: 2.2786
Epoch 5, Loss: 2.1592
Epoch 6, Loss: 1.6419
Epoch 7, Loss: 0.9544
Epoch 8, Loss: 0.7263
Epoch 9, Loss: 0.6402
Epoch 10, Loss: 0.5778
Epoch 11, Loss: 0.5228
Epoch 12, Loss: 0.4814
Epoch 13, Loss: 0.4516
Epoch 14, Loss: 0.4279
Epoch 15, Loss: 0.4073
Epoch 16, Loss: 0.3887
Epoch 17, Loss: 0.3720
Epoch 18, Loss: 0.3568
Epoch 19, Loss: 0.3431
Epoch 20, Loss: 0.3305
Shape:  torch.Size([128, 10])
Loss:  0.2947104573249817
True labels: [7, 2, 1, 0, 4, 1, 4, 9, 5, 9]
Pred labels: [7, 2, 1, 0, 4, 1, 4, 9, 6, 9]
```

### Backpropagation for a 2-Hidden-Layer MLP (MNIST)

- Maps a PyTorch MLP implementation directly to the backpropagation equations derived mathematically.

---

## Model Definition

Input:

- $$  X \in \mathbb{R}^{B \times 784}  $$

Parameters:

- $$  W_1 \in \mathbb{R}^{784 \times H_1}, \quad b_1 \in \mathbb{R}^{H_1}  $$
- $$  W_2 \in \mathbb{R}^{H_1 \times H_2}, \quad b_2 \in \mathbb{R}^{H_2}  $$
- $$  W_3 \in \mathbb{R}^{H_2 \times 10}, \quad b_3 \in \mathbb{R}^{10}  $$

---

## Forward Pass

$$
\begin{aligned}
z_1 &= XW_1 + b_1 \\
h_1 &= \text{ReLU}(z_1) \\
z_2 &= h_1W_2 + b_2
\end{aligned}
$$

The output \( z_3 \) are the **logits**.

---

## Loss Function (Cross-Entropy)

For one sample:

$$
L = -\sum_{k=1}^{10} y_k \log(\hat{y}_k)
$$

where:

$$
\hat{y} = \text{softmax}(z_3)
$$

---

## Key Gradient Identity (Softmax + Cross Entropy)

$$
\boxed{
\frac{\partial L}{\partial z_3} = \hat{y} - y
}
$$

This is why softmax is **not implemented explicitly** in code.

---

## Backpropagation

### Output Layer (Layer 3)

$$
\begin{aligned}
\delta_3 &= \hat{y} - y \quad (\text{Error at output}) \\
\frac{\partial L}{\partial W_2} &= h_1^\top \delta_3 \\
\delta_2 &= (\delta_3 W_2^\top) \odot \mathbb{1}(z_1 > 0) \quad (\text{Error backpropagated through ReLU}) \\
\frac{\partial L}{\partial W_1} &= X^\top \delta_2
\end{aligned}
$$

---

### Backprop through ReLU (Layer 2)

ReLU derivative:

$$
\text{ReLU}'(z) =
\begin{cases}
1 & z > 0 \\
0 & z \le 0
\end{cases}
$$

$$
\delta_2 = (\delta_3 W_3^\top) \odot \mathbb{1}(z_2 > 0)
$$

---

### Hidden Layer 2 Parameters

$$
\begin{aligned}
\frac{\partial L}{\partial W_2} &= h_1^\top \delta_2 \\
\frac{\partial L}{\partial b_2} &= \sum_{i=1}^{B} \delta_2^{(i)}
\end{aligned}
$$

---

### Backprop through ReLU (Layer 1)

$$
\delta_1 = (\delta_2 W_2^\top) \odot \mathbb{1}(z_1 > 0)
$$

---

### Hidden Layer 1 Parameters

$$
\begin{aligned}
\frac{\partial L}{\partial W_1} &= X^\top \delta_1 \\
\frac{\partial L}{\partial b_1} &= \sum_{i=1}^{B} \delta_1^{(i)}
\end{aligned}
$$

---

## Gradient Descent Update Rule

For any parameter $ \theta \ $:

$$
\theta \leftarrow \theta - \eta \frac{\partial L}{\partial \theta}
$$

Where $ (\eta \ )$ is the learning rate.

---

## Mapping to PyTorch Code

| PyTorch Code | Mathematical Meaning |
|-------------|---------------------|
| `x @ W + b` | affine transformation |
| `F.relu(z)` | $( \max(0, z) )$ |
| `CrossEntropyLoss` | softmax + NLL |
| `loss.backward()` | chain rule |
| `param.grad` | $ \frac{\partial L}{\partial \theta} $ |
| `optimizer.step()` | gradient descent |

---

## Key Insight

> **PyTorch autograd executes the exact same backpropagation equations derived above, node by node, using the chain rule.**

Nothing is approximated or learned — it is symbolic differentiation.

---

## Setting up dataloaders

![dl](/assets/images/pytorch/data_loader.png)

- Creating your own custom data class
- Dataset of five training examples with two features each.

```python
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import torch
X_train = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
])
y_train = torch.tensor([0, 0, 0, 1, 1])

X_test = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
])

y_test = torch.tensor([0, 1])

class ToyDataset(Dataset):
    def __init__(self, X, y, transform=None):
        self.X = X
        self.y = y
        self.transform = transform

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        image = self.X[idx]
        label = self.y[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

train_ds = ToyDataset(X_train, y_train)
test_ds = ToyDataset(X_test, y_test)

torch.manual_seed(123)

train_loader = DataLoader(
    dataset=train_ds,
    batch_size=2,
    shuffle=True,
    num_workers=0
)

test_ds = ToyDataset(X_test, y_test)

test_loader = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=False,
    num_workers=0
)

for idx, (x, y) in enumerate(train_loader):
    print(f"Batch {idx+1}:", x, y)

'''
Batch 1: tensor([[ 2.3000, -1.1000],
        [-0.9000,  2.9000]]) tensor([1, 0])
Batch 2: tensor([[-1.2000,  3.1000],
        [-0.5000,  2.6000]]) tensor([0, 0])
Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])
'''

```
- As we can see based on the output above, the train_loader iterates over the training dataset visiting each training example exactly once. This is known as a training epoch
- When num_workers is set to 0, the data loading will be done in the main process and not in separate worker processes. This might seem unproblematic, but it can lead to significant slowdowns during model training when we train larger networks on a GPU
- As a result, the GPU can sit idle while waiting for the CPU to finish these tasks. In contrast, when num_workers is set to a number greater than zero, multiple worker processes are launched to load data in parallel, freeing the main process to focus on training your model and better utilizing your system’s resources

![mw](/assets/images/pytorch/multipleworkers.png)

## This is how a training loop looks like (Recap)
```python
import torch.nn.functional as F


torch.manual_seed(123)
model = NeuralNetwork(num_inputs=2, num_outputs=2)
optimizer = torch.optim.SGD(model.parameters(), lr=0.5)

num_epochs = 3

for epoch in range(num_epochs):

    model.train()
    for batch_idx, (features, labels) in enumerate(train_loader):

        logits = model(features)

        loss = F.cross_entropy(logits, labels) # Loss function

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        ### LOGGING
        print(f"Epoch: {epoch+1:03d}/{num_epochs:03d}"
              f" | Batch {batch_idx:03d}/{len(train_loader):03d}"
              f" | Train/Val Loss: {loss:.2f}")

    model.eval()
    # Optional model evaluation
'''
Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75
Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65
Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44
Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13
Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03
Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00
'''
```


- We initialized a model with two inputs and two outputs
- We used a stochastic gradient descent (SGD) optimizer with a learning rate (lr) of 0.5
- Ideally, we want to choose a learning rate such that the loss converges after a certain number of epochs – the number of epochs is another hyperparameter to choose.
- We also introduced new settings called model.train() and model.eval(). As these names imply, these settings are used to put the model into a training and an evaluation mode (best practice)
- We pass the logits directly into the cross_entropy loss function, which will apply the softmax function internally for efficiency and numerical stability reasons
- Calling loss.backward() will calculate the gradients in the computation graph that PyTorch constructed in the background. 
- optimizer.step() method will use the gradients to update the model parameters to minimize the loss. In the case of the SGD optimizer, this means multiplying the gradients with the learning rate and adding the scaled negative gradient to the parameters.

> It is important to include an optimizer.zero_grad() call in each update round to reset the gradients to zero. Otherwise, the gradients will accumulate, which may be undesired.

```python
# After training -> evaluate
model.eval()

with torch.no_grad():
    outputs = model(X_train)

print(outputs)

'''
tensor([[ 2.8569, -4.1618],
        [ 2.5382, -3.7548],
        [ 2.0944, -3.1820],
        [-1.4814,  1.4816],
        [-1.7176,  1.7342]])
'''
probas = torch.softmax(outputs, dim=1)
print(probas)

'''
tensor([[    0.9991,     0.0009],
        [    0.9982,     0.0018],
        [    0.9949,     0.0051],
        [    0.0491,     0.9509],
        [    0.0307,     0.9693]])
first value (column) means that the training example has a 99.91% probability of belonging to class 0 and a 0.09% probability of belonging to class 1.
'''
predictions = torch.argmax(probas, dim=1)
print(predictions) # tensor([0, 0, 0, 1, 1])

# Just use argmax of outputs along first dimension (their actual values):
predictions = torch.argmax(outputs, dim=1)
print(predictions) # tensor([0, 0, 0, 1, 1])

# Now we compare this to true training labels to see if model is correct
predictions == y_train # tensor([True, True, True, True, True])


# Better way to do this:
torch.sum(predictions == y_train) # output = 5 (100% accuracy)

## Cleaner wayt to do this:
def compute_accuracy(model, dataloader):

    model = model.eval()
    correct = 0.0
    total_examples = 0

    for idx, (features, labels) in enumerate(dataloader):

        with torch.no_grad():
            logits = model(features)

        predictions = torch.argmax(logits, dim=1)
        compare = labels == predictions
        correct += torch.sum(compare)
        total_examples += len(compare)

    return (correct / total_examples).item()

'''
compute_accuracy(model, train_loader) # result = 1.0
compute_accuracy(model, test_loader) # result = 1.0
All correct
'''

```

- Saving the model: torch.save(model.state_dict(), "model.pth")
state_dict is a Python dictionary object that maps each layer in the model to its trainable parameters (weights and biases)

- Trasnfering tensors into GPU:

```python
# New: Define a device variable that defaults to a GPU.
device = torch.device("cuda")
# New: Transfer the model onto the GPU.
model.to(device)
```

### GPU and more on PyTorch

You can create tensors in multiple ways:
    x = torch.tensor([[1., 2, 3], [4, 5, 6]])  # @inspect x
    x = torch.zeros(4, 8)  # 4x8 matrix of all zeros @inspect x
    x = torch.ones(4, 8)  # 4x8 matrix of all ones @inspect x
    x = torch.randn(4, 8)  # 4x8 matrix of iid Normal(0, 1) samples @inspect x

 Almost everything (parameters, gradients, activations, optimizer states) are stored as floating point numbers.
 The float32 data type (also known as fp32 or single precision) is the default.
    Traditionally, in scientific computing, float32 is the baseline; you could use double precision (float64) in some cases.

By default, tensors are stored in CPU memory.
    x = torch.zeros(32, 32)
    assert x.device == torch.device("cpu")
    However, in order to take advantage of the massive parallelism of GPUs, we need to move them to GPU memory.

![cputogpu](/assets/images/pytorch/cpu_to_gpu.png)

```python
x = torch.ones(16, 32)
w = torch.ones(32, 2)
y = x @ w
assert y.size() == torch.Size([16, 2]) # true
```

![bs](/assets/images/pytorch/batchvsseq.png)

- In general, we perform operations for every example in a batch and token in a sequence.

```python
x = torch.ones(4, 8, 16, 32)
w = torch.ones(32, 2)
y = x @ w
assert y.size() == torch.Size([4, 8, 16, 2]) 
# In this case, we iterate over values of the first 2 dimensions of x and multiply by w.
```

## FLOPS

Having gone through all the operations, let us examine their computational cost.
    A floating-point operation (FLOP) is a basic operation like addition (x + y) or multiplication (x y).
    Two terribly confusing acronyms (pronounced the same!):
    
FLOPs: floating-point operations (measure of computation done)
    
FLOP/s: floating-point operations per second (also written as FLOPS), which is used to measure the speed of hardware.

- Training GPT-3 (2020) took 3.14e23 FLOPs
- Training GPT-4 (2023) is speculated to take 2e25 FLOPs 

### Recap

```python
 y = 0.5 (x * w - 5)^2
# Forward pass: compute loss
x = torch.tensor([1., 2, 3])
w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient
pred_y = x @ w
loss = 0.5 * (pred_y - 5).pow(2)
# Backward pass: compute gradients
loss.backward()
assert loss.grad is None
assert pred_y.grad is None
assert x.grad is None
assert torch.equal(w.grad, torch.tensor([1, 2, 3]))
```

# Additional
[PyTorch Internals](https://blog.ezyang.com/2019/05/pytorch-internals/)

# Full flow

```python
# input layer: 10 inputs
# 1st hidden layer: 6 nodes and 1 bias unit (edges represent weight connections)
# 2nd hidden layer has 4 nodes and a node repr bias units
# output layer: 3 outputs

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

class NeuralNetwork(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super(NeuralNetwork, self).__init__()
        self.layers = torch.nn.Sequential(
            # 1st hidden layer
            torch.nn.Linear(num_inputs, 30),
            torch.nn.ReLU(),
            # 2nd hidden layer
            torch.nn.Linear(30, 20),
            torch.nn.ReLU(),
            # output layer
            torch.nn.Linear(20, num_outputs),
        )
    
    def forward(self, x):
        return self.layers(x) # logits
    
model = NeuralNetwork(50, 3)
# print(model)

num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total number of trainable model parameters: {num_parameters}")
 
# print(model.layers[0].weight)
# print(model.layers[0].weight.shape) # 30, 50
# print(model.layers[0].bias.shape) # 30

    

torch.manual_seed(123)
# model = NeuralNetwork(50, 3)
# print(model.layers[0].weight)

x = torch.rand((1, 50)) # our network expects 50-dimensional feature vectors
out = model(x)
print(out) # 3 outputs

# disable gd, use for inference
with torch.no_grad():
    out=model(x)
print(out)

# class prob
with torch.no_grad():
    out=torch.softmax(model(x), dim=1)
print(out)

# 5 training examples with two features each, 3 classes belong to 0, 2 belong to class 1, also make test set of two entries.
x_train = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
])
y_train = torch.tensor([0, 0, 0, 1, 1])
x_test = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
])
y_test = torch.tensor([0, 1])

# toy dataset
class ToyDataset(Dataset):
    def __init__(self, x, y):
        self.features = x
        self.labels = y

    def __getitem__(self, index):
        one_x = self.features[index]
        one_y = self.labels[index]
        return one_x, one_y
    
    def __len__(self):
        return self.labels.shape[0]
    
train_ds = ToyDataset(x_train, y_train)
test_ds = ToyDataset(x_test, y_test)

# purpose: use it to instantiate dataloader 
# print(len(train_ds)) # 5

torch.manual_seed(123)
train_loader = DataLoader(dataset=train_ds, batch_size=2, shuffle=True, num_workers=0)
test_loader = DataLoader(dataset=test_ds, batch_size=2, shuffle=False, num_workers=0)

for idx, (x, y) in enumerate(train_loader):
    print(f"Batch {idx + 1}: ", x, y)


'''
Note that we specified a batch size of 2 above, but the 3rd batch only contains a single example. That’s because we have five training examples, which is not evenly divisible by 2. In practice, having a substantially smaller batch as the last batch in a training epoch can disturb the convergence during training. To prevent this, it’s recommended to set drop_last=True
which will drop the last batch in each epoch: drop_last=True

Batch 1: tensor([[-1.2000,  3.1000],
        [-0.5000,  2.6000]]) tensor([0, 0])
Batch 2: tensor([[ 2.3000, -1.1000],
        [-0.9000,  2.9000]]) tensor([1, 0])
'''


# Training loop
torch.manual_seed(123)
model = NeuralNetwork(2, 3)
optimizer = torch.optim.SGD(model.parameters(), lr=0.5)

num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch_idx, (features, labels) in enumerate(train_loader):

        logits = model(features)
        loss = F.cross_entropy(logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}")

    model.eval()

    with torch.no_grad():
        outputs = model(x_train)

    torch.set_printoptions(sci_mode=False)
    prob = torch.softmax(outputs, dim=1)
    print(prob)
    '''
     means that the training example has a 99.91% probability of belonging to class 0 and a 0.09% probability of belonging to class 1. (The set_printoptions call is used here to make the outputs more legible.)
    '''
    # convert these into class label predictions using argmax (returns index posn of highest val in each row if wet dim=1 and highest value in each column if dim=0)
    predictions = torch.argmax(prob, dim=1)
    print(predictions) # [0,0,0,1,1] our desired output
    # verifying
    print(f"Number of correct predictions out of 5: {torch.sum(predictions==y_train)}")

    # print(outputs)

def compute_accuracy(model, dataloader):
    model = model.eval()
    correct= 0.0
    total_examples = 0
    for idx, (features, labels) in enumerate(dataloader):
        with torch.no_grad():
            logits = model(features)
        predictions = torch.argmax(logits, dim=1)
        compare = (labels == predictions)
        correct += torch.sum(compare)
        total_examples += len(compare)
    return (correct / total_examples).item()

print(compute_accuracy(model, train_loader)) # 1.0 since all are correct predictions

# saving the model
torch.save(model.state_dict(), 'model.pth')


# loading the model
model = NeuralNetwork(2, 3)# needs to match orig model exactly
print(model.load_state_dict(torch.load('model.pth'))) # <All keys matched>

tensor_1 = torch.tensor([1., 2., 3.])
tensor_2 = torch.tensor([4., 5., 6.])

print(tensor_1 + tensor_2)
# using cuda
# tensor_1 = tensor_1.to('cuda')
# tensor_2 = tensor_2.to('cuda')
# print(tensor_1 + tensor_2)

# device = torch.device("cuda")
# New: Transfer the model onto the GPU. 
# model.to(device)
# features, labels = features.to(device), labels.to(device)    

# better method: device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```