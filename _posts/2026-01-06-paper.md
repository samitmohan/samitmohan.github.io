---
layout: post
title: "how to read a paper (Deepseekv3)"
date: 2026-01-06
tokens: "~4.6k"
---

Taking a paper (DeepSeekV3) and discussing why it works

## [DeepSeekv3](https://arxiv.org/abs/2412.19437)


- First off, this is a heavy ass paper. 53 pages (same as Esio Trot by Roald Dahl).. is not a research paper.
- This is off first read of the paper and how I usually read papers.
  - Step 1: Skim abstract + figures + tables + read conclusion
  - Step 2: First glance -> read and think about it
  - Step 3: Look at the formulas, upload them to GPT and explain in the easiest way, upload the paper to NotebookLLM, ask questions (all stupid questions you have after first read)
  - Step 4: Second read -> just to make sure you read it right the first time.
  - Step 5: Third read -> probably read a blog post on it, and write something about it explaining same stuff to yourself

> breaking it down using this paper

**Better than Llama 3.1 405b, Qwen, and Mistral.**
**Trained for 2048 GPUs for 2 months, cost = $6M with no loss spikes reported.**



## Key takeaways on first glance

> major innovations I see as soon as I read this paper:

- Cheap as hell
- Performs really well
- MoE LLM scaled to 671B with only 37B active per token
- Auxiliary-loss-free load-balancing for MOE routing (keeps experts balanced without hurting performance)
- Multi-Token Prediction

If a paper is long, it almost always has **3â€“4 real ideas** and a lot of supporting engineering.

For DeepSeek-V3, the core ideas are:

1. **Multi-Head Latent Attention (MLA)** â†’ fixes KV-cache / inference memory
2. **DeepSeekMoE with aux-loss-free balancing** â†’ fixes MoE instability
3. **Multi-Token Prediction (MTP)** â†’ denser training signal + faster inference
4. **FP8 + DualPipe (systems)** â†’ makes all of this feasible at scale

We'll get into these later.

Some results for motivation:
![results](/assets/images/paper/results.png)

## Architecture



![arch](/assets/images/paper/arch.png)

Before reading text, pause here.

Ask:

- What replaces attention? â†’ MLA
- What replaces FFN? â†’ MoE
- Whatâ€™s new vs standard transformer? â†’ both attention *and* FFN are modified

At this point, you already know *where to zoom in*.


- Mixture of Experts
- Multi-Head Latent Attention
- Pretraining using 14.8T tokens
  - First stage: max context length = 32k
  - Second stage: max context length = 128k

- Post-training: SFT + RL on the base model of DSv3 (alignment to human preferences)
  - During post-training, they have distilled ("teacher" model transfers its learned knowledge to a smaller, more efficient "student" model) using R1 models.
    - Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. 

- Load Balancing Strategy and Multi-Token-Prediction Objective (MTP) for performance + inference speed
- FP8 mixed precision for Quantisation

> Three things to focus on

- Multi-Head Latent Attention
- DeepSeek MOE - Auxiliary-Loss-Free Load Balancing
- Multi-Token Prediction

## Multi-Head Latent Attention 

> introduced in DeepSeek-v2 paper, fraction of resources + outperforms standard MHA 

- We all know Mult-Head attention works, but it has a huge problem -> KV cache is HUGEEE. How do we reduce this space?
- This is how attention scores look like for a normal MHA
  - ![attention_scores](/assets/images/paper/attention_mha.png)
    - *i* = i-th attention head, *j* = indexes over all previous tokens in sequence (from 1 -> t)
    - *dh* = dimension of each attention head = ( total embedding dimension of model ) / number_of_heads
    - *softmax* = attention score (converts logits to [0-1] probability ensuring they sum to 1) ~ normalisation
    - outputs from all attention heads are concatenated to a single vector * Weight matrix $W_o$ 
      - Converts back to original embedded dimension (so that attention input = output dimension)
    - $K_t$ and $V_t$ are stored in a cache so when new token comes in, it doesn't have to calculate k, v for all previous tokens again to compute attention scores. Since we are only having new Q every time, K and V stay the same for previous tokens. *This is smart, but we do need to store this K and V for previous tokens somewhere right? We store it in a cache. KV-Cache 101.*
    - Storing KV Cache for every token = memory bottleneck
    - Don't just believe what you read, what do we mean by a memory bottleneck here? How to compute memory for this?
      - **start**
      - For each token:
        - [K1, K2, K3, .. K_number_of_heads] = number_of_heads * dimension_head for keys
        - [V1, V2, V3, .. V_number_of_heads] = number_of_heads * dimension_head for values
      - Total: 2 * number_of_heads * dimension_head
      - Sequence Length = L
      - Total Memory -> (2 * number_of_heads * dimension_heads * Length)
        - As Length grows -> 2 * number_of_heads * dimension_heads per token becomes LARGEEE.
      - **fin**

    - Suffers from high KV-Cache requirements (inference bottleneck)
    - So how does MLA solve this? *drumroll*:  compress key-value pairs. How though? *Low Rank Key-Value Joint Compression*
- **Instead of storing full key-value pairs, MLA compresses them into a shared latent space and reconstruct K and V *only when needed***

```python
  # Original MHA
key = Weight_K @ input_token      # Full size key
value = Weight_V @ input_token         # Full size value

# MLA Compression:
compressed_kv = Weight_down @ input_token  # Compressed latent vector (store only this) low dim repre that stores only essential info
    # Weight_down = down projection matrix of Weight that reduces the dimension

# when needed -> reconstruct key and values
key = Weight_up_k @ compressed_kv  
value = Weight_up_v @ compressed_kv

# These can also be applied to queries to reduce memory usage
```

- What are these Weight_down and Weight_up? **Projections** : Instead of storing K, V, store a small compressed vector.

> Think of these projections like creating a detailed shadow of an object. The shadow is simpler and has fewer dimensions (it's 2D), but it still captures the object's essential shape. MLA creates a compressed 'shadow' of the Key and Value vectors, capturing the necessary information in a much smaller space."

- How to do this? Store weight_up_key, weight_up_val (Fixed weights of model, one-time storage cost)
- MLA does have large projection matrices, theyâ€™re part of the model parameters (stored once) rather than per-token memory requirements.
- Per-token memory (what we need to cache during inference) is just the small l-dimensional vector called compressed_kv
- ![cl](/assets/images/paper/commutative_law.png)

```python
# fixed weights of the model:

Weight_up_k: (number_of_heads * dimension_heads) Ã— Length  
Weight_up_v: (number_of_heads * dimension_heads) Ã— Length  

# caching per token
compressed_kv: dimension Length only  

# DIFFERENCE
# mha
mha_tokens_memory = sequence_length * (2 * number_of_heads * dimension_heads) 

# mla
mla_tokens_memory = sequence_length * Length (compressed_kv)  # Much smaller!
model_params = 2 * (number_of_heads * dimension_heads * Length (compressed_kv))   # Fixed, one-time cost
```

- Only storing small compressed vectors for each token instead of full key-value pairs. Reduces KV Cache by **93.3%**

> one problem: ROPE is incompatible with low rank compression since ROPE = position sensitive to keys and queries making absorption impossible


RoPE is **position-sensitive**  
Low-rank compression is **linear algebra**

Traditional RoPE applies position encoding to both K and Q

This becomes problematic with compressed KV pairs because matrix multiplication isnâ€™t commutative

So in MLA, we canâ€™t merge RoPE with compressed representations efficiently

"The main issue is that RoPE is sensitive to the exact position of a token. The compression step in MLA, however, involves matrix multiplication which is not commutative (A  B â‰  B  A). This means you can't just apply RoPE before compression and expect it to work correctly afterward. The positional information gets scrambled.

> To solve this, DeepSeek decouples it: they first create the compressed representations and then apply positional encodings to generate the final, position-aware keys and queries.

- **Decouple positional and compressed components**
- Apply RoPE *after* reconstruction

This is a classic example of:
> A practical hack driven by math constraints

- Compressed query is projected to obtain the decoupled queries
- Rope is then applied to this to obtain positional queries : result-> set of positional queries across all attention heads
- Similarily, input tokens are projected to obtain decoupled keys, ROPE is applied to make these keys positional aware
- These two vectors are concatenated (compressed repr and positional info)
- Similarily this happens for Keys (not just Q)
- Attention Score is calculated

```python
# New approach - dECOUPLEDrOpe:
# 1. Separate position-aware queries
query_R = RoPE(Weight^QR @ compressed_query)    # Position-aware queries
key_R = RoPE(Weight^KR @ input_token)    # Position-aware shared key

# 2. Split into heads and concatenate
query_t,i = [query_R_1, query_R_2, ..., query_R_no_of_heads] 
key_t = [key_R_1, key_R_2]            

# 3. Concatenate for final attention
query_full = [query_t, 1; query_t,2]              
key_full = [key_t,1; key_t,2]        
```

- ![drope](/assets/images/paper/decoupledROPE.png)

### Memory Requirements Summary

![comp](/assets/images/paper/comparision.png)

## Let's look at the code for MLA

```python
'''
Replace normal multi-head attention, Reduce KV cache size, Share latent representation

Standard Attention
  Q = X * Wq
  K = X * Wk
  V = X * Wv
  Attention(Q, K, V)

MLA
  Q = X Wq
  Latent = X * W_latent
  K = Latent * Wk_recon
  V = Latent * Wv_recon
  Attention(Q, K, V)
'''
import torch
import torch.nn as nn
import torch.nn.functional as F

class MLA(nn.Module):
    def __init__(self, d_model, n_heads, d_latent, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.h = n_heads
        self.dh = d_model // n_heads

        self.q = nn.Linear(d_model, d_model, bias=False)
        self.z = nn.Linear(d_model, d_latent, bias=False)
        self.kv = nn.Linear(d_latent, 2 * d_model, bias=False)

        self.out = nn.Linear(d_model, d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        B, T, D = x.shape

        q = self.q(x)
        k, v = self.kv(self.z(x)).chunk(2, dim=-1)

        def split(t):
            return t.view(B, T, self.h, self.dh).transpose(1, 2)

        q, k, v = map(split, (q, k, v))

        attn = (q @ k.transpose(-2, -1)) / (self.dh ** 0.5)
        attn = self.drop(attn.softmax(dim=-1))

        out = (attn @ v).transpose(1, 2).reshape(B, T, D)
        return self.out(out)

class MLABlock(nn.Module):
    def __init__(self, d_model, n_heads, d_latent):
        super().__init__()
        self.attn = MLA(d_model, n_heads, d_latent)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )
        self.n1 = nn.LayerNorm(d_model)
        self.n2 = nn.LayerNorm(d_model)

    def forward(self, x):
        x = self.n1(x + self.attn(x))
        return self.n2(x + self.ffn(x))

# transformer
class MLATransformer(nn.Module):
    def __init__(self, vocab, d_model=4096, n_heads=32, d_latent=512, n_layers=24):
        super().__init__()
        self.embed = nn.Embedding(vocab, d_model)
        self.blocks = nn.ModuleList(
            [MLABlock(d_model, n_heads, d_latent) for _ in range(n_layers)]
        )
        self.head = nn.Linear(d_model, vocab)

    def forward(self, ids):
        x = self.embed(ids)
        for b in self.blocks:
            x = b(x)
        return self.head(x)

# MLA compresses keys and values through a shared latent space, reducing KV-cache memory from O(2Â·d_model) to O(d_latent) per token, without changing the attention equation.

# let embed_dim = 4096 and latent_dim = 512

# | Method       | Stored per token |
# | ------------ | ---------------- |
# | Standard MHA | `2 Ã— 4096`       |
# | MLA          | `512`            |
# ~8Ã— KV memory reduction :: this is HUGE.
```

---

> Now let's look at the next optimisation DSv3 does:

## MOE

> MOE: reduces computation cost to just 18% of dense models

**Q. What are dense models?**
![dm](/assets/images/paper/ffn_dense.png)

**Q. What are sparse models?**
![sm](/assets/images/paper/sparse.png)
![sm2](/assets/images/paper/sm2.png)


- Instead of one huge brain doing everything, MoE uses many small specialist brains, and a router that decides which ones to consult for each token.

How standard transformer works (recap)
Token â†’ Attention â†’ Feed-Forward Network (FFN) â†’ Next layer

- Every token uses the same FFN
- Compute cost grows linearly with model size
- Bigger model = slower + more expensive

- **In Mixture of Experts, the FFN is replaced with many FFNs (experts)**

![replace](/assets/images/paper/replaceFFN.png)

Token â†’ Attention â†’ Router â†’ Selected Experts â†’ Combine â†’ Next layer

Instead of *1 FFN for all tokens* we get: *N experts (e.g. 256 FFNs)* from which only K experts are used per token (e.g. K = 2)

> The expert selection process happens dynamically at each step of computation, where different experts can be activated even within the same sequence. The router network continuously evaluates which expert(s) should handle each part of the input or generation, making this a fine-grained, token-level division of labor rather than a coarse query-level split.

**Intuition**

In a hospital, we have

- Cardiologist
- Neurologist
- Orthopedic
- Dermatologist

We donâ€™t send every patient to all doctors, A nurse (router) decides who sees whom

- MoE = hospital
- Experts = doctors
- Router = triage nurse
- Token = patient

Similarily DeepSeekv3 uses:

- Many experts per MoE layer
- Sparse activation: Only top-K experts are active per token

Experts are:

- Independent FFNs
- Same input/output shape

Outputs are weighted and summed

> Examples

*So for a token like "gradient":*

- Expert #17 (math)
- Expert #42 (ML)
might get activated (top K here is 2)

*For "Shakespeare":*

- Expert #3 (literature)
- Expert #91 (language style)
might get activated

**How is this better than vanilla dense model?**

| Model | Total Params | Active Params per token |
| ----- | ------------ | ----------------------- |
| Dense | 70B          | 70B                     |
| MoE   | 230B         | ~30B                    |

- Knowledge of a 230B model
- Cost of a ~30B model

**Q. What does the router do actually?**
token_embedding â†’ router â†’ scores for each expert
Then:

- Pick top-K experts
- Normalize scores
- Dispatch token to those experts

Important: Routing happens per token, not per sentence.

![moe](/assets/images/paper/moe.png)

### Architecture of MOE

MOE architecture consists of the following 2 components-

- Experts
- Gating Network / Router

- Input Encoding
  - The input (token embedding) enters the model.
  - This embedding is sent both to the normal transformer path and to a gating (router) network.
  - The routerâ€™s job is only to decide which experts should handle this input.

- Gating / Routing Decision
  - The gating network scores all available experts for the given input.
  - These scores represent how useful each expert would be for this input.
  - Usually, only the top-K experts (e.g. K=1 or 2) are selected.

- Sparsity (Conditional Computation)
  - Instead of running all experts, the model runs only the selected ones. This is called sparse activation.
  - Result:
    - Massive model capacity
    - Low compute cost per token

- Expert Processing
  - Each selected expert is a small neural network (typically a Feed-Forward Network).
  - The same input is sent to each selected expert.
  - Each expert processes the input independently.

- Expert Weighting
  - The output of each expert is scaled by its gating weight.
  - More relevant experts contribute more.
  - Less relevant experts contribute less.

- Output Aggregation
  - The weighted outputs from the selected experts are combined (usually summed).
  - This combined output replaces the standard FFN output in the transformer layer.

- Pass to Next Layer
  - The aggregated output is passed to the next transformer layer.
  - The process repeats for every MoE layer and every token

*tldr;*

- Router chooses experts
- Only a few experts run
- Outputs are weighted and combined
- You get big-model intelligence at small-model cost

![moe2](/assets/images/paper/moe2.png)


Where can this be a problem? **Router collapse** (same expert always picked)

- expert1: 5%
- expert2': 2%
- expert3': 1%
- expert4: 6%
- expert5: 76%

As you can see, this is not balanced at all. Workload isn't being distributed properly. If the same experts get picked:

- Others donâ€™t train
- Capacity is wasted
- Performance drops

**Q. What is the problem?**
Not only will there be an uneven distribution of experts chosen, but some experts will hardly be trained at all. This results in issues during both training and inference.

Instead, we want equal importance among experts during training and inference. How to prevent overfitting on the same experts.

**How to solve this? Load Balancing!!!!**

- DeepSeek v3 does Bias-Based Routing for DeepseekMoE
  - ![bias](/assets/images/paper/bias.png)
  - score_for_routing = original_score + bias
  - For each expert after each training step:
    - If expert is overloaded:
      - bias -= Î³  (make them less likely to get picked next time)
    - If expert is underloaded:
      - bias += Î³  (make them more likely to get picked next time)

> bias term (Î³) is dynamically added or subtracted from an expert's score based on whether it is over- or under-loaded during training
Maintains quality (original affinity scores) while achieving balance (through bias adjustments)

> This is like a smart traffic controller at a busy intersection. If one road (expert) gets too congested, the controller adjusts the traffic lights (bias) to redirect cars (tokens) to less busy roads. The Î³ is like a small adjustment, ensuring the change isn't too drastic. This keeps traffic flowing smoothly without creating new jams elsewhere.


Instead of adding a big auxiliary loss (which hurts quality), they:

- Add a **bias term** to routing scores
- Adjust it dynamically based on expert load

This is subtle but important:

- Keeps routing *mostly semantic*
- Gently nudges balance

When reading papers, look for:
> â€œDoes this fix introduce a new problem elsewhere?â€

Here, the answer seems mostly **no**,

- For each token:
  - We want many experts available, but we want to run only a few & combine their outputs intelligently.

- Routing answers two questions:
  - *Which experts should run?*
  - *How much should each selected expert contribute?*

Let $u_t$â€‹ (Everything the model knows so far about this token) be the embedding of the t-th token coming into the FFN/MoE layer

DeepSeek-style MoE splits experts into:

- A. Shared experts (always on)
  - Capture general-purpose transformations
- B. Routed experts (conditionally on)
  - Only a few are activated, captures specialized behavior

- Shared experts â†’ no weights
- Routed experts â†’ weighted by routing scores

**output = input + shared expert outputs + routed expert outputs**

Here's the formula for the same (it's the input + FFN output for shared + FFN for gated routed (gi,t) output)
![formula](/assets/images/paper/formula.png)

## How routing decides which experts to use

**Step 1: Compute an affinity score for each routed expert**

For each routed expert i, compute:

- s(i, t) = sigmoid (input.T * $e_i$) where $e_i$ = learned vector representing expert i (dot product here means similarity and sigmoid squashes score to [0,1]

The core of this formula is the dot product input.T * $e_i$. In vector math, a dot product is a measure of similarity. 

This step is essentially asking: 'How similar is the current token's meaning to the specialty of expert i? The result is a similarity score.

Meaning: *How suitable is expert i for token t?*

**Step 2: Sparsity: only keep the top-K experts**
Instead of using all routed experts:

- Select the top K experts with highest scores
- Set all others to zero

![sparsity](/assets/images/paper/sparsity.png)

**Step 3: Normalize the selected experts (soft weighting)**
![normalise](/assets/images/paper/normalise.png)
> So they sum upto 1 (makes outputs stable, prevents exploding activations)

**Step 4: Apply routed experts**
![routed](/assets/images/paper/routed_exp.png)

*Final MoE output:*

- Start with the original input (residual)
- Add shared expert outputs
- Add weighted routed expert outputs


> Each token asks a few experts â€œhow would you process this?â€, weighs their answers, and combines them â€” instead of asking the entire model.

| Design choice          | Reason                      |
| ---------------------- | --------------------------- |
| Dot product with `e_i` | Learn expert specialization |
| Sigmoid                | Stable scores               |
| Top-K                  | Sparsity & efficiency       |
| Normalization          | Stable training             |
| Weighted sum           | Smooth expert blending      |
| Residual connection    | Training stability          |

## Let's look at the code for MOE

```python
'''
In a Mixture-of-Experts layer, each token is routed to only a small subset of experts (top-k) instead of all parameters being activated. A lightweight router assigns tokens to experts, and only the selected experts process that token. This keeps compute roughly constant while allowing the total parameter count to scale.

x_after_mla â”€â”€â–º router scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º top-k experts selected
                 â”‚  (gating network)
                 â–¼
        selected experts compute
    weighted outputs â†’ aggregated

g_i' = Router(x)       # raw scores for each expert
g_i  = top-k + normalize
output = Î£ (g_i * Expert_i(x))

for token:
    pick top-k experts
    run only those
'''

# Top-K MOE
import torch
import torch.nn as nn
import torch.nn.functional as F

class MoE(nn.Module):
    def __init__(self, d_model, n_experts=8, k=2):
        super().__init__()
        self.k = k
        self.router = nn.Linear(d_model, n_experts, bias=False)
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, 4 * d_model),
                nn.GELU(),
                nn.Linear(4 * d_model, d_model)
            )
            for _ in range(n_experts)
        ])

    def forward(self, x):
        B, T, D = x.shape
        scores = self.router(x)                # (B,T,E)
        probs = scores.softmax(dim=-1)

        topk_val, topk_idx = probs.topk(self.k, dim=-1)

        out = torch.zeros_like(x)

        for i in range(self.k):
            expert_ids = topk_idx[..., i]
            expert_wts = topk_val[..., i].unsqueeze(-1)

            for e, expert in enumerate(self.experts):
                mask = (expert_ids == e)
                if mask.any():
                    out[mask] += expert(x[mask]) * expert_wts[mask]

        return out

# MOE
class MoEBlock(nn.Module):
    def __init__(self, d_model, n_heads, n_experts, k):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.moe = MoE(d_model, n_experts, k)
        self.n1 = nn.LayerNorm(d_model)
        self.n2 = nn.LayerNorm(d_model)

    def forward(self, x):
        x = self.n1(x + self.attn(x, x, x)[0])
        return self.n2(x + self.moe(x))

# model
class SimpleMoE(nn.Module):
    def __init__(self, vocab, d_model=512, n_heads=8,
                 n_layers=6, n_experts=8, k=2):
        super().__init__()
        self.emb = nn.Embedding(vocab, d_model)
        self.layers = nn.ModuleList([
            MoEBlock(d_model, n_heads, n_experts, k)
            for _ in range(n_layers)
        ])
        self.head = nn.Linear(d_model, vocab)

    def forward(self, ids):
        x = self.emb(ids)
        for l in self.layers:
            x = l(x)
        return self.head(x)
```

### Joining the dots

- How does transformer for DSv3 look like?
- FFN -> MOE
- Attention -> MLA (low rank key-val joint compression)

```text
Input
  â†“
MLA (attention)
  â†“
MoE (FFN replacement)
  â†“
Output
```

```text
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Input Tokens (X)      â”‚
                            â”‚ shape: (B, T, D)        â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚             Input Embeddings          â”‚
                    â”‚ (or output from previous layer)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚     MLA Layer    â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ 1) Query projection Q                â”‚
                       â”‚ 2) Latent projection Z               â”‚
                       â”‚ 3) Reconstruct K,V from latent Z     â”‚
                       â”‚ 4) Attention(Q,K,V)                  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  MLA Output (Attn Out) â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚        Add & LayerNorm               â”‚
                      â”‚  (residual connection)               â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚     MoE Layer    â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                MoE Inside                       â”‚
              â”‚  1) Router computes scores for routed experts   â”‚
              â”‚  2) Take Top-K experts per token                â”‚
              â”‚  3) Normalize gating weights                    â”‚
              â”‚  4) Run only selected experts                   â”‚
              â”‚  5) Weighted sum of expert outputs + shared FFNsâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Add & LayerNorm (MoE residual)       â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Output to Next Block    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

----

> Now let's look at the final optimisation DSv3 does:

## Multi-token Prediction

- Most standard language models â€” like GPT, LLaMA, etc. â€” are trained with next-token prediction (NTP)
- This is what people mean when they say the word *autoregressive* (one token at a time)
- Multi-token prediction is basically predicting multiple future tokens at once (e.g., the next ð‘› tokens) from the same input context
  - Instead of p(xt+1â€‹ âˆ£ x1:tâ€‹), we do p(xt+1â€‹, xt+2â€‹, â€¦, xt+nâ€‹âˆ£x1:tâ€‹)
  - Each head predicts one future token. These predictions are trained jointly.
- Training only: inference still generates one token at a time
- Shared backbone: no extra transformer compute
- Works best with long sequences

**Q. Why?**

- **Denser learning signal**
  - With next-token prediction, each training example yields one scalar loss.
  - With multi-token prediction, each position yields n prediction losses simultaneously.
  - More gradient signal per token (faster learning per example)
- **Inference acceleration**
  - If you can predict multiple tokens in one pass, you reduce the number of sequential forward passes needed
  - Fewer sequential decoding step

**Q. How does this speed up inference?**

- A standard autoregressive model generates one token at a time, requiring one full forward pass of the model for each token. 
- To generate 100 tokens, it needs 100 sequential passes. 
- With MTP, if the model can predict, say, 3 tokens in a single pass, you might only need around 33 passes to generate the same 100 tokens. This reduction in the number of sequential steps is a major source of acceleration.

DeepseekV3 does the same, it doesnâ€™t just train on next-token prediction. It also learns to predict several future tokens from each position, jointly, during training.

### How is it different than original MTP

- The original academic MTP proposal predicts all future tokens in parallel using independent heads at each position.
- DeepSeekâ€™s implementation is slightly different
  - From a single hidden state (hidden_state_t), the model uses multiple independent 'prediction heads.' Each head is trained to predict a different future token.
    - For example, head_1 predicts token t+1, head_2 predicts token t+2, and so on. 
    - All these predictions happen simultaneously from the same point in the sequence, which is what provides the rich training signal.

- DeepSeek-V3 predicts two or more future tokens in this chained manner
- Advantages
  - Because the model learns to anticipate more outcomes from a single context position, it gets more training signal per token seen
  - Empirical results in the paper show that multi-token prediction improves performance on generative tasks (e.g., code generation tasks)
  - ![loss](/assets/images/paper/loss_for_mtp.png)
  - All losses are accumulated, giving a denser total objective per position

## Let's look at the code for MTP

```python
'''
hidden_t â”€â”€ linear â”€â”€â–º [P(t+1), P(t+2), P(t+3)]
'''
import torch
import torch.nn as nn
import torch.nn.functional as F

class MTPTransformer(nn.Module):
    def __init__(self, vocab, d_model=512, n_layers=6, n_heads=8, K=3):
        super().__init__()
        self.K = K

        self.tok = nn.Embedding(vocab, d_model)
        self.pos = nn.Embedding(2048, d_model)

        layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, batch_first=True
        )
        self.encoder = nn.TransformerEncoder(layer, n_layers)

        # One projection predicts K future tokens
        self.head = nn.Linear(d_model, K * vocab)
        self.vocab = vocab

    def forward(self, ids):
        B, T = ids.shape
        pos = torch.arange(T, device=ids.device).unsqueeze(0)

        x = self.tok(ids) + self.pos(pos)
        h = self.encoder(x)                          # (B,T,D)

        logits = self.head(h)                        # (B,T,KÂ·V)
        return logits.view(B, T, self.K, self.vocab) # (B,T,K,V)


# loss
def mtp_loss(logits, ids):
    """
    logits: (B, T, K, V)
    ids:    (B, T)
    """
    B, T, K, V = logits.shape
    loss = 0.0

    for k in range(K):
        pred = logits[:, :-k-1, k]     # predict t+k+1
        tgt  = ids[:, k+1:]            # ground truth
        loss += F.cross_entropy(
            pred.reshape(-1, V),
            tgt.reshape(-1)
        )
    return loss

# usage
model = MTPTransformer(vocab=32000, K=3).cuda()
opt = torch.optim.AdamW(model.parameters(), 3e-4)

ids = torch.randint(0, 32000, (8, 128)).cuda()

logits = model(ids)
loss = mtp_loss(logits, ids)

loss.backward()
opt.step()

print(loss.item())

```

-----

## Summary

Evidence?

Look for:
- **Ablations** (does removing X hurt?)
- **Costs** (not just accuracy)
- **Stability** (loss spikes, divergence)

DeepSeek-V3 shows:
- MTP improves downstream performance
- MoE balancing stabilizes training
- FP8 does not cause instability at scale

Every good paper has limits.

DeepSeek-V3:

- Requires massive infra (2048 H800s)
- Custom kernels are non-trivial to reproduce
- Deployment unit is still large
- Inference speed â‰  solved forever

```text
Input
â†“
MLA â†’ fixes memory
â†“
MoE â†’ fixes compute
â†“
MTP â†’ fixes learning signal
â†“
FP8 + DualPipe â†’ makes it trainable
```

DeepSeek-V3 is an MoE Transformer that reduces active compute per token by routing tokens to a small subset of experts (37B activated vs 671B total). To make routing cheap and performant they (a) compress KV caches via Multi-Head Latent Attention (to shrink inference memory), (b) use a bias-driven routing update to balance expert load without a heavy auxiliary loss, and (c) densify supervision via Multi-Token Prediction (sequentially predicting D future tokens while preserving causal chains). On the infra side they push FP8 training (tile/block quantization + FP32 accumulation windows) and a pipeline scheme (DualPipe) to hide all-to-all communication â€” together those engineering choices let them claim strong benchmarks at a lower reported GPU cost

- MLA handles the memory/inference bottleneck.
- MoE handles the compute/scaling bottleneck.
- MTP provides a denser learning signal and further accelerates throughput

> If the model were a global shipping company, MLA is the advanced packaging that allows 90% more goods to fit in the same container; MoE is the specialized regional logistics centers that ensure only the necessary experts handle specific packages; and MTP is the predictive scheduling that allows the company to plan the next three stops of a delivery van at once instead of just one.

-----

## References

-[MOE](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

-[MLA](https://www.youtube.com/watch?v=NlDQUj1olXM)

-[MTP](https://dataturbo.medium.com/deepseek-technical-analysis-3-multi-token-prediction-f8f3ea7eaf9c)
