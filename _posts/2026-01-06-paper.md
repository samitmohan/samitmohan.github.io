---
layout: post
title: "How to read a paper"
date: 2026-01-06
---

# [DeepSeekv3](https://arxiv.org/abs/2412.19437)

> breaking it down

**Better than Llama 3.1 405b, Qwen, and Mistral.**

## Architecture

- Mixture of Experts
- Multi-Head Latent Attention
- Pretraining using 14.8T tokens
  - First stage: max context length = 32k
  - Second stage: max contenxt length = 128k

- Posttraining: SFT + RL on the base model of DSv3 (alignment to human preferences)
  - During post-training, they have distilled ("teacher" model transfers its learned knowledge to a smaller, more efficient "student" model) using R1 models.

- Load Balancing Strategy and Multi-Token-Prediction Objective (MTP) for performance + inference speed
- FP8 mixed precision for Quantisation
  - ![quant](/assets/images/paper/quantisation.png)
- Trained for 2048 GPUs for 2 months, $6M

![arch](/assets/images/paper/arch.png)

> Three things to focus on

- Multi-Head Latent Attention
- DeepSeek MOE - Auxiliary-Loss-Free Load Balancing
- Multi-Token Prediction

## Multi-Head Latent Attention (introduced in DeepSeek-v2 paper)

> fraction of resources + outperforms standard MHA 

- We all know Mult-Head attention works, but it has a huge problem -> KV cache is HUGEEE. How do we reduce this space?
- This is how attention scores look like for a normal MHA
  - ![attention_scores](/assets/images/paper/attention_mha.png)
    - i = i-th attention head
    - j indexes over all previous tokens in sequence (from 1 -> t)
    - dh = dimension of each attention head = ( total embedding dimension of model ) / number_of_heads
    - softmax = attention score (converts logits to [0-1] probability ensuring they sum to one (normalisation))
    - outputs from all attention heads are concatenated to a single vector * Weight matrix W_o (converts back to original embedded dimension (so that attention input = output dimension))

    - K_t and V_t are stored in a cache so when new token comes in, it doesn't have to calculate k, v for all previous tokens again to compute attention scores. Since we are only having new Q every time, K and V say the same for previous tokens. This is smart, but we do need to store this K and V for previous tokens somewhere right? We store it in a cache. KV-Cache 101.
    - Storing KV Cache for every token = memory bottleneck
    - Don't just believe what you read, what do we mean by a memory bottleneck here? How to compute memory for this?
      - **start**
      - For each token:
        - [K1, K2, K3, .. K_number_of_heads] = number_of_heads * dimension_head for keys
        - [V1, V2, V3, .. V_number_of_heads] = number_of_heads * dimension_head for values
      - Total: 2 * number_of_heads * dimension_head
      - Sequence Length = L
      - Total Memory -> (2 * number_of_heads * dimension_heads * Length)
        - As Length grows -> 2 * number_of_heads * dimension_heads per token becomes LARGEEE.
      - **fin**

    - Suffers from high KV-Cache requirements (inference bottleneck)
    - So how does MLA solve this? *drumroll*:  compress key-value pairs. How though? *Low Rank Key-Value Joint Compression*
- Instead of storing full key-value pairs, MLA compresses them into a shared latent space.

```python
  # Original MHA
key = Weight_K @ input_token      # Full size key
value = Weight_V @ input_token         # Full size value

# MLA Compression:
compressed_kv = Weight_down @ input_token  # Compressed latent vector (store only this) low dim repre that stores only essential info
    # Weight_down = down projection matrix of Weight that reduces the dimension

# when needed -> reconstruct key and values
key = Weight_up_k @ compressed_kv  
value = Weight_up_v @ compressed_kv

# These can also be applied to queries to reduce memory usage
```

- What are these Weight_down and Weight_up? **Projections** : Instead of storing K, V, store a small compressed vector.

> Think of these projections like creating a detailed shadow of an object. The shadow is simpler and has fewer dimensions (it's 2D), but it still captures the object's essential shape. MLA creates a compressed 'shadow' of the Key and Value vectors, capturing the necessary information in a much smaller space."

- How to do this? Store weight_up_key, weight_up_val (Fixed weights of model, one-time storage cost)
- MLA does have large projection matrices, theyâ€™re part of the model parameters (stored once) rather than per-token memory requirements.
- per-token memory (what we need to cache during inference) is just the small l-dimensional vector called compressed_kv
- ![cl](/assets/images/paper/commutative_law.png)

```python
# Fixed weights of the model:

Weight_up_k: (number_of_heads * dimension_heads) Ã— Length  
Weight_up_v: (number_of_heads * dimension_heads) Ã— Length  

# What we cache PER TOKEN:
compressed_kv: dimension Length only  

# DIFFERENCE
# MHA memory 
tokens_memory = sequence_length * (2 * number_of_heads * dimension_heads) 

# MLA memory
tokens_memory = sequence_length * Length (compressed_kv)  # Much smaller!
model_params = 2 * (number_of_heads * dimension_heads * Length (compressed_kv))   # Fixed, one-time cost
```

- Only storing small compressed vectors for each token instead of full key-value pairs. Reduces KV Cache by **93.3%**

> one problem: ROPE is incompatible with low rank compression since ROPE = position sensitive to keys and queries making absorption impossible

Traditional RoPE applies position encoding to both K and Q
This becomes problematic with compressed KV pairs because matrix multiplication isnâ€™t commutative
So in MLA, we canâ€™t merge RoPE with compressed representations efficiently

"The main issue is that RoPE is sensitive to the exact position of a token. The compression step in MLA, however, involves matrix multiplication which is not commutative (A  B â‰  B  A). This means you can't just apply RoPE before compression and expect it to work correctly afterward. The positional information gets scrambled.

> To solve this, DeepSeek decouples it: they first create the compressed representations and then apply positional encodings to generate the final, position-aware keys and queries."


- To solve this: Decoupled ROPE.
- Compressed query is projected to obtain the decoupled queries
- Rope is then applied to this to obtain positional queries : result-> set of positional queries across all attention heads
- Similarily, input tokens are projected to obtain decoupled keys, ROPE is applied to make these keys positional aware
- These two vectors are concatenated (compressed repr and positional info)
- Similarily this happens for Keys (not just Q)
- Attention Score is calculated

```python
# New approach - dECOUPLEDrOpe:
# 1. Separate position-aware queries
q_R = RoPE(W^QR @ c_q)    # Position-aware queries
k_R = RoPE(W^KR @ h_t)    # Position-aware shared key

# 2. Split into heads and concatenate
q_t,i = [q_R_1, q_R_2, ..., q_R_nh] 
k_t = [k_R_1, k_R_2]            

# 3. Concatenate for final attention
q_full = [q_t,1; q_t,2]              
k_full = [k_t,1; k_t,2]        
```

- ![drope](/assets/images/paper/decoupledROPE.png)

### Memory Requirements Summary

```python
# Key parameters:
n_h = num_heads
d_h = dim_per_head
l = compression_dim
d_c = decoupled_dim

# Memory per token for each mechanism:
MHA:   2 * n_h * d_h        # Full KV storage
GQA:   2 * n_g * d_h        # Grouped KV storage
MQA:   2 * d_h              # Single KV pair
MLA:   (d_c + d_R) â‰ˆ 4.5 * d_h # Compressed + RoPE
```

![comp](/assets/images/paper/comparision.png)

## Let's look at the code for MLA

```python
'''
Standard Attention
Q = X * Wq
K = X * Wk
V = X * Wv
Attention(Q, K, V)

MLA
Q = X Wq
Latent = X * W_latent
K = Latent * Wk_recon
V = Latent * Wv_recon
Attention(Q, K, V)

(Replace normal multi-head attention, Reduce KV cache size, Share latent representation)

input â†’ [Q = x W_q]
      â†’ [latent Z = x W_latent]
      â†’ [keys = Z W_k_recon]
      â†’ [values = Z W_v_recon]
      â†’ compute Attention(Q,K,V)
      â†’ MLA output

'''

import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadLatentAttention(nn.Module):
    def __init__(
        self,
        embed_dim,
        num_heads,
        latent_dim,
        dropout=0.1
    ):
        super().__init__()
        assert embed_dim % num_heads == 0

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.latent_dim = latent_dim

        # Query projection (normal)
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)

        # Latent projection (shared for K and V)
        self.latent_proj = nn.Linear(embed_dim, latent_dim, bias=False)

        # Reconstruction from latent space
        self.k_recon = nn.Linear(latent_dim, embed_dim, bias=False)
        self.v_recon = nn.Linear(latent_dim, embed_dim, bias=False)

        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        x: (B, T, D)
        """
        B, T, D = x.shape

        # Queries
        Q = self.q_proj(x)  # (B,T,D)

        # Latent representation (cached in inference)
        Z = self.latent_proj(x)  # (B,T,d_latent)

        # Reconstruct K and V
        K = self.k_recon(Z)
        V = self.v_recon(Z)

        # Reshape for multi-head
        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

        # Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        out = torch.matmul(attn, V)

        # Merge heads
        out = out.transpose(1, 2).contiguous().view(B, T, D)
        return self.out_proj(out)

class MLATransformerBlock(nn.Module):
    def __init__(
        self,
        embed_dim,
        num_heads,
        latent_dim,
        dropout=0.1
    ):
        super().__init__()

        self.attn = MultiHeadLatentAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            latent_dim=latent_dim,
            dropout=dropout
        )

        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.GELU(),
            nn.Linear(4 * embed_dim, embed_dim)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # MLA Attention
        x = x + self.dropout(self.attn(x))
        x = self.norm1(x)

        # FFN
        x = x + self.dropout(self.ffn(x))
        x = self.norm2(x)

        return x
class MLATransformer(nn.Module):
    def __init__(
        self,
        vocab_size,
        embed_dim=512,
        num_heads=8,
        latent_dim=128,
        num_layers=6
    ):
        super().__init__()

        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.layers = nn.ModuleList([
            MLATransformerBlock(
                embed_dim=embed_dim,
                num_heads=num_heads,
                latent_dim=latent_dim
            )
            for _ in range(num_layers) ])

        self.lm_head = nn.Linear(embed_dim, vocab_size)

    def forward(self, input_ids):
        x = self.embed(input_ids)  # (B,T,D)
        for layer in self.layers:
            x = layer(x)
        return self.lm_head(x)

'''
let embed_dim = 4096 and latent_dim = 512

| Method       | Stored per token |
| ------------ | ---------------- |
| Standard MHA | `2 Ã— 4096`       |
| MLA          | `512`            |
~8Ã— KV memory reduction
'''

> Now let's look at the next optimisation DSv3 does:

----

## MOE

> MOE: reduces computation cost to just 18% of dense models

**Q. What are dense models?**
![dm](/assets/images/paper/ffn_dense.png)

**Q. What are sparse models?**
![sm](/assets/images/paper/sparse.png)
![sm2](/assets/images/paper/sm2.png)


- Instead of one huge brain doing everything, MoE uses many small specialist brains, and a router that decides which ones to consult for each token.

How standard transformer works (recap)
Token â†’ Attention â†’ Feed-Forward Network (FFN) â†’ Next layer

- Every token uses the same FFN
- Compute cost grows linearly with model size
- Bigger model = slower + more expensive

- **In Mixture of Experts, the FFN is replaced with many FFNs (experts)**

![replace](/assets/images/paper/replaceFFN.png)

Token â†’ Attention â†’ Router â†’ Selected Experts â†’ Combine â†’ Next layer

Instead of *1 FFN for all tokens* we get: *N experts (e.g. 256 FFNs)* from which only K experts are used per token (e.g. K = 2)

> The expert selection process happens dynamically at each step of computation, where different experts can be activated even within the same sequence. The router network continuously evaluates which expert(s) should handle each part of the input or generation, making this a fine-grained, token-level division of labor rather than a coarse query-level split.

**Intuition**
In a hospital, we have

- Cardiologist
- Neurologist
- Orthopedic
- Dermatologist

We donâ€™t send every patient to all doctors, A nurse (router) decides who sees whom

- MoE = hospital
- Experts = doctors
- Router = triage nurse
- Token = patient

Similarily DeepSeekv3 uses:

- Many experts per MoE layer
- Sparse activation: Only top-K experts are active per token

Experts are:

- Independent FFNs
- Same input/output shape

Outputs are weighted and summed

### Examples

*So for a token like "gradient":*

- Expert #17 (math)
- Expert #42 (ML)
might get activated (top K here is 2)

*For "Shakespeare":*

- Expert #3 (literature)
- Expert #91 (language style)
might get activated

**How is this better than vanilla dense model?**

| Model | Total Params | Active Params per token |
| ----- | ------------ | ----------------------- |
| Dense | 70B          | 70B                     |
| MoE   | 230B         | ~30B                    |

- Knowledge of a 230B model
- Cost of a ~30B model

**Q. What does the router do actually?**
token_embedding â†’ router â†’ scores for each expert
Then:

- Pick top-K experts
- Normalize scores
- Dispatch token to those experts

Important: Routing happens per token, not per sentence.

![moe](/assets/images/paper/moe.png)

### Architecture of MOE

MOE architecture consists of the following 2 components-

- Experts
- Gating Network / Router

- Input Encoding
  - The input (token embedding) enters the model.
  - This embedding is sent both to the normal transformer path and to a gating (router) network.
  - The routerâ€™s job is only to decide which experts should handle this input.

- Gating / Routing Decision
  - The gating network scores all available experts for the given input.
  - These scores represent how useful each expert would be for this input.
  - Usually, only the top-K experts (e.g. K=1 or 2) are selected.

- Sparsity (Conditional Computation)
  - Instead of running all experts, the model runs only the selected ones. This is called sparse activation.
  - Result:
    - Massive model capacity
    - Low compute cost per token

- Expert Processing
  - Each selected expert is a small neural network (typically a Feed-Forward Network).
  - The same input is sent to each selected expert.
  - Each expert processes the input independently.

- Expert Weighting
  - The output of each expert is scaled by its gating weight.
  - More relevant experts contribute more.
  - Less relevant experts contribute less.

- Output Aggregation
  - The weighted outputs from the selected experts are combined (usually summed).
  - This combined output replaces the standard FFN output in the transformer layer.

- Pass to Next Layer
  - The aggregated output is passed to the next transformer layer.
  - The process repeats for every MoE layer and every token

*tldr;*

- Router chooses experts
- Only a few experts run
- Outputs are weighted and combined
- You get big-model intelligence at small-model cost

![moe2](/assets/images/paper/moe2.png)


Where can this be a problem? **Router collapse** (same expert always picked)

```text

expert1: 5%
expert2': 2%
expert3': 1%
expert4: 6%
expert5: 76%

As you can see, this is not balanced at all. Workload isn't being distributed properly
```

**Q. What is the problem?**
Not only will there be an uneven distribution of experts chosen, but some experts will hardly be trained at all. This results in issues during both training and inference.

Instead, we want equal importance among experts during training and inference. How to prevent overfitting on the same experts.

**How to solve this? Load Balancing!!!!**

- DeepSeek v3 does Bias-Based Routing for DeepseekMoE
  - ![bias](/assets/images/paper/bias_mech.png)
    - [credit for this beautiful explanation](https://www.himanshustwts.com/posts/deepseek-v3)
  - score_for_routing = original_score + bias
  - For each expert after each training step:
    - If expert is overloaded:
      - bias -= Î³  (make them less likely to get picked next time)
    - If expert is underloaded:
      - bias += Î³  (make them more likely to get picked next time)

> bias term (Î³) is dynamically added or subtracted from an expert's score based on whether it is over- or under-loaded during training
Maintains quality (original affinity scores) while achieving balance (through bias adjustments)

> This is like a smart traffic controller at a busy intersection. If one road (expert) gets too congested, the controller adjusts the traffic lights (bias) to redirect cars (tokens) to less busy roads. The Î³ is like a small adjustment, ensuring the change isn't too drastic. This keeps traffic flowing smoothly without creating new jams elsewhere.

- For each token:
  - We want many experts available, but we want to run only a few & combine their outputs intelligently.

- Routing answers two questions:
  - *Which experts should run?*
  - *How much should each selected expert contribute?*

Let utâ€‹ (Everything the model knows so far about this token) be the embedding of the t-th token coming into the FFN/MoE layer

DeepSeek-style MoE splits experts into:

- A. Shared experts (always on)
  - Capture general-purpose transformations
- B. Routed experts (conditionally on)
  - Only a few are activated, captures specialized behavior

- Shared experts â†’ no weights
- Routed experts â†’ weighted by routing scores

**output = input + shared expert outputs + routed expert outputs**
Here's the formula for the same (it's the input + FFN output for shared + FFN for gated routed (gi,t) output)
![formula](/assets/images/paper/formula.png)

## How routing decides which experts to use

**Step 1: Compute an affinity score for each routed expert**
For each routed expert i, compute:

- s(i, t) = sigmoid (input.T * e_i) where e_i = learned vector representing expert i (dot product here means similarity and sigmoid squashes score to [0,1]
The core of this formula is the dot product input.T * e_i. In vector math, a dot product is a measure of similarity. So, this step is essentially asking: 'How similar is the current token's meaning to the specialty of expert i?' The result is a similarity score."

Meaning: *How suitable is expert i for token t?*

**Step 2: Sparsity: only keep the top-K experts**
Instead of using all routed experts:

- Select the top K experts with highest scores
- Set all others to zero

![sparsity](/assets/images/paper/sparsity.png)

**Step 3: Normalize the selected experts (soft weighting)**
![normalise](/assets/images/paper/normalise.png)
> So they sum upto 1 (makes outputs stable, prevents exploding activations)

**Step 4: Apply routed experts**
![routed](/assets/images/paper/routed_exp.png)

*Final MoE output:*

- Start with the original input (residual)
- Add shared expert outputs
- Add weighted routed expert outputs


> Each token asks a few experts â€œhow would you process this?â€, weighs their answers, and combines them â€” instead of asking the entire model.

| Design choice          | Reason                      |
| ---------------------- | --------------------------- |
| Dot product with `e_i` | Learn expert specialization |
| Sigmoid                | Stable scores               |
| Top-K                  | Sparsity & efficiency       |
| Normalization          | Stable training             |
| Weighted sum           | Smooth expert blending      |
| Residual connection    | Training stability          |








## Let's look at the code for MOE

```python
'''
x_after_mla â”€â”€â–º router scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º top-k experts selected
                 â”‚  (gating network)
                 â–¼
        selected experts compute
    weighted outputs â†’ aggregated

g_i' = Router(x)       # raw scores for each expert
g_i  = top-k + normalize
output = Î£ (g_i * Expert_i(x))

'''

import torch
import torch.nn as nn
import torch.nn.functional as F

class MoELayer(nn.Module):
    def __init__(self, embed_dim, num_experts=8, k=2):
        """
        embed_dim: token embedding dimension
        num_experts: total experts
        k: number of experts to activate
        """
        super().__init__()
        self.num_experts = num_experts
        self.k = k

        # Each expert is a small feedforward network
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.ReLU(),
                nn.Linear(embed_dim, embed_dim)
            )
            for _ in range(num_experts)
        ])

        # Router: key vectors for experts
        self.router = nn.Linear(embed_dim, num_experts, bias=False)

    def forward(self, x):
        """
        x: (batch_size, seq_len, embed_dim)
        """
        # Compute raw scores for each expert
        scores = self.router(x)  # (B, S, E)

        # Softmax across experts
        probs = F.softmax(scores, dim=-1)  # normalized weighting

        # Get top-k indices and corresponding probabilities
        topk_val, topk_idx = probs.topk(self.k, dim=-1)  # (B,S,k)

        # Prepare empty output
        output = torch.zeros_like(x)

        # For each selected expert, compute output
        for expert_i in range(self.num_experts):
            # create mask where this expert is selected
            mask = (topk_idx == expert_i).float()  # (B,S,k)

            # gather probabilities for this expert
            expert_mask_val = (mask * topk_val).sum(dim=-1, keepdim=True)  # (B,S,1)

            if expert_mask_val.sum() == 0:
                continue

            # run expert on all tokens
            expert_out = self.experts[expert_i](x)
            output += expert_out * expert_mask_val

        return output

# Attention â†’ FFN

class MoETransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, num_experts=8, k=2, dropout=0.1):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        self.moe_ffn = MoELayer(embed_dim, num_experts=num_experts, k=k)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Attention
        att_out, _ = self.attn(x, x, x)
        x = x + self.dropout(att_out)
        x = self.norm1(x)

        # MoE FFN replacement part
        moe_out = self.moe_ffn(x)
        x = x + self.dropout(moe_out)
        x = self.norm2(x)

        return x

class SimpleMoEModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=512,
                 num_heads=8, num_layers=6,
                 num_experts=8, k=2):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.layers = nn.ModuleList([
            MoETransformerBlock(
                embed_dim=embed_dim,
                num_heads=num_heads,
                num_experts=num_experts,
                k=k
            )
            for _ in range(num_layers)
        ])
        self.output = nn.Linear(embed_dim, vocab_size)

    def forward(self, input_ids):
        x = self.embed(input_ids)  # (B, S, E)
        x = x.transpose(0, 1)  # for attention
        for layer in self.layers:
            x = layer(x)
        x = x.transpose(0, 1)
        logits = self.output(x)
        return logits

# usage
data = torch.randint(0, vocab_size, (64, 128))  # (batch, seq)
labels = data.clone()

model = SimpleMoEModel(vocab_size).cuda()
opt = torch.optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(10):
    model.train()
    opt.zero_grad()

    logits = model(data.cuda())
    loss = F.cross_entropy(
        logits.view(-1, logits.size(-1)),
        labels.view(-1).cuda()
    )

    loss.backward()
    opt.step()

    print(f"Epoch {epoch} | Loss {loss.item():.4f}")


```

### Joining the dots

- How does transformer for DSv3 look like?
- FFN -> MOE
- Attention -> MLA (low rank key-val joint compression)

```text
Input
  â†“
MLA (attention)
  â†“
MoE (FFN replacement)
  â†“
Output
```

```text
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Input Tokens (X)      â”‚
                            â”‚ shape: (B, T, D)        â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚             Input Embeddings          â”‚
                    â”‚ (or output from previous layer)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚     MLA Layer     â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ 1) Query projection Q                â”‚
                       â”‚ 2) Latent projection Z               â”‚
                       â”‚ 3) Reconstruct K,V from latent Z      â”‚
                       â”‚ 4) Attention(Q,K,V)                  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  MLA Output (Attn Out) â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚        Add & LayerNorm              â”‚
                      â”‚  (residual connection)              â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚     MoE Layer     â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                MoE Inside                          â”‚
              â”‚  1) Router computes scores for routed experts      â”‚
              â”‚  2) Take Top-K experts per token                   â”‚
              â”‚  3) Normalize gating weights                       â”‚
              â”‚  4) Run only selected experts                      â”‚
              â”‚  5) Weighted sum of expert outputs + shared FFNs    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Add & LayerNorm (MoE residual)       â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Output to Next Block    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

----

> Now let's look at the final optimisation DSv3 does:

## Multi-token Prediction

- Most standard language models â€” like GPT, LLaMA, etc. â€” are trained with next-token prediction (NTP)
- This is what people mean when they say the word *autoregressive* (one token at a time)
- Multi-token prediction is basically predicting multiple future tokens at once (e.g., the next ð‘› tokens) from the same input context
  - Instead of p(xt+1â€‹ âˆ£ x1:tâ€‹), we do p(xt+1â€‹, xt+2â€‹, â€¦, xt+nâ€‹âˆ£x1:tâ€‹)
  - Each head predicts one future token. These predictions are trained jointly.

Q. Why?

- **Denser learning signal**
  - With next-token prediction, each training example yields one scalar loss.
  - With multi-token prediction, each position yields n prediction losses simultaneously â€” i.e., more gradient signal per training position. More signal â†’ faster learning per example

- **Inference acceleration**
  - If you can predict multiple tokens in one pass, you reduce the number of sequential forward passes needed

Q. How does this speed up inference? 

- A standard autoregressive model generates one token at a time, requiring one full forward pass of the model for each token. 
- To generate 100 tokens, it needs 100 sequential passes. 
- With MTP, if the model can predict, say, 3 tokens in a single pass, you might only need around 33 passes to generate the same 100 tokens. This reduction in the number of sequential steps is a major source of acceleration.

DeepseekV3 does the same, it doesnâ€™t just train on next-token prediction. It also learns to predict several future tokens from each position, jointly, during training.

### How is it different than original MTP

- The original academic MTP proposal predicts all future tokens in parallel using independent heads at each position.
- DeepSeekâ€™s implementation is slightly different
  - From a single hidden state (hidden_state_t), the model uses multiple independent 'prediction heads.' Each head is trained to predict a different future token.
    - For example, head_1 predicts token t+1, head_2 predicts token t+2, and so on. 
    - All these predictions happen simultaneously from the same point in the sequence, which is what provides the rich training signal.

- DeepSeek-V3 predicts two or more future tokens in this chained manner
- Advantages
  - Because the model learns to anticipate more outcomes from a single context position, it gets more training signal per token seen
  - Empirical results in the paper show that multi-token prediction improves performance on generative tasks (e.g., code generation tasks)
  - ![loss](/assets/images/paper/loss_for_mtp.png)
  - All losses are accumulated, giving a denser total objective per position

## Let's look at the code for MTP

```python
'''
All heads share the same backbone.
hidden_state_t
   â”œâ”€â”€ head_1 â†’ predicts token t+1
   â”œâ”€â”€ head_2 â†’ predicts token t+2
   â”œâ”€â”€ head_3 â†’ predicts token t+3
   â””â”€â”€ ...
'''
import torch
import torch.nn as nn
import torch.nn.functional as F

class MTPTransformer(nn.Module):
    def __init__( self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, max_future_tokens=3):
        super().__init__()

        self.embed_dim = embed_dim
        self.max_future_tokens = max_future_tokens

        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        self.pos_embed = nn.Embedding(2048, embed_dim)

        encoder_layer = nn.TransformerEncoderLayer( d_model=embed_dim, nhead=num_heads, batch_first=True)

        self.transformer = nn.TransformerEncoder( encoder_layer, num_layers=num_layers)

        # One output head per future token
        self.output_heads = nn.ModuleList([
            nn.Linear(embed_dim, vocab_size)
            for _ in range(max_future_tokens)
        ])

    def forward(self, input_ids):
        """
        input_ids: (B, T)
        """
        B, T = input_ids.shape

        positions = torch.arange(T, device=input_ids.device)
        positions = positions.unsqueeze(0).expand(B, T)

        x = self.token_embed(input_ids) + self.pos_embed(positions)

        hidden = self.transformer(x)  # (B, T, D)

        # Each head predicts a different future token
        logits = [
            head(hidden) for head in self.output_heads
        ]

        # logits[k] predicts token t + (k+1)
        return logits

    def mtp_loss(self, logits, input_ids):
    """
    logits: list of length K
            each element is (B, T, vocab_size)
    input_ids: (B, T)
    """
    total_loss = 0.0
    B, T = input_ids.shape

    for k, logit in enumerate(logits):
        # shift targets by k+1
        target = input_ids[:, k+1:]
        pred = logit[:, :T - (k + 1), :]

        loss = F.cross_entropy( pred.reshape(-1, pred.size(-1)), target.reshape(-1))

        total_loss += loss

    return total_loss

# Example usage
vocab_size = 32000
model = MTPTransformer(
    vocab_size=vocab_size,
    embed_dim=512,
    num_layers=6,
    num_heads=8,
    max_future_tokens=3
).cuda()

optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

# Dummy batch
input_ids = torch.randint(0, vocab_size, (8, 128)).cuda()

model.train()
optimizer.zero_grad()

logits = model(input_ids)
loss = mtp_loss(logits, input_ids)

loss.backward()
optimizer.step()

print("Loss:", loss.item())
```

-----

## Summary

![moe_final](/assets/images/paper/moe_final.jpg)

- MLA handles the memory/inference bottleneck.
- MoE handles the compute/scaling bottleneck.
- MTP provides a denser learning signal and further accelerates throughput

> If the model were a global shipping company, MLA is the advanced packaging that allows 90% more goods to fit in the same container; MoE is the specialized regional logistics centers that ensure only the necessary experts handle specific packages; and MTP is the predictive scheduling that allows the company to plan the next three stops of a delivery van at once instead of just one.

-----

## References

-[MOE](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

-[In depth explanation of MLA](https://www.youtube.com/watch?v=NlDQUj1olXM)

-[MTP](https://dataturbo.medium.com/deepseek-technical-analysis-3-multi-token-prediction-f8f3ea7eaf9c)